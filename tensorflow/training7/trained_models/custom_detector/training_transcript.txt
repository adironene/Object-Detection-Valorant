[2025-05-11 01:49:20] Starting training session
Model configuration: IMAGE_SIZE=(416, 416), BATCH_SIZE=16, EPOCHS=100, LEARNING_RATE=0.0001

[2025-05-11 01:49:20] === Starting Valorany Detector ===
[2025-05-11 01:49:20] Training data: ../tf_files/train.record
[2025-05-11 01:49:20] Validation data: ../tf_files/valid.record
[2025-05-11 01:49:20] Loading data from ../tf_files/train.record and ../tf_files/valid.record
[2025-05-11 01:49:21] Estimating dataset sizes...
[2025-05-11 01:49:21] Train dataset size: ~337 batches (approx. 5385 examples)
[2025-05-11 01:49:21] Validation dataset size: ~33 batches (approx. 514 examples)
[2025-05-11 01:49:21] Creating model...
[2025-05-11 01:49:21] Compiling model with learning rate: 1e-05
[2025-05-11 01:49:21] Model: "functional"
????????????????????????????????????????????????????????????????????????????????????????????????????????????
? Layer (type)                  ? Output Shape              ?         Param # ? Connected to               ?
????????????????????????????????????????????????????????????????????????????????????????????????????????????
? input_layer (InputLayer)      ? (None, 416, 416, 3)       ?               0 ? -                          ?
????????????????????????????????????????????????????????????????????????????????????????????????????????????
? Conv1 (Conv2D)                ? (None, 208, 208, 32)      ?             864 ? input_layer[0][0]          ?
????????????????????????????????????????????????????????????????????????????????????????????????????????????
? bn_Conv1 (BatchNormalization) ? (None, 208, 208, 32)      ?             128 ? Conv1[0][0]                ?
????????????????????????????????????????????????????????????????????????????????????????????????????????????
? Conv1_relu (ReLU)             ? (None, 208, 208, 32)      ?               0 ? bn_Conv1[0][0]             ?
????????????????????????????????????????????????????????????????????????????????????????????????????????????
? expanded_conv_depthwise       ? (None, 208, 208, 32)      ?             288 ? Conv1_relu[0][0]           ?
? (DepthwiseConv2D)             ?                           ?                 ?                            ?
????????????????????????????????????????????????????????????????????????????????????????????????????????????
? expanded_conv_depthwise_BN    ? (None, 208, 208, 32)      ?             128 ? expanded_conv_depthwise[0? ?
? (BatchNormalization)          ?                           ?                 ?                            ?
????????????????????????????????????????????????????????????????????????????????????????????????????????????
? expanded_conv_depthwise_relu  ? (None, 208, 208, 32)      ?               0 ? expanded_conv_depthwise_B? ?
? (ReLU)                        ?                           ?                 ?                            ?
????????????????????????????????????????????????????????????????????????????????????????????????????????????
? expanded_conv_project         ? (None, 208, 208, 16)      ?             512 ? expanded_conv_depthwise_r? ?
? (Conv2D)                      ?                           ?                 ?                            ?
????????????????????????????????????????????????????????????????????????????????????????????????????????????
? expanded_conv_project_BN      ? (None, 208, 208, 16)      ?              64 ? expanded_conv_project[0][? ?
? (BatchNormalization)          ?                           ?                 ?                            ?
????????????????????????????????????????????????????????????????????????????????????????????????????????????
? block_1_expand (Conv2D)       ? (None, 208, 208, 96)      ?           1,536 ? expanded_conv_project_BN[? ?
????????????????????????????????????????????????????????????????????????????????????????????????????????????
? block_1_expand_BN             ? (None, 208, 208, 96)      ?             384 ? block_1_expand[0][0]       ?
? (BatchNormalization)          ?                           ?                 ?                            ?
????????????????????????????????????????????????????????????????????????????????????????????????????????????
? block_1_expand_relu (ReLU)    ? (None, 208, 208, 96)      ?               0 ? block_1_expand_BN[0][0]    ?
????????????????????????????????????????????????????????????????????????????????????????????????????????????
? block_1_pad (ZeroPadding2D)   ? (None, 209, 209, 96)      ?               0 ? block_1_expand_relu[0][0]  ?
????????????????????????????????????????????????????????????????????????????????????????????????????????????
? block_1_depthwise             ? (None, 104, 104, 96)      ?             864 ? block_1_pad[0][0]          ?
? (DepthwiseConv2D)             ?                           ?                 ?                            ?
????????????????????????????????????????????????????????????????????????????????????????????????????????????
? block_1_depthwise_BN          ? (None, 104, 104, 96)      ?             384 ? block_1_depthwise[0][0]    ?
? (BatchNormalization)          ?                           ?                 ?                            ?
????????????????????????????????????????????????????????????????????????????????????????????????????????????
? block_1_depthwise_relu (ReLU) ? (None, 104, 104, 96)      ?               0 ? block_1_depthwise_BN[0][0] ?
????????????????????????????????????????????????????????????????????????????????????????????????????????????
? block_1_project (Conv2D)      ? (None, 104, 104, 24)      ?           2,304 ? block_1_depthwise_relu[0]? ?
????????????????????????????????????????????????????????????????????????????????????????????????????????????
? block_1_project_BN            ? (None, 104, 104, 24)      ?              96 ? block_1_project[0][0]      ?
? (BatchNormalization)          ?                           ?                 ?                            ?
????????????????????????????????????????????????????????????????????????????????????????????????????????????
? block_2_expand (Conv2D)       ? (None, 104, 104, 144)     ?           3,456 ? block_1_project_BN[0][0]   ?
????????????????????????????????????????????????????????????????????????????????????????????????????????????
? block_2_expand_BN             ? (None, 104, 104, 144)     ?             576 ? block_2_expand[0][0]       ?
? (BatchNormalization)          ?                           ?                 ?                            ?
????????????????????????????????????????????????????????????????????????????????????????????????????????????
? block_2_expand_relu (ReLU)    ? (None, 104, 104, 144)     ?               0 ? block_2_expand_BN[0][0]    ?
????????????????????????????????????????????????????????????????????????????????????????????????????????????
? block_2_depthwise             ? (None, 104, 104, 144)     ?           1,296 ? block_2_expand_relu[0][0]  ?
? (DepthwiseConv2D)             ?                           ?                 ?                            ?
????????????????????????????????????????????????????????????????????????????????????????????????????????????
? block_2_depthwise_BN          ? (None, 104, 104, 144)     ?             576 ? block_2_depthwise[0][0]    ?
? (BatchNormalization)          ?                           ?                 ?                            ?
????????????????????????????????????????????????????????????????????????????????????????????????????????????
? block_2_depthwise_relu (ReLU) ? (None, 104, 104, 144)     ?               0 ? block_2_depthwise_BN[0][0] ?
????????????????????????????????????????????????????????????????????????????????????????????????????????????
? block_2_project (Conv2D)      ? (None, 104, 104, 24)      ?           3,456 ? block_2_depthwise_relu[0]? ?
????????????????????????????????????????????????????????????????????????????????????????????????????????????
? block_2_project_BN            ? (None, 104, 104, 24)      ?              96 ? block_2_project[0][0]      ?
? (BatchNormalization)          ?                           ?                 ?                            ?
????????????????????????????????????????????????????????????????????????????????????????????????????????????
? block_2_add (Add)             ? (None, 104, 104, 24)      ?               0 ? block_1_project_BN[0][0],  ?
?                               ?                           ?                 ? block_2_project_BN[0][0]   ?
????????????????????????????????????????????????????????????????????????????????????????????????????????????
? block_3_expand (Conv2D)       ? (None, 104, 104, 144)     ?           3,456 ? block_2_add[0][0]          ?
????????????????????????????????????????????????????????????????????????????????????????????????????????????
? block_3_expand_BN             ? (None, 104, 104, 144)     ?             576 ? block_3_expand[0][0]       ?
? (BatchNormalization)          ?                           ?                 ?                            ?
????????????????????????????????????????????????????????????????????????????????????????????????????????????
? block_3_expand_relu (ReLU)    ? (None, 104, 104, 144)     ?               0 ? block_3_expand_BN[0][0]    ?
????????????????????????????????????????????????????????????????????????????????????????????????????????????
? block_3_pad (ZeroPadding2D)   ? (None, 105, 105, 144)     ?               0 ? block_3_expand_relu[0][0]  ?
????????????????????????????????????????????????????????????????????????????????????????????????????????????
? block_3_depthwise             ? (None, 52, 52, 144)       ?           1,296 ? block_3_pad[0][0]          ?
? (DepthwiseConv2D)             ?                           ?                 ?                            ?
????????????????????????????????????????????????????????????????????????????????????????????????????????????
? block_3_depthwise_BN          ? (None, 52, 52, 144)       ?             576 ? block_3_depthwise[0][0]    ?
? (BatchNormalization)          ?                           ?                 ?                            ?
????????????????????????????????????????????????????????????????????????????????????????????????????????????
? block_3_depthwise_relu (ReLU) ? (None, 52, 52, 144)       ?               0 ? block_3_depthwise_BN[0][0] ?
????????????????????????????????????????????????????????????????????????????????????????????????????????????
? block_3_project (Conv2D)      ? (None, 52, 52, 32)        ?           4,608 ? block_3_depthwise_relu[0]? ?
????????????????????????????????????????????????????????????????????????????????????????????????????????????
? block_3_project_BN            ? (None, 52, 52, 32)        ?             128 ? block_3_project[0][0]      ?
? (BatchNormalization)          ?                           ?                 ?                            ?
????????????????????????????????????????????????????????????????????????????????????????????????????????????
? block_4_expand (Conv2D)       ? (None, 52, 52, 192)       ?           6,144 ? block_3_project_BN[0][0]   ?
????????????????????????????????????????????????????????????????????????????????????????????????????????????
? block_4_expand_BN             ? (None, 52, 52, 192)       ?             768 ? block_4_expand[0][0]       ?
? (BatchNormalization)          ?                           ?                 ?                            ?
????????????????????????????????????????????????????????????????????????????????????????????????????????????
? block_4_expand_relu (ReLU)    ? (None, 52, 52, 192)       ?               0 ? block_4_expand_BN[0][0]    ?
????????????????????????????????????????????????????????????????????????????????????????????????????????????
? block_4_depthwise             ? (None, 52, 52, 192)       ?           1,728 ? block_4_expand_relu[0][0]  ?
? (DepthwiseConv2D)             ?                           ?                 ?                            ?
????????????????????????????????????????????????????????????????????????????????????????????????????????????
? block_4_depthwise_BN          ? (None, 52, 52, 192)       ?             768 ? block_4_depthwise[0][0]    ?
? (BatchNormalization)          ?                           ?                 ?                            ?
????????????????????????????????????????????????????????????????????????????????????????????????????????????
? block_4_depthwise_relu (ReLU) ? (None, 52, 52, 192)       ?               0 ? block_4_depthwise_BN[0][0] ?
????????????????????????????????????????????????????????????????????????????????????????????????????????????
? block_4_project (Conv2D)      ? (None, 52, 52, 32)        ?           6,144 ? block_4_depthwise_relu[0]? ?
????????????????????????????????????????????????????????????????????????????????????????????????????????????
? block_4_project_BN            ? (None, 52, 52, 32)        ?             128 ? block_4_project[0][0]      ?
? (BatchNormalization)          ?                           ?                 ?                            ?
????????????????????????????????????????????????????????????????????????????????????????????????????????????
? block_4_add (Add)             ? (None, 52, 52, 32)        ?               0 ? block_3_project_BN[0][0],  ?
?                               ?                           ?                 ? block_4_project_BN[0][0]   ?
????????????????????????????????????????????????????????????????????????????????????????????????????????????
? block_5_expand (Conv2D)       ? (None, 52, 52, 192)       ?           6,144 ? block_4_add[0][0]          ?
????????????????????????????????????????????????????????????????????????????????????????????????????????????
? block_5_expand_BN             ? (None, 52, 52, 192)       ?             768 ? block_5_expand[0][0]       ?
? (BatchNormalization)          ?                           ?                 ?                            ?
????????????????????????????????????????????????????????????????????????????????????????????????????????????
? block_5_expand_relu (ReLU)    ? (None, 52, 52, 192)       ?               0 ? block_5_expand_BN[0][0]    ?
????????????????????????????????????????????????????????????????????????????????????????????????????????????
? block_5_depthwise             ? (None, 52, 52, 192)       ?           1,728 ? block_5_expand_relu[0][0]  ?
? (DepthwiseConv2D)             ?                           ?                 ?                            ?
????????????????????????????????????????????????????????????????????????????????????????????????????????????
? block_5_depthwise_BN          ? (None, 52, 52, 192)       ?             768 ? block_5_depthwise[0][0]    ?
? (BatchNormalization)          ?                           ?                 ?                            ?
????????????????????????????????????????????????????????????????????????????????????????????????????????????
? block_5_depthwise_relu (ReLU) ? (None, 52, 52, 192)       ?               0 ? block_5_depthwise_BN[0][0] ?
????????????????????????????????????????????????????????????????????????????????????????????????????????????
? block_5_project (Conv2D)      ? (None, 52, 52, 32)        ?           6,144 ? block_5_depthwise_relu[0]? ?
????????????????????????????????????????????????????????????????????????????????????????????????????????????
? block_5_project_BN            ? (None, 52, 52, 32)        ?             128 ? block_5_project[0][0]      ?
? (BatchNormalization)          ?                           ?                 ?                            ?
????????????????????????????????????????????????????????????????????????????????????????????????????????????
? block_5_add (Add)             ? (None, 52, 52, 32)        ?               0 ? block_4_add[0][0],         ?
?                               ?                           ?                 ? block_5_project_BN[0][0]   ?
????????????????????????????????????????????????????????????????????????????????????????????????????????????
? block_6_expand (Conv2D)       ? (None, 52, 52, 192)       ?           6,144 ? block_5_add[0][0]          ?
????????????????????????????????????????????????????????????????????????????????????????????????????????????
? block_6_expand_BN             ? (None, 52, 52, 192)       ?             768 ? block_6_expand[0][0]       ?
? (BatchNormalization)          ?                           ?                 ?                            ?
????????????????????????????????????????????????????????????????????????????????????????????????????????????
? block_6_expand_relu (ReLU)    ? (None, 52, 52, 192)       ?               0 ? block_6_expand_BN[0][0]    ?
????????????????????????????????????????????????????????????????????????????????????????????????????????????
? block_6_pad (ZeroPadding2D)   ? (None, 53, 53, 192)       ?               0 ? block_6_expand_relu[0][0]  ?
????????????????????????????????????????????????????????????????????????????????????????????????????????????
? block_6_depthwise             ? (None, 26, 26, 192)       ?           1,728 ? block_6_pad[0][0]          ?
? (DepthwiseConv2D)             ?                           ?                 ?                            ?
????????????????????????????????????????????????????????????????????????????????????????????????????????????
? block_6_depthwise_BN          ? (None, 26, 26, 192)       ?             768 ? block_6_depthwise[0][0]    ?
? (BatchNormalization)          ?                           ?                 ?                            ?
????????????????????????????????????????????????????????????????????????????????????????????????????????????
? block_6_depthwise_relu (ReLU) ? (None, 26, 26, 192)       ?               0 ? block_6_depthwise_BN[0][0] ?
????????????????????????????????????????????????????????????????????????????????????????????????????????????
? block_6_project (Conv2D)      ? (None, 26, 26, 64)        ?          12,288 ? block_6_depthwise_relu[0]? ?
????????????????????????????????????????????????????????????????????????????????????????????????????????????
? block_6_project_BN            ? (None, 26, 26, 64)        ?             256 ? block_6_project[0][0]      ?
? (BatchNormalization)          ?                           ?                 ?                            ?
????????????????????????????????????????????????????????????????????????????????????????????????????????????
? block_7_expand (Conv2D)       ? (None, 26, 26, 384)       ?          24,576 ? block_6_project_BN[0][0]   ?
????????????????????????????????????????????????????????????????????????????????????????????????????????????
? block_7_expand_BN             ? (None, 26, 26, 384)       ?           1,536 ? block_7_expand[0][0]       ?
? (BatchNormalization)          ?                           ?                 ?                            ?
????????????????????????????????????????????????????????????????????????????????????????????????????????????
? block_7_expand_relu (ReLU)    ? (None, 26, 26, 384)       ?               0 ? block_7_expand_BN[0][0]    ?
????????????????????????????????????????????????????????????????????????????????????????????????????????????
? block_7_depthwise             ? (None, 26, 26, 384)       ?           3,456 ? block_7_expand_relu[0][0]  ?
? (DepthwiseConv2D)             ?                           ?                 ?                            ?
????????????????????????????????????????????????????????????????????????????????????????????????????????????
? block_7_depthwise_BN          ? (None, 26, 26, 384)       ?           1,536 ? block_7_depthwise[0][0]    ?
? (BatchNormalization)          ?                           ?                 ?                            ?
????????????????????????????????????????????????????????????????????????????????????????????????????????????
? block_7_depthwise_relu (ReLU) ? (None, 26, 26, 384)       ?               0 ? block_7_depthwise_BN[0][0] ?
????????????????????????????????????????????????????????????????????????????????????????????????????????????
? block_7_project (Conv2D)      ? (None, 26, 26, 64)        ?          24,576 ? block_7_depthwise_relu[0]? ?
????????????????????????????????????????????????????????????????????????????????????????????????????????????
? block_7_project_BN            ? (None, 26, 26, 64)        ?             256 ? block_7_project[0][0]      ?
? (BatchNormalization)          ?                           ?                 ?                            ?
????????????????????????????????????????????????????????????????????????????????????????????????????????????
? block_7_add (Add)             ? (None, 26, 26, 64)        ?               0 ? block_6_project_BN[0][0],  ?
?                               ?                           ?                 ? block_7_project_BN[0][0]   ?
????????????????????????????????????????????????????????????????????????????????????????????????????????????
? block_8_expand (Conv2D)       ? (None, 26, 26, 384)       ?          24,576 ? block_7_add[0][0]          ?
????????????????????????????????????????????????????????????????????????????????????????????????????????????
? block_8_expand_BN             ? (None, 26, 26, 384)       ?           1,536 ? block_8_expand[0][0]       ?
? (BatchNormalization)          ?                           ?                 ?                            ?
????????????????????????????????????????????????????????????????????????????????????????????????????????????
? block_8_expand_relu (ReLU)    ? (None, 26, 26, 384)       ?               0 ? block_8_expand_BN[0][0]    ?
????????????????????????????????????????????????????????????????????????????????????????????????????????????
? block_8_depthwise             ? (None, 26, 26, 384)       ?           3,456 ? block_8_expand_relu[0][0]  ?
? (DepthwiseConv2D)             ?                           ?                 ?                            ?
????????????????????????????????????????????????????????????????????????????????????????????????????????????
? block_8_depthwise_BN          ? (None, 26, 26, 384)       ?           1,536 ? block_8_depthwise[0][0]    ?
? (BatchNormalization)          ?                           ?                 ?                            ?
????????????????????????????????????????????????????????????????????????????????????????????????????????????
? block_8_depthwise_relu (ReLU) ? (None, 26, 26, 384)       ?               0 ? block_8_depthwise_BN[0][0] ?
????????????????????????????????????????????????????????????????????????????????????????????????????????????
? block_8_project (Conv2D)      ? (None, 26, 26, 64)        ?          24,576 ? block_8_depthwise_relu[0]? ?
????????????????????????????????????????????????????????????????????????????????????????????????????????????
? block_8_project_BN            ? (None, 26, 26, 64)        ?             256 ? block_8_project[0][0]      ?
? (BatchNormalization)          ?                           ?                 ?                            ?
????????????????????????????????????????????????????????????????????????????????????????????????????????????
? block_8_add (Add)             ? (None, 26, 26, 64)        ?               0 ? block_7_add[0][0],         ?
?                               ?                           ?                 ? block_8_project_BN[0][0]   ?
????????????????????????????????????????????????????????????????????????????????????????????????????????????
? block_9_expand (Conv2D)       ? (None, 26, 26, 384)       ?          24,576 ? block_8_add[0][0]          ?
????????????????????????????????????????????????????????????????????????????????????????????????????????????
? block_9_expand_BN             ? (None, 26, 26, 384)       ?           1,536 ? block_9_expand[0][0]       ?
? (BatchNormalization)          ?                           ?                 ?                            ?
????????????????????????????????????????????????????????????????????????????????????????????????????????????
? block_9_expand_relu (ReLU)    ? (None, 26, 26, 384)       ?               0 ? block_9_expand_BN[0][0]    ?
????????????????????????????????????????????????????????????????????????????????????????????????????????????
? block_9_depthwise             ? (None, 26, 26, 384)       ?           3,456 ? block_9_expand_relu[0][0]  ?
? (DepthwiseConv2D)             ?                           ?                 ?                            ?
????????????????????????????????????????????????????????????????????????????????????????????????????????????
? block_9_depthwise_BN          ? (None, 26, 26, 384)       ?           1,536 ? block_9_depthwise[0][0]    ?
? (BatchNormalization)          ?                           ?                 ?                            ?
????????????????????????????????????????????????????????????????????????????????????????????????????????????
? block_9_depthwise_relu (ReLU) ? (None, 26, 26, 384)       ?               0 ? block_9_depthwise_BN[0][0] ?
????????????????????????????????????????????????????????????????????????????????????????????????????????????
? block_9_project (Conv2D)      ? (None, 26, 26, 64)        ?          24,576 ? block_9_depthwise_relu[0]? ?
????????????????????????????????????????????????????????????????????????????????????????????????????????????
? block_9_project_BN            ? (None, 26, 26, 64)        ?             256 ? block_9_project[0][0]      ?
? (BatchNormalization)          ?                           ?                 ?                            ?
????????????????????????????????????????????????????????????????????????????????????????????????????????????
? block_9_add (Add)             ? (None, 26, 26, 64)        ?               0 ? block_8_add[0][0],         ?
?                               ?                           ?                 ? block_9_project_BN[0][0]   ?
????????????????????????????????????????????????????????????????????????????????????????????????????????????
? block_10_expand (Conv2D)      ? (None, 26, 26, 384)       ?          24,576 ? block_9_add[0][0]          ?
????????????????????????????????????????????????????????????????????????????????????????????????????????????
? block_10_expand_BN            ? (None, 26, 26, 384)       ?           1,536 ? block_10_expand[0][0]      ?
? (BatchNormalization)          ?                           ?                 ?                            ?
????????????????????????????????????????????????????????????????????????????????????????????????????????????
? block_10_expand_relu (ReLU)   ? (None, 26, 26, 384)       ?               0 ? block_10_expand_BN[0][0]   ?
????????????????????????????????????????????????????????????????????????????????????????????????????????????
? block_10_depthwise            ? (None, 26, 26, 384)       ?           3,456 ? block_10_expand_relu[0][0] ?
? (DepthwiseConv2D)             ?                           ?                 ?                            ?
????????????????????????????????????????????????????????????????????????????????????????????????????????????
? block_10_depthwise_BN         ? (None, 26, 26, 384)       ?           1,536 ? block_10_depthwise[0][0]   ?
? (BatchNormalization)          ?                           ?                 ?                            ?
????????????????????????????????????????????????????????????????????????????????????????????????????????????
? block_10_depthwise_relu       ? (None, 26, 26, 384)       ?               0 ? block_10_depthwise_BN[0][? ?
? (ReLU)                        ?                           ?                 ?                            ?
????????????????????????????????????????????????????????????????????????????????????????????????????????????
? block_10_project (Conv2D)     ? (None, 26, 26, 96)        ?          36,864 ? block_10_depthwise_relu[0? ?
????????????????????????????????????????????????????????????????????????????????????????????????????????????
? block_10_project_BN           ? (None, 26, 26, 96)        ?             384 ? block_10_project[0][0]     ?
? (BatchNormalization)          ?                           ?                 ?                            ?
????????????????????????????????????????????????????????????????????????????????????????????????????????????
? block_11_expand (Conv2D)      ? (None, 26, 26, 576)       ?          55,296 ? block_10_project_BN[0][0]  ?
????????????????????????????????????????????????????????????????????????????????????????????????????????????
? block_11_expand_BN            ? (None, 26, 26, 576)       ?           2,304 ? block_11_expand[0][0]      ?
? (BatchNormalization)          ?                           ?                 ?                            ?
????????????????????????????????????????????????????????????????????????????????????????????????????????????
? block_11_expand_relu (ReLU)   ? (None, 26, 26, 576)       ?               0 ? block_11_expand_BN[0][0]   ?
????????????????????????????????????????????????????????????????????????????????????????????????????????????
? block_11_depthwise            ? (None, 26, 26, 576)       ?           5,184 ? block_11_expand_relu[0][0] ?
? (DepthwiseConv2D)             ?                           ?                 ?                            ?
????????????????????????????????????????????????????????????????????????????????????????????????????????????
? block_11_depthwise_BN         ? (None, 26, 26, 576)       ?           2,304 ? block_11_depthwise[0][0]   ?
? (BatchNormalization)          ?                           ?                 ?                            ?
????????????????????????????????????????????????????????????????????????????????????????????????????????????
? block_11_depthwise_relu       ? (None, 26, 26, 576)       ?               0 ? block_11_depthwise_BN[0][? ?
? (ReLU)                        ?                           ?                 ?                            ?
????????????????????????????????????????????????????????????????????????????????????????????????????????????
? block_11_project (Conv2D)     ? (None, 26, 26, 96)        ?          55,296 ? block_11_depthwise_relu[0? ?
????????????????????????????????????????????????????????????????????????????????????????????????????????????
? block_11_project_BN           ? (None, 26, 26, 96)        ?             384 ? block_11_project[0][0]     ?
? (BatchNormalization)          ?                           ?                 ?                            ?
????????????????????????????????????????????????????????????????????????????????????????????????????????????
? block_11_add (Add)            ? (None, 26, 26, 96)        ?               0 ? block_10_project_BN[0][0], ?
?                               ?                           ?                 ? block_11_project_BN[0][0]  ?
????????????????????????????????????????????????????????????????????????????????????????????????????????????
? block_12_expand (Conv2D)      ? (None, 26, 26, 576)       ?          55,296 ? block_11_add[0][0]         ?
????????????????????????????????????????????????????????????????????????????????????????????????????????????
? block_12_expand_BN            ? (None, 26, 26, 576)       ?           2,304 ? block_12_expand[0][0]      ?
? (BatchNormalization)          ?                           ?                 ?                            ?
????????????????????????????????????????????????????????????????????????????????????????????????????????????
? block_12_expand_relu (ReLU)   ? (None, 26, 26, 576)       ?               0 ? block_12_expand_BN[0][0]   ?
????????????????????????????????????????????????????????????????????????????????????????????????????????????
? block_12_depthwise            ? (None, 26, 26, 576)       ?           5,184 ? block_12_expand_relu[0][0] ?
? (DepthwiseConv2D)             ?                           ?                 ?                            ?
????????????????????????????????????????????????????????????????????????????????????????????????????????????
? block_12_depthwise_BN         ? (None, 26, 26, 576)       ?           2,304 ? block_12_depthwise[0][0]   ?
? (BatchNormalization)          ?                           ?                 ?                            ?
????????????????????????????????????????????????????????????????????????????????????????????????????????????
? block_12_depthwise_relu       ? (None, 26, 26, 576)       ?               0 ? block_12_depthwise_BN[0][? ?
? (ReLU)                        ?                           ?                 ?                            ?
????????????????????????????????????????????????????????????????????????????????????????????????????????????
? block_12_project (Conv2D)     ? (None, 26, 26, 96)        ?          55,296 ? block_12_depthwise_relu[0? ?
????????????????????????????????????????????????????????????????????????????????????????????????????????????
? block_12_project_BN           ? (None, 26, 26, 96)        ?             384 ? block_12_project[0][0]     ?
? (BatchNormalization)          ?                           ?                 ?                            ?
????????????????????????????????????????????????????????????????????????????????????????????????????????????
? block_12_add (Add)            ? (None, 26, 26, 96)        ?               0 ? block_11_add[0][0],        ?
?                               ?                           ?                 ? block_12_project_BN[0][0]  ?
????????????????????????????????????????????????????????????????????????????????????????????????????????????
? block_13_expand (Conv2D)      ? (None, 26, 26, 576)       ?          55,296 ? block_12_add[0][0]         ?
????????????????????????????????????????????????????????????????????????????????????????????????????????????
? block_13_expand_BN            ? (None, 26, 26, 576)       ?           2,304 ? block_13_expand[0][0]      ?
? (BatchNormalization)          ?                           ?                 ?                            ?
????????????????????????????????????????????????????????????????????????????????????????????????????????????
? block_13_expand_relu (ReLU)   ? (None, 26, 26, 576)       ?               0 ? block_13_expand_BN[0][0]   ?
????????????????????????????????????????????????????????????????????????????????????????????????????????????
? block_13_pad (ZeroPadding2D)  ? (None, 27, 27, 576)       ?               0 ? block_13_expand_relu[0][0] ?
????????????????????????????????????????????????????????????????????????????????????????????????????????????
? block_13_depthwise            ? (None, 13, 13, 576)       ?           5,184 ? block_13_pad[0][0]         ?
? (DepthwiseConv2D)             ?                           ?                 ?                            ?
????????????????????????????????????????????????????????????????????????????????????????????????????????????
? block_13_depthwise_BN         ? (None, 13, 13, 576)       ?           2,304 ? block_13_depthwise[0][0]   ?
? (BatchNormalization)          ?                           ?                 ?                            ?
????????????????????????????????????????????????????????????????????????????????????????????????????????????
? block_13_depthwise_relu       ? (None, 13, 13, 576)       ?               0 ? block_13_depthwise_BN[0][? ?
? (ReLU)                        ?                           ?                 ?                            ?
????????????????????????????????????????????????????????????????????????????????????????????????????????????
? block_13_project (Conv2D)     ? (None, 13, 13, 160)       ?          92,160 ? block_13_depthwise_relu[0? ?
????????????????????????????????????????????????????????????????????????????????????????????????????????????
? block_13_project_BN           ? (None, 13, 13, 160)       ?             640 ? block_13_project[0][0]     ?
? (BatchNormalization)          ?                           ?                 ?                            ?
????????????????????????????????????????????????????????????????????????????????????????????????????????????
? block_14_expand (Conv2D)      ? (None, 13, 13, 960)       ?         153,600 ? block_13_project_BN[0][0]  ?
????????????????????????????????????????????????????????????????????????????????????????????????????????????
? block_14_expand_BN            ? (None, 13, 13, 960)       ?           3,840 ? block_14_expand[0][0]      ?
? (BatchNormalization)          ?                           ?                 ?                            ?
????????????????????????????????????????????????????????????????????????????????????????????????????????????
? block_14_expand_relu (ReLU)   ? (None, 13, 13, 960)       ?               0 ? block_14_expand_BN[0][0]   ?
????????????????????????????????????????????????????????????????????????????????????????????????????????????
? block_14_depthwise            ? (None, 13, 13, 960)       ?           8,640 ? block_14_expand_relu[0][0] ?
? (DepthwiseConv2D)             ?                           ?                 ?                            ?
????????????????????????????????????????????????????????????????????????????????????????????????????????????
? block_14_depthwise_BN         ? (None, 13, 13, 960)       ?           3,840 ? block_14_depthwise[0][0]   ?
? (BatchNormalization)          ?                           ?                 ?                            ?
????????????????????????????????????????????????????????????????????????????????????????????????????????????
? block_14_depthwise_relu       ? (None, 13, 13, 960)       ?               0 ? block_14_depthwise_BN[0][? ?
? (ReLU)                        ?                           ?                 ?                            ?
????????????????????????????????????????????????????????????????????????????????????????????????????????????
? block_14_project (Conv2D)     ? (None, 13, 13, 160)       ?         153,600 ? block_14_depthwise_relu[0? ?
????????????????????????????????????????????????????????????????????????????????????????????????????????????
? block_14_project_BN           ? (None, 13, 13, 160)       ?             640 ? block_14_project[0][0]     ?
? (BatchNormalization)          ?                           ?                 ?                            ?
????????????????????????????????????????????????????????????????????????????????????????????????????????????
? block_14_add (Add)            ? (None, 13, 13, 160)       ?               0 ? block_13_project_BN[0][0], ?
?                               ?                           ?                 ? block_14_project_BN[0][0]  ?
????????????????????????????????????????????????????????????????????????????????????????????????????????????
? block_15_expand (Conv2D)      ? (None, 13, 13, 960)       ?         153,600 ? block_14_add[0][0]         ?
????????????????????????????????????????????????????????????????????????????????????????????????????????????
? block_15_expand_BN            ? (None, 13, 13, 960)       ?           3,840 ? block_15_expand[0][0]      ?
? (BatchNormalization)          ?                           ?                 ?                            ?
????????????????????????????????????????????????????????????????????????????????????????????????????????????
? block_15_expand_relu (ReLU)   ? (None, 13, 13, 960)       ?               0 ? block_15_expand_BN[0][0]   ?
????????????????????????????????????????????????????????????????????????????????????????????????????????????
? block_15_depthwise            ? (None, 13, 13, 960)       ?           8,640 ? block_15_expand_relu[0][0] ?
? (DepthwiseConv2D)             ?                           ?                 ?                            ?
????????????????????????????????????????????????????????????????????????????????????????????????????????????
? block_15_depthwise_BN         ? (None, 13, 13, 960)       ?           3,840 ? block_15_depthwise[0][0]   ?
? (BatchNormalization)          ?                           ?                 ?                            ?
????????????????????????????????????????????????????????????????????????????????????????????????????????????
? block_15_depthwise_relu       ? (None, 13, 13, 960)       ?               0 ? block_15_depthwise_BN[0][? ?
? (ReLU)                        ?                           ?                 ?                            ?
????????????????????????????????????????????????????????????????????????????????????????????????????????????
? block_15_project (Conv2D)     ? (None, 13, 13, 160)       ?         153,600 ? block_15_depthwise_relu[0? ?
????????????????????????????????????????????????????????????????????????????????????????????????????????????
? block_15_project_BN           ? (None, 13, 13, 160)       ?             640 ? block_15_project[0][0]     ?
? (BatchNormalization)          ?                           ?                 ?                            ?
????????????????????????????????????????????????????????????????????????????????????????????????????????????
? block_15_add (Add)            ? (None, 13, 13, 160)       ?               0 ? block_14_add[0][0],        ?
?                               ?                           ?                 ? block_15_project_BN[0][0]  ?
????????????????????????????????????????????????????????????????????????????????????????????????????????????
? block_16_expand (Conv2D)      ? (None, 13, 13, 960)       ?         153,600 ? block_15_add[0][0]         ?
????????????????????????????????????????????????????????????????????????????????????????????????????????????
? block_16_expand_BN            ? (None, 13, 13, 960)       ?           3,840 ? block_16_expand[0][0]      ?
? (BatchNormalization)          ?                           ?                 ?                            ?
????????????????????????????????????????????????????????????????????????????????????????????????????????????
? block_16_expand_relu (ReLU)   ? (None, 13, 13, 960)       ?               0 ? block_16_expand_BN[0][0]   ?
????????????????????????????????????????????????????????????????????????????????????????????????????????????
? block_16_depthwise            ? (None, 13, 13, 960)       ?           8,640 ? block_16_expand_relu[0][0] ?
? (DepthwiseConv2D)             ?                           ?                 ?                            ?
????????????????????????????????????????????????????????????????????????????????????????????????????????????
? block_16_depthwise_BN         ? (None, 13, 13, 960)       ?           3,840 ? block_16_depthwise[0][0]   ?
? (BatchNormalization)          ?                           ?                 ?                            ?
????????????????????????????????????????????????????????????????????????????????????????????????????????????
? block_16_depthwise_relu       ? (None, 13, 13, 960)       ?               0 ? block_16_depthwise_BN[0][? ?
? (ReLU)                        ?                           ?                 ?                            ?
????????????????????????????????????????????????????????????????????????????????????????????????????????????
? block_16_project (Conv2D)     ? (None, 13, 13, 320)       ?         307,200 ? block_16_depthwise_relu[0? ?
????????????????????????????????????????????????????????????????????????????????????????????????????????????
? block_16_project_BN           ? (None, 13, 13, 320)       ?           1,280 ? block_16_project[0][0]     ?
? (BatchNormalization)          ?                           ?                 ?                            ?
????????????????????????????????????????????????????????????????????????????????????????????????????????????
? Conv_1 (Conv2D)               ? (None, 13, 13, 1280)      ?         409,600 ? block_16_project_BN[0][0]  ?
????????????????????????????????????????????????????????????????????????????????????????????????????????????
? Conv_1_bn                     ? (None, 13, 13, 1280)      ?           5,120 ? Conv_1[0][0]               ?
? (BatchNormalization)          ?                           ?                 ?                            ?
????????????????????????????????????????????????????????????????????????????????????????????????????????????
? out_relu (ReLU)               ? (None, 13, 13, 1280)      ?               0 ? Conv_1_bn[0][0]            ?
????????????????????????????????????????????????????????????????????????????????????????????????????????????
? conv2d (Conv2D)               ? (None, 13, 13, 256)       ?         327,936 ? out_relu[0][0]             ?
????????????????????????????????????????????????????????????????????????????????????????????????????????????
? conv2d_3 (Conv2D)             ? (None, 13, 13, 256)       ?         590,080 ? conv2d[0][0]               ?
????????????????????????????????????????????????????????????????????????????????????????????????????????????
? batch_normalization           ? (None, 13, 13, 256)       ?           1,024 ? conv2d_3[0][0]             ?
? (BatchNormalization)          ?                           ?                 ?                            ?
????????????????????????????????????????????????????????????????????????????????????????????????????????????
? conv2d_4 (Conv2D)             ? (None, 13, 13, 17)        ?           4,369 ? batch_normalization[0][0]  ?
????????????????????????????????????????????????????????????????????????????????????????????????????????????
 Total params: 3,181,393 (12.14 MB)
 Trainable params: 3,146,769 (12.00 MB)
 Non-trainable params: 34,624 (135.25 KB)

[2025-05-11 01:49:21] Model output shape: (1, 13, 13, 17)
[2025-05-11 01:49:21] Starting model training...
[2025-05-11 01:49:21] Performing test training with 1 batch to verify setup...
[2025-05-11 01:49:39] Test training successful, proceeding with full training...
[2025-05-11 01:49:39] === Training Started ===
[2025-05-11 01:49:39] Model structure:
[2025-05-11 01:49:39] Total layers: 158
[2025-05-11 01:49:39] Total parameters: 3,181,393
[2025-05-11 01:49:39] Trainable parameters: 3,146,769
[2025-05-11 01:49:39] Non-trainable parameters: 34,624
[2025-05-11 01:49:41] Batch 0 - Loss: 123.966187
[2025-05-11 01:49:49] Batch 5 - Loss: 121.490761
[2025-05-11 01:49:57] Batch 10 - Loss: 118.999535
[2025-05-11 01:50:04] Batch 15 - Loss: 117.774460
[2025-05-11 01:50:12] Batch 20 - Loss: 116.316383
[2025-05-11 01:50:20] Batch 25 - Loss: 115.463539
[2025-05-11 01:50:28] Batch 30 - Loss: 114.439316
[2025-05-11 01:50:35] Batch 35 - Loss: 113.476097
[2025-05-11 01:50:43] Batch 40 - Loss: 112.469940
[2025-05-11 01:50:51] Batch 45 - Loss: 111.606857
[2025-05-11 01:50:58] Batch 50 - Loss: 110.747177
[2025-05-11 01:51:06] Batch 55 - Loss: 110.049507
[2025-05-11 01:51:14] Batch 60 - Loss: 109.247009
[2025-05-11 01:51:22] Batch 65 - Loss: 108.514816
[2025-05-11 01:51:29] Batch 70 - Loss: 107.816582
[2025-05-11 01:51:37] Batch 75 - Loss: 107.182426
[2025-05-11 01:51:45] Batch 80 - Loss: 106.432472
[2025-05-11 01:51:52] Batch 85 - Loss: 105.813454
[2025-05-11 01:52:00] Batch 90 - Loss: 105.110909
[2025-05-11 01:52:08] Batch 95 - Loss: 104.502403
[2025-05-11 01:52:19] === Epoch 1/100 Summary ===
Training Loss: 103.950592
Validation Loss: 78.614487
Learning Rate: 0.00001000
[2025-05-11 01:52:21] Batch 0 - Loss: 93.355316
[2025-05-11 01:52:28] Batch 5 - Loss: 90.717255
[2025-05-11 01:52:36] Batch 10 - Loss: 89.724113
[2025-05-11 01:52:44] Batch 15 - Loss: 89.878273
[2025-05-11 01:52:51] Batch 20 - Loss: 89.213768
[2025-05-11 01:52:59] Batch 25 - Loss: 89.090462
[2025-05-11 01:53:07] Batch 30 - Loss: 88.586266
[2025-05-11 01:53:14] Batch 35 - Loss: 88.207085
[2025-05-11 01:53:22] Batch 40 - Loss: 87.841599
[2025-05-11 01:53:29] Batch 45 - Loss: 87.440842
[2025-05-11 01:53:37] Batch 50 - Loss: 86.924324
[2025-05-11 01:53:45] Batch 55 - Loss: 86.395134
[2025-05-11 01:53:53] Batch 60 - Loss: 85.980324
[2025-05-11 01:54:00] Batch 65 - Loss: 85.505936
[2025-05-11 01:54:08] Batch 70 - Loss: 85.093567
[2025-05-11 01:54:16] Batch 75 - Loss: 84.572525
[2025-05-11 01:54:23] Batch 80 - Loss: 84.116859
[2025-05-11 01:54:31] Batch 85 - Loss: 83.673027
[2025-05-11 01:54:39] Batch 90 - Loss: 83.157799
[2025-05-11 01:54:46] Batch 95 - Loss: 82.707878
[2025-05-11 01:54:58] === Epoch 2/100 Summary ===
Training Loss: 82.379715
Validation Loss: 77.519012
Learning Rate: 0.00001000
[2025-05-11 01:54:59] Batch 0 - Loss: 73.666229
[2025-05-11 01:55:07] Batch 5 - Loss: 72.220535
[2025-05-11 01:55:15] Batch 10 - Loss: 71.979248
[2025-05-11 01:55:22] Batch 15 - Loss: 72.247589
[2025-05-11 01:55:30] Batch 20 - Loss: 71.314651
[2025-05-11 01:55:38] Batch 25 - Loss: 70.958359
[2025-05-11 01:55:45] Batch 30 - Loss: 70.586945
[2025-05-11 01:55:53] Batch 35 - Loss: 70.118095
[2025-05-11 01:56:01] Batch 40 - Loss: 69.702911
[2025-05-11 01:56:08] Batch 45 - Loss: 69.240494
[2025-05-11 01:56:16] Batch 50 - Loss: 68.950287
[2025-05-11 01:56:24] Batch 55 - Loss: 68.371140
[2025-05-11 01:56:31] Batch 60 - Loss: 67.944046
[2025-05-11 01:56:39] Batch 65 - Loss: 67.521385
[2025-05-11 01:56:47] Batch 70 - Loss: 67.142960
[2025-05-11 01:56:54] Batch 75 - Loss: 66.669167
[2025-05-11 01:57:02] Batch 80 - Loss: 66.324654
[2025-05-11 01:57:10] Batch 85 - Loss: 65.943657
[2025-05-11 01:57:17] Batch 90 - Loss: 65.580742
[2025-05-11 01:57:25] Batch 95 - Loss: 65.228249
[2025-05-11 01:57:36] === Epoch 3/100 Summary ===
Training Loss: 64.928787
Validation Loss: 71.429558
Learning Rate: 0.00001000
[2025-05-11 01:57:38] Batch 0 - Loss: 57.460476
[2025-05-11 01:57:46] Batch 5 - Loss: 57.204407
[2025-05-11 01:57:53] Batch 10 - Loss: 57.041039
[2025-05-11 01:58:01] Batch 15 - Loss: 57.140331
[2025-05-11 01:58:08] Batch 20 - Loss: 57.004871
[2025-05-11 01:58:16] Batch 25 - Loss: 56.909237
[2025-05-11 01:58:24] Batch 30 - Loss: 56.588436
[2025-05-11 01:58:32] Batch 35 - Loss: 56.310997
[2025-05-11 01:58:38] === Epoch 4/100 Summary ===
Training Loss: 56.304314
Validation Loss: 66.726715
Learning Rate: 0.00001000
[2025-05-11 01:58:40] Batch 0 - Loss: 56.154984
[2025-05-11 01:58:47] Batch 5 - Loss: 55.162601
[2025-05-11 01:58:55] Batch 10 - Loss: 55.198452
[2025-05-11 01:59:03] Batch 15 - Loss: 55.404312
[2025-05-11 01:59:10] Batch 20 - Loss: 55.448162
[2025-05-11 01:59:18] Batch 25 - Loss: 55.203945
[2025-05-11 01:59:26] Batch 30 - Loss: 55.005772
[2025-05-11 01:59:33] Batch 35 - Loss: 54.955643
[2025-05-11 01:59:41] Batch 40 - Loss: 54.853725
[2025-05-11 01:59:49] Batch 45 - Loss: 54.792969
[2025-05-11 01:59:56] Batch 50 - Loss: 54.708828
[2025-05-11 02:00:04] Batch 55 - Loss: 54.618599
[2025-05-11 02:00:12] Batch 60 - Loss: 54.459656
[2025-05-11 02:00:19] Batch 65 - Loss: 54.314926
[2025-05-11 02:00:27] Batch 70 - Loss: 54.350636
[2025-05-11 02:00:34] Batch 75 - Loss: 54.255135
[2025-05-11 02:00:42] Batch 80 - Loss: 54.130817
[2025-05-11 02:00:50] Batch 85 - Loss: 54.009880
[2025-05-11 02:00:57] Batch 90 - Loss: 53.928780
[2025-05-11 02:01:05] Batch 95 - Loss: 53.822964
[2025-05-11 02:01:17] === Epoch 5/100 Summary ===
Training Loss: 53.785492
Validation Loss: 54.976204
Learning Rate: 0.00001000
[2025-05-11 02:01:17] Generating detection visualizations for epoch 5...
[2025-05-11 02:01:17] Saved visualization to trained_models/custom_detector\visualizations\epoch_5\sample_1.png
[2025-05-11 02:01:18] Saved visualization to trained_models/custom_detector\visualizations\epoch_5\sample_2.png
[2025-05-11 02:01:18] Saved visualization to trained_models/custom_detector\visualizations\epoch_5\sample_3.png
[2025-05-11 02:01:20] Batch 0 - Loss: 50.644684
[2025-05-11 02:01:27] Batch 5 - Loss: 51.058788
[2025-05-11 02:01:35] Batch 10 - Loss: 51.291439
[2025-05-11 02:01:42] Batch 15 - Loss: 51.434250
[2025-05-11 02:01:50] Batch 20 - Loss: 51.500755
[2025-05-11 02:01:58] Batch 25 - Loss: 51.609879
[2025-05-11 02:02:05] Batch 30 - Loss: 51.629150
[2025-05-11 02:02:13] Batch 35 - Loss: 51.651955
[2025-05-11 02:02:21] Batch 40 - Loss: 51.521030
[2025-05-11 02:02:28] Batch 45 - Loss: 51.415443
[2025-05-11 02:02:36] Batch 50 - Loss: 51.443733
[2025-05-11 02:02:44] Batch 55 - Loss: 51.372536
[2025-05-11 02:02:51] Batch 60 - Loss: 51.392235
[2025-05-11 02:02:59] Batch 65 - Loss: 51.347191
[2025-05-11 02:03:06] Batch 70 - Loss: 51.325787
[2025-05-11 02:03:14] Batch 75 - Loss: 51.312733
[2025-05-11 02:03:22] Batch 80 - Loss: 51.240845
[2025-05-11 02:03:30] Batch 85 - Loss: 51.256268
[2025-05-11 02:03:37] Batch 90 - Loss: 51.183327
[2025-05-11 02:03:45] Batch 95 - Loss: 51.126617
[2025-05-11 02:03:56] === Epoch 6/100 Summary ===
Training Loss: 51.058914
Validation Loss: 49.456612
Learning Rate: 0.00001000
[2025-05-11 02:03:58] Batch 0 - Loss: 49.007641
[2025-05-11 02:04:06] Batch 5 - Loss: 49.454441
[2025-05-11 02:04:13] Batch 10 - Loss: 49.817101
[2025-05-11 02:04:21] Batch 15 - Loss: 50.014671
[2025-05-11 02:04:29] Batch 20 - Loss: 49.994308
[2025-05-11 02:04:36] Batch 25 - Loss: 50.001465
[2025-05-11 02:04:44] Batch 30 - Loss: 50.022560
[2025-05-11 02:04:52] Batch 35 - Loss: 50.054832
[2025-05-11 02:04:59] Batch 40 - Loss: 50.139446
[2025-05-11 02:05:07] Batch 45 - Loss: 50.054310
[2025-05-11 02:05:15] Batch 50 - Loss: 49.919975
[2025-05-11 02:05:22] Batch 55 - Loss: 49.887936
[2025-05-11 02:05:30] Batch 60 - Loss: 49.861828
[2025-05-11 02:05:38] Batch 65 - Loss: 49.858356
[2025-05-11 02:05:45] Batch 70 - Loss: 49.850483
[2025-05-11 02:05:53] Batch 75 - Loss: 49.811249
[2025-05-11 02:06:00] Batch 80 - Loss: 49.731792
[2025-05-11 02:06:08] Batch 85 - Loss: 49.672092
[2025-05-11 02:06:16] Batch 90 - Loss: 49.684174
[2025-05-11 02:06:23] Batch 95 - Loss: 49.695465
[2025-05-11 02:06:35] === Epoch 7/100 Summary ===
Training Loss: 49.658989
Validation Loss: 47.617805
Learning Rate: 0.00001000
[2025-05-11 02:06:36] Batch 0 - Loss: 50.681435
[2025-05-11 02:06:44] Batch 5 - Loss: 48.437878
[2025-05-11 02:06:52] Batch 10 - Loss: 48.792652
[2025-05-11 02:07:00] Batch 15 - Loss: 48.764965
[2025-05-11 02:07:07] Batch 20 - Loss: 48.887775
[2025-05-11 02:07:15] Batch 25 - Loss: 48.812439
[2025-05-11 02:07:22] Batch 30 - Loss: 48.706921
[2025-05-11 02:07:30] Batch 35 - Loss: 48.528076
[2025-05-11 02:07:36] === Epoch 8/100 Summary ===
Training Loss: 48.553024
Validation Loss: 47.329231
Learning Rate: 0.00001000
[2025-05-11 02:07:38] Batch 0 - Loss: 47.391830
[2025-05-11 02:07:46] Batch 5 - Loss: 49.418030
[2025-05-11 02:07:53] Batch 10 - Loss: 49.217564
[2025-05-11 02:08:01] Batch 15 - Loss: 48.950649
[2025-05-11 02:08:09] Batch 20 - Loss: 48.967106
[2025-05-11 02:08:17] Batch 25 - Loss: 49.031780
[2025-05-11 02:08:24] Batch 30 - Loss: 49.088341
[2025-05-11 02:08:32] Batch 35 - Loss: 49.036648
[2025-05-11 02:08:40] Batch 40 - Loss: 49.113293
[2025-05-11 02:08:47] Batch 45 - Loss: 48.972118
[2025-05-11 02:08:55] Batch 50 - Loss: 49.084003
[2025-05-11 02:09:03] Batch 55 - Loss: 48.875916
[2025-05-11 02:09:10] Batch 60 - Loss: 48.865143
[2025-05-11 02:09:18] Batch 65 - Loss: 48.832413
[2025-05-11 02:09:26] Batch 70 - Loss: 48.778610
[2025-05-11 02:09:33] Batch 75 - Loss: 48.708961
[2025-05-11 02:09:41] Batch 80 - Loss: 48.746662
[2025-05-11 02:09:49] Batch 85 - Loss: 48.649899
[2025-05-11 02:09:57] Batch 90 - Loss: 48.644001
[2025-05-11 02:10:05] Batch 95 - Loss: 48.588215
[2025-05-11 02:10:16] === Epoch 9/100 Summary ===
Training Loss: 48.551903
Validation Loss: 46.825230
Learning Rate: 0.00001000
[2025-05-11 02:10:18] Batch 0 - Loss: 49.236713
[2025-05-11 02:10:25] Batch 5 - Loss: 46.931137
[2025-05-11 02:10:33] Batch 10 - Loss: 47.316490
[2025-05-11 02:10:41] Batch 15 - Loss: 47.669899
[2025-05-11 02:10:48] Batch 20 - Loss: 47.575539
[2025-05-11 02:10:56] Batch 25 - Loss: 47.498013
[2025-05-11 02:11:04] Batch 30 - Loss: 47.461353
[2025-05-11 02:11:11] Batch 35 - Loss: 47.471790
[2025-05-11 02:11:19] Batch 40 - Loss: 47.467838
[2025-05-11 02:11:26] Batch 45 - Loss: 47.398205
[2025-05-11 02:11:34] Batch 50 - Loss: 47.371300
[2025-05-11 02:11:42] Batch 55 - Loss: 47.372429
[2025-05-11 02:11:49] Batch 60 - Loss: 47.468239
[2025-05-11 02:11:57] Batch 65 - Loss: 47.398930
[2025-05-11 02:12:05] Batch 70 - Loss: 47.350368
[2025-05-11 02:12:12] Batch 75 - Loss: 47.395462
[2025-05-11 02:12:20] Batch 80 - Loss: 47.402565
[2025-05-11 02:12:28] Batch 85 - Loss: 47.454330
[2025-05-11 02:12:35] Batch 90 - Loss: 47.444916
[2025-05-11 02:12:43] Batch 95 - Loss: 47.420444
[2025-05-11 02:12:54] === Epoch 10/100 Summary ===
Training Loss: 47.419708
Validation Loss: 46.163620
Learning Rate: 0.00001000
[2025-05-11 02:12:54] Generating detection visualizations for epoch 10...
[2025-05-11 02:12:55] Saved visualization to trained_models/custom_detector\visualizations\epoch_10\sample_1.png
[2025-05-11 02:12:55] Saved visualization to trained_models/custom_detector\visualizations\epoch_10\sample_2.png
[2025-05-11 02:12:55] Saved visualization to trained_models/custom_detector\visualizations\epoch_10\sample_3.png
[2025-05-11 02:12:57] Batch 0 - Loss: 48.802635
[2025-05-11 02:13:05] Batch 5 - Loss: 47.352772
[2025-05-11 02:13:12] Batch 10 - Loss: 46.710209
[2025-05-11 02:13:20] Batch 15 - Loss: 47.226112
[2025-05-11 02:13:27] Batch 20 - Loss: 47.214924
[2025-05-11 02:13:35] Batch 25 - Loss: 47.138695
[2025-05-11 02:13:43] Batch 30 - Loss: 47.118797
[2025-05-11 02:13:50] Batch 35 - Loss: 47.090199
[2025-05-11 02:13:58] Batch 40 - Loss: 46.990742
[2025-05-11 02:14:06] Batch 45 - Loss: 46.915707
[2025-05-11 02:14:14] Batch 50 - Loss: 46.885136
[2025-05-11 02:14:21] Batch 55 - Loss: 46.860508
[2025-05-11 02:14:29] Batch 60 - Loss: 46.788006
[2025-05-11 02:14:37] Batch 65 - Loss: 46.711628
[2025-05-11 02:14:44] Batch 70 - Loss: 46.583920
[2025-05-11 02:14:52] Batch 75 - Loss: 46.560921
[2025-05-11 02:14:59] Batch 80 - Loss: 46.510551
[2025-05-11 02:15:07] Batch 85 - Loss: 46.454441
[2025-05-11 02:15:15] Batch 90 - Loss: 46.420570
[2025-05-11 02:15:23] Batch 95 - Loss: 46.437725
[2025-05-11 02:15:34] === Epoch 11/100 Summary ===
Training Loss: 46.422279
Validation Loss: 45.520500
Learning Rate: 0.00001000
[2025-05-11 02:15:36] Batch 0 - Loss: 45.785572
[2025-05-11 02:15:43] Batch 5 - Loss: 45.103985
[2025-05-11 02:15:51] Batch 10 - Loss: 45.240967
[2025-05-11 02:15:58] Batch 15 - Loss: 45.421139
[2025-05-11 02:16:06] Batch 20 - Loss: 45.544441
[2025-05-11 02:16:14] Batch 25 - Loss: 45.357918
[2025-05-11 02:16:21] Batch 30 - Loss: 45.263451
[2025-05-11 02:16:29] Batch 35 - Loss: 45.244671
[2025-05-11 02:16:35] === Epoch 12/100 Summary ===
Training Loss: 45.219143
Validation Loss: 45.456215
Learning Rate: 0.00001000
[2025-05-11 02:16:37] Batch 0 - Loss: 44.960690
[2025-05-11 02:16:45] Batch 5 - Loss: 46.074604
[2025-05-11 02:16:52] Batch 10 - Loss: 45.930538
[2025-05-11 02:17:00] Batch 15 - Loss: 45.986992
[2025-05-11 02:17:08] Batch 20 - Loss: 45.953350
[2025-05-11 02:17:15] Batch 25 - Loss: 46.042751
[2025-05-11 02:17:23] Batch 30 - Loss: 46.180244
[2025-05-11 02:17:31] Batch 35 - Loss: 46.141258
[2025-05-11 02:17:38] Batch 40 - Loss: 46.023880
[2025-05-11 02:17:46] Batch 45 - Loss: 46.079967
[2025-05-11 02:17:53] Batch 50 - Loss: 46.040195
[2025-05-11 02:18:01] Batch 55 - Loss: 45.921242
[2025-05-11 02:18:09] Batch 60 - Loss: 45.901012
[2025-05-11 02:18:16] Batch 65 - Loss: 45.808327
[2025-05-11 02:18:24] Batch 70 - Loss: 45.722336
[2025-05-11 02:18:32] Batch 75 - Loss: 45.614376
[2025-05-11 02:18:40] Batch 80 - Loss: 45.545475
[2025-05-11 02:18:47] Batch 85 - Loss: 45.491795
[2025-05-11 02:18:55] Batch 90 - Loss: 45.458439
[2025-05-11 02:19:03] Batch 95 - Loss: 45.378708
[2025-05-11 02:19:14] === Epoch 13/100 Summary ===
Training Loss: 45.331158
Validation Loss: 44.418236
Learning Rate: 0.00001000
[2025-05-11 02:19:16] Batch 0 - Loss: 45.187206
[2025-05-11 02:19:23] Batch 5 - Loss: 44.696041
[2025-05-11 02:19:31] Batch 10 - Loss: 44.409214
[2025-05-11 02:19:39] Batch 15 - Loss: 44.949207
[2025-05-11 02:19:46] Batch 20 - Loss: 44.871758
[2025-05-11 02:19:54] Batch 25 - Loss: 44.778885
[2025-05-11 02:20:02] Batch 30 - Loss: 44.768272
[2025-05-11 02:20:09] Batch 35 - Loss: 44.627285
[2025-05-11 02:20:17] Batch 40 - Loss: 44.673664
[2025-05-11 02:20:25] Batch 45 - Loss: 44.630367
[2025-05-11 02:20:32] Batch 50 - Loss: 44.735420
[2025-05-11 02:20:40] Batch 55 - Loss: 44.863400
[2025-05-11 02:20:48] Batch 60 - Loss: 44.857990
[2025-05-11 02:20:55] Batch 65 - Loss: 44.825008
[2025-05-11 02:21:03] Batch 70 - Loss: 44.782963
[2025-05-11 02:21:11] Batch 75 - Loss: 44.736732
[2025-05-11 02:21:18] Batch 80 - Loss: 44.697964
[2025-05-11 02:21:26] Batch 85 - Loss: 44.707600
[2025-05-11 02:21:34] Batch 90 - Loss: 44.732613
[2025-05-11 02:21:41] Batch 95 - Loss: 44.700550
[2025-05-11 02:21:53] === Epoch 14/100 Summary ===
Training Loss: 44.653053
Validation Loss: 43.843140
Learning Rate: 0.00001000
[2025-05-11 02:21:54] Batch 0 - Loss: 42.331673
[2025-05-11 02:22:02] Batch 5 - Loss: 43.476318
[2025-05-11 02:22:09] Batch 10 - Loss: 43.526249
[2025-05-11 02:22:17] Batch 15 - Loss: 43.603058
[2025-05-11 02:22:25] Batch 20 - Loss: 43.361412
[2025-05-11 02:22:32] Batch 25 - Loss: 43.615330
[2025-05-11 02:22:40] Batch 30 - Loss: 43.757423
[2025-05-11 02:22:48] Batch 35 - Loss: 43.881882
[2025-05-11 02:22:55] Batch 40 - Loss: 43.726299
[2025-05-11 02:23:03] Batch 45 - Loss: 43.641018
[2025-05-11 02:23:10] Batch 50 - Loss: 43.584759
[2025-05-11 02:23:18] Batch 55 - Loss: 43.535709
[2025-05-11 02:23:26] Batch 60 - Loss: 43.502716
[2025-05-11 02:23:33] Batch 65 - Loss: 43.556999
[2025-05-11 02:23:41] Batch 70 - Loss: 43.495411
[2025-05-11 02:23:49] Batch 75 - Loss: 43.407288
[2025-05-11 02:23:56] Batch 80 - Loss: 43.359085
[2025-05-11 02:24:04] Batch 85 - Loss: 43.341000
[2025-05-11 02:24:12] Batch 90 - Loss: 43.326069
[2025-05-11 02:24:19] Batch 95 - Loss: 43.280346
[2025-05-11 02:24:31] === Epoch 15/100 Summary ===
Training Loss: 43.247726
Validation Loss: 43.478607
Learning Rate: 0.00001000
[2025-05-11 02:24:31] Generating detection visualizations for epoch 15...
[2025-05-11 02:24:31] Saved visualization to trained_models/custom_detector\visualizations\epoch_15\sample_1.png
[2025-05-11 02:24:31] Saved visualization to trained_models/custom_detector\visualizations\epoch_15\sample_2.png
[2025-05-11 02:24:32] Saved visualization to trained_models/custom_detector\visualizations\epoch_15\sample_3.png
[2025-05-11 02:24:33] Batch 0 - Loss: 43.310219
[2025-05-11 02:24:41] Batch 5 - Loss: 42.220417
[2025-05-11 02:24:48] Batch 10 - Loss: 41.898785
[2025-05-11 02:24:56] Batch 15 - Loss: 41.844513
[2025-05-11 02:25:04] Batch 20 - Loss: 41.803226
[2025-05-11 02:25:11] Batch 25 - Loss: 41.690586
[2025-05-11 02:25:19] Batch 30 - Loss: 41.775356
[2025-05-11 02:25:27] Batch 35 - Loss: 41.851120
[2025-05-11 02:25:33] === Epoch 16/100 Summary ===
Training Loss: 41.845032
Validation Loss: 43.072479
Learning Rate: 0.00001000
[2025-05-11 02:25:35] Batch 0 - Loss: 43.427547
[2025-05-11 02:25:42] Batch 5 - Loss: 42.878109
[2025-05-11 02:25:50] Batch 10 - Loss: 42.786285
[2025-05-11 02:25:58] Batch 15 - Loss: 42.584389
[2025-05-11 02:26:05] Batch 20 - Loss: 42.895248
[2025-05-11 02:26:13] Batch 25 - Loss: 42.916985
[2025-05-11 02:26:21] Batch 30 - Loss: 43.003292
[2025-05-11 02:26:28] Batch 35 - Loss: 42.957047
[2025-05-11 02:26:36] Batch 40 - Loss: 43.034595
[2025-05-11 02:26:44] Batch 45 - Loss: 43.156326
[2025-05-11 02:26:51] Batch 50 - Loss: 43.049179
[2025-05-11 02:26:59] Batch 55 - Loss: 43.005310
[2025-05-11 02:27:06] Batch 60 - Loss: 43.009205
[2025-05-11 02:27:14] Batch 65 - Loss: 42.915981
[2025-05-11 02:27:22] Batch 70 - Loss: 42.850304
[2025-05-11 02:27:29] Batch 75 - Loss: 42.710289
[2025-05-11 02:27:37] Batch 80 - Loss: 42.624779
[2025-05-11 02:27:45] Batch 85 - Loss: 42.559647
[2025-05-11 02:27:52] Batch 90 - Loss: 42.483517
[2025-05-11 02:28:00] Batch 95 - Loss: 42.433311
[2025-05-11 02:28:11] === Epoch 17/100 Summary ===
Training Loss: 42.397633
Validation Loss: 42.292736
Learning Rate: 0.00001000
[2025-05-11 02:28:13] Batch 0 - Loss: 40.852791
[2025-05-11 02:28:20] Batch 5 - Loss: 41.558273
[2025-05-11 02:28:28] Batch 10 - Loss: 41.166344
[2025-05-11 02:28:36] Batch 15 - Loss: 41.121933
[2025-05-11 02:28:43] Batch 20 - Loss: 41.125134
[2025-05-11 02:28:51] Batch 25 - Loss: 41.187214
[2025-05-11 02:28:59] Batch 30 - Loss: 41.071438
[2025-05-11 02:29:06] Batch 35 - Loss: 41.040916
[2025-05-11 02:29:14] Batch 40 - Loss: 41.087761
[2025-05-11 02:29:22] Batch 45 - Loss: 41.220131
[2025-05-11 02:29:29] Batch 50 - Loss: 41.284130
[2025-05-11 02:29:37] Batch 55 - Loss: 41.262142
[2025-05-11 02:29:45] Batch 60 - Loss: 41.234184
[2025-05-11 02:29:52] Batch 65 - Loss: 41.330097
[2025-05-11 02:30:00] Batch 70 - Loss: 41.354374
[2025-05-11 02:30:08] Batch 75 - Loss: 41.284443
[2025-05-11 02:30:15] Batch 80 - Loss: 41.207520
[2025-05-11 02:30:23] Batch 85 - Loss: 41.150806
[2025-05-11 02:30:31] Batch 90 - Loss: 41.093246
[2025-05-11 02:30:38] Batch 95 - Loss: 41.128262
[2025-05-11 02:30:50] === Epoch 18/100 Summary ===
Training Loss: 41.090164
Validation Loss: 41.594887
Learning Rate: 0.00001000
[2025-05-11 02:30:51] Batch 0 - Loss: 40.560543
[2025-05-11 02:30:59] Batch 5 - Loss: 41.508263
[2025-05-11 02:31:06] Batch 10 - Loss: 41.231499
[2025-05-11 02:31:14] Batch 15 - Loss: 40.803600
[2025-05-11 02:31:22] Batch 20 - Loss: 40.950962
[2025-05-11 02:31:29] Batch 25 - Loss: 40.859989
[2025-05-11 02:31:37] Batch 30 - Loss: 40.721153
[2025-05-11 02:31:45] Batch 35 - Loss: 40.723373
[2025-05-11 02:31:53] Batch 40 - Loss: 40.582973
[2025-05-11 02:32:00] Batch 45 - Loss: 40.533482
[2025-05-11 02:32:08] Batch 50 - Loss: 40.642323
[2025-05-11 02:32:15] Batch 55 - Loss: 40.546215
[2025-05-11 02:32:23] Batch 60 - Loss: 40.471535
[2025-05-11 02:32:31] Batch 65 - Loss: 40.500385
[2025-05-11 02:32:38] Batch 70 - Loss: 40.386421
[2025-05-11 02:32:46] Batch 75 - Loss: 40.363941
[2025-05-11 02:32:54] Batch 80 - Loss: 40.355812
[2025-05-11 02:33:01] Batch 85 - Loss: 40.304794
[2025-05-11 02:33:09] Batch 90 - Loss: 40.261818
[2025-05-11 02:33:17] Batch 95 - Loss: 40.205891
[2025-05-11 02:33:28] === Epoch 19/100 Summary ===
Training Loss: 40.145527
Validation Loss: 41.010963
Learning Rate: 0.00001000
[2025-05-11 02:33:30] Batch 0 - Loss: 38.099297
[2025-05-11 02:33:37] Batch 5 - Loss: 39.118759
[2025-05-11 02:33:45] Batch 10 - Loss: 38.660660
[2025-05-11 02:33:53] Batch 15 - Loss: 38.472904
[2025-05-11 02:34:00] Batch 20 - Loss: 38.582558
[2025-05-11 02:34:08] Batch 25 - Loss: 38.750633
[2025-05-11 02:34:15] Batch 30 - Loss: 38.770367
[2025-05-11 02:34:23] Batch 35 - Loss: 38.890469
[2025-05-11 02:34:29] === Epoch 20/100 Summary ===
Training Loss: 38.868580
Validation Loss: 40.516430
Learning Rate: 0.00001000
[2025-05-11 02:34:29] Generating detection visualizations for epoch 20...
[2025-05-11 02:34:30] Saved visualization to trained_models/custom_detector\visualizations\epoch_20\sample_1.png
[2025-05-11 02:34:30] Saved visualization to trained_models/custom_detector\visualizations\epoch_20\sample_2.png
[2025-05-11 02:34:30] Saved visualization to trained_models/custom_detector\visualizations\epoch_20\sample_3.png
[2025-05-11 02:34:32] Batch 0 - Loss: 37.938496
[2025-05-11 02:34:40] Batch 5 - Loss: 39.314808
[2025-05-11 02:34:48] Batch 10 - Loss: 39.255276
[2025-05-11 02:34:55] Batch 15 - Loss: 39.612976
[2025-05-11 02:35:03] Batch 20 - Loss: 39.578369
[2025-05-11 02:35:11] Batch 25 - Loss: 39.794209
[2025-05-11 02:35:19] Batch 30 - Loss: 39.637062
[2025-05-11 02:35:27] Batch 35 - Loss: 39.476677
[2025-05-11 02:35:35] Batch 40 - Loss: 39.515530
[2025-05-11 02:35:42] Batch 45 - Loss: 39.496441
[2025-05-11 02:35:50] Batch 50 - Loss: 39.418491
[2025-05-11 02:35:58] Batch 55 - Loss: 39.502911
[2025-05-11 02:36:05] Batch 60 - Loss: 39.521599
[2025-05-11 02:36:13] Batch 65 - Loss: 39.467278
[2025-05-11 02:36:21] Batch 70 - Loss: 39.415375
[2025-05-11 02:36:29] Batch 75 - Loss: 39.360485
[2025-05-11 02:36:36] Batch 80 - Loss: 39.379581
[2025-05-11 02:36:44] Batch 85 - Loss: 39.339306
[2025-05-11 02:36:52] Batch 90 - Loss: 39.257545
[2025-05-11 02:37:00] Batch 95 - Loss: 39.213421
[2025-05-11 02:37:12] === Epoch 21/100 Summary ===
Training Loss: 39.161476
Validation Loss: 39.585327
Learning Rate: 0.00001000
[2025-05-11 02:37:13] Batch 0 - Loss: 37.295715
[2025-05-11 02:37:21] Batch 5 - Loss: 38.314472
[2025-05-11 02:37:28] Batch 10 - Loss: 37.973759
[2025-05-11 02:37:36] Batch 15 - Loss: 37.988773
[2025-05-11 02:37:44] Batch 20 - Loss: 38.009014
[2025-05-11 02:37:52] Batch 25 - Loss: 37.827915
[2025-05-11 02:38:00] Batch 30 - Loss: 37.912365
[2025-05-11 02:38:07] Batch 35 - Loss: 37.969765
[2025-05-11 02:38:15] Batch 40 - Loss: 37.939941
[2025-05-11 02:38:23] Batch 45 - Loss: 38.000137
[2025-05-11 02:38:30] Batch 50 - Loss: 38.030052
[2025-05-11 02:38:38] Batch 55 - Loss: 37.970249
[2025-05-11 02:38:46] Batch 60 - Loss: 38.019203
[2025-05-11 02:38:53] Batch 65 - Loss: 38.081032
[2025-05-11 02:39:01] Batch 70 - Loss: 38.071007
[2025-05-11 02:39:09] Batch 75 - Loss: 38.072632
[2025-05-11 02:39:16] Batch 80 - Loss: 38.048077
[2025-05-11 02:39:24] Batch 85 - Loss: 38.058762
[2025-05-11 02:39:32] Batch 90 - Loss: 38.013481
[2025-05-11 02:39:39] Batch 95 - Loss: 38.026714
[2025-05-11 02:39:51] === Epoch 22/100 Summary ===
Training Loss: 38.015114
Validation Loss: 38.813133
Learning Rate: 0.00001000
[2025-05-11 02:39:52] Batch 0 - Loss: 35.713638
[2025-05-11 02:40:00] Batch 5 - Loss: 37.678478
[2025-05-11 02:40:08] Batch 10 - Loss: 37.607914
[2025-05-11 02:40:15] Batch 15 - Loss: 37.791451
[2025-05-11 02:40:23] Batch 20 - Loss: 37.630589
[2025-05-11 02:40:31] Batch 25 - Loss: 37.482704
[2025-05-11 02:40:38] Batch 30 - Loss: 37.516876
[2025-05-11 02:40:46] Batch 35 - Loss: 37.338230
[2025-05-11 02:40:54] Batch 40 - Loss: 37.204163
[2025-05-11 02:41:01] Batch 45 - Loss: 37.198166
[2025-05-11 02:41:09] Batch 50 - Loss: 37.213810
[2025-05-11 02:41:17] Batch 55 - Loss: 37.158928
[2025-05-11 02:41:25] Batch 60 - Loss: 37.126617
[2025-05-11 02:41:32] Batch 65 - Loss: 37.092854
[2025-05-11 02:41:40] Batch 70 - Loss: 37.081413
[2025-05-11 02:41:48] Batch 75 - Loss: 37.061207
[2025-05-11 02:41:55] Batch 80 - Loss: 36.970005
[2025-05-11 02:42:03] Batch 85 - Loss: 36.899410
[2025-05-11 02:42:11] Batch 90 - Loss: 36.905666
[2025-05-11 02:42:18] Batch 95 - Loss: 36.891026
[2025-05-11 02:42:30] === Epoch 23/100 Summary ===
Training Loss: 36.875931
Validation Loss: 38.220100
Learning Rate: 0.00001000
[2025-05-11 02:42:31] Batch 0 - Loss: 34.922516
[2025-05-11 02:42:39] Batch 5 - Loss: 36.167980
[2025-05-11 02:42:47] Batch 10 - Loss: 36.023159
[2025-05-11 02:42:54] Batch 15 - Loss: 35.942650
[2025-05-11 02:43:02] Batch 20 - Loss: 35.962090
[2025-05-11 02:43:10] Batch 25 - Loss: 35.779259
[2025-05-11 02:43:17] Batch 30 - Loss: 35.775963
[2025-05-11 02:43:25] Batch 35 - Loss: 35.653847
[2025-05-11 02:43:31] === Epoch 24/100 Summary ===
Training Loss: 35.631111
Validation Loss: 37.463963
Learning Rate: 0.00001000
[2025-05-11 02:43:33] Batch 0 - Loss: 36.868568
[2025-05-11 02:43:41] Batch 5 - Loss: 35.921917
[2025-05-11 02:43:49] Batch 10 - Loss: 36.269051
[2025-05-11 02:43:56] Batch 15 - Loss: 36.502666
[2025-05-11 02:44:04] Batch 20 - Loss: 36.584229
[2025-05-11 02:44:12] Batch 25 - Loss: 36.597210
[2025-05-11 02:44:19] Batch 30 - Loss: 36.541283
[2025-05-11 02:44:27] Batch 35 - Loss: 36.544819
[2025-05-11 02:44:35] Batch 40 - Loss: 36.489807
[2025-05-11 02:44:42] Batch 45 - Loss: 36.373070
[2025-05-11 02:44:50] Batch 50 - Loss: 36.401169
[2025-05-11 02:44:58] Batch 55 - Loss: 36.305489
[2025-05-11 02:45:05] Batch 60 - Loss: 36.222603
[2025-05-11 02:45:13] Batch 65 - Loss: 36.175323
[2025-05-11 02:45:21] Batch 70 - Loss: 36.152813
[2025-05-11 02:45:28] Batch 75 - Loss: 36.127396
[2025-05-11 02:45:36] Batch 80 - Loss: 36.095081
[2025-05-11 02:45:44] Batch 85 - Loss: 36.037041
[2025-05-11 02:45:51] Batch 90 - Loss: 35.996777
[2025-05-11 02:45:59] Batch 95 - Loss: 35.928638
[2025-05-11 02:46:10] === Epoch 25/100 Summary ===
Training Loss: 35.896030
Validation Loss: 36.639050
Learning Rate: 0.00001000
[2025-05-11 02:46:10] Generating detection visualizations for epoch 25...
[2025-05-11 02:46:11] Saved visualization to trained_models/custom_detector\visualizations\epoch_25\sample_1.png
[2025-05-11 02:46:11] Saved visualization to trained_models/custom_detector\visualizations\epoch_25\sample_2.png
[2025-05-11 02:46:11] Saved visualization to trained_models/custom_detector\visualizations\epoch_25\sample_3.png
[2025-05-11 02:46:13] Batch 0 - Loss: 32.570038
[2025-05-11 02:46:21] Batch 5 - Loss: 34.983700
[2025-05-11 02:46:28] Batch 10 - Loss: 34.759388
[2025-05-11 02:46:36] Batch 15 - Loss: 35.280445
[2025-05-11 02:46:44] Batch 20 - Loss: 35.249161
[2025-05-11 02:46:51] Batch 25 - Loss: 35.127102
[2025-05-11 02:46:59] Batch 30 - Loss: 34.988892
[2025-05-11 02:47:07] Batch 35 - Loss: 34.928188
[2025-05-11 02:47:14] Batch 40 - Loss: 34.977009
[2025-05-11 02:47:22] Batch 45 - Loss: 35.186066
[2025-05-11 02:47:30] Batch 50 - Loss: 35.149708
[2025-05-11 02:47:38] Batch 55 - Loss: 35.145126
[2025-05-11 02:47:45] Batch 60 - Loss: 35.064693
[2025-05-11 02:47:53] Batch 65 - Loss: 35.080811
[2025-05-11 02:48:01] Batch 70 - Loss: 34.997997
[2025-05-11 02:48:08] Batch 75 - Loss: 34.939835
[2025-05-11 02:48:16] Batch 80 - Loss: 34.906132
[2025-05-11 02:48:24] Batch 85 - Loss: 34.916595
[2025-05-11 02:48:31] Batch 90 - Loss: 34.912159
[2025-05-11 02:48:39] Batch 95 - Loss: 34.888809
[2025-05-11 02:48:51] === Epoch 26/100 Summary ===
Training Loss: 34.896622
Validation Loss: 35.557442
Learning Rate: 0.00001000
[2025-05-11 02:48:52] Batch 0 - Loss: 34.747307
[2025-05-11 02:49:00] Batch 5 - Loss: 34.941975
[2025-05-11 02:49:08] Batch 10 - Loss: 34.749516
[2025-05-11 02:49:15] Batch 15 - Loss: 34.170666
[2025-05-11 02:49:23] Batch 20 - Loss: 34.178421
[2025-05-11 02:49:31] Batch 25 - Loss: 34.160995
[2025-05-11 02:49:38] Batch 30 - Loss: 34.138451
[2025-05-11 02:49:46] Batch 35 - Loss: 34.235756
[2025-05-11 02:49:54] Batch 40 - Loss: 34.212490
[2025-05-11 02:50:01] Batch 45 - Loss: 34.175011
[2025-05-11 02:50:09] Batch 50 - Loss: 34.026802
[2025-05-11 02:50:17] Batch 55 - Loss: 33.959770
[2025-05-11 02:50:25] Batch 60 - Loss: 33.971138
[2025-05-11 02:50:32] Batch 65 - Loss: 33.923645
[2025-05-11 02:50:40] Batch 70 - Loss: 33.849037
[2025-05-11 02:50:48] Batch 75 - Loss: 33.831261
[2025-05-11 02:50:55] Batch 80 - Loss: 33.758125
[2025-05-11 02:51:03] Batch 85 - Loss: 33.701229
[2025-05-11 02:51:11] Batch 90 - Loss: 33.644871
[2025-05-11 02:51:18] Batch 95 - Loss: 33.585049
[2025-05-11 02:51:30] === Epoch 27/100 Summary ===
Training Loss: 33.559425
Validation Loss: 34.700294
Learning Rate: 0.00001000
[2025-05-11 02:51:31] Batch 0 - Loss: 32.979927
[2025-05-11 02:51:39] Batch 5 - Loss: 33.101543
[2025-05-11 02:51:47] Batch 10 - Loss: 32.717377
[2025-05-11 02:51:54] Batch 15 - Loss: 32.867435
[2025-05-11 02:52:02] Batch 20 - Loss: 32.664906
[2025-05-11 02:52:10] Batch 25 - Loss: 32.738899
[2025-05-11 02:52:17] Batch 30 - Loss: 32.748402
[2025-05-11 02:52:25] Batch 35 - Loss: 32.766891
[2025-05-11 02:52:31] === Epoch 28/100 Summary ===
Training Loss: 32.743408
Validation Loss: 34.177380
Learning Rate: 0.00001000
[2025-05-11 02:52:33] Batch 0 - Loss: 33.905315
[2025-05-11 02:52:41] Batch 5 - Loss: 33.410175
[2025-05-11 02:52:48] Batch 10 - Loss: 32.671627
[2025-05-11 02:52:56] Batch 15 - Loss: 32.716900
[2025-05-11 02:53:04] Batch 20 - Loss: 32.908600
[2025-05-11 02:53:11] Batch 25 - Loss: 32.972538
[2025-05-11 02:53:19] Batch 30 - Loss: 33.130001
[2025-05-11 02:53:27] Batch 35 - Loss: 33.167892
[2025-05-11 02:53:34] Batch 40 - Loss: 33.134655
[2025-05-11 02:53:42] Batch 45 - Loss: 33.179672
[2025-05-11 02:53:50] Batch 50 - Loss: 33.165436
[2025-05-11 02:53:57] Batch 55 - Loss: 33.159714
[2025-05-11 02:54:05] Batch 60 - Loss: 33.179283
[2025-05-11 02:54:13] Batch 65 - Loss: 33.092503
[2025-05-11 02:54:20] Batch 70 - Loss: 33.004097
[2025-05-11 02:54:28] Batch 75 - Loss: 32.988605
[2025-05-11 02:54:36] Batch 80 - Loss: 32.935421
[2025-05-11 02:54:44] Batch 85 - Loss: 32.906403
[2025-05-11 02:54:51] Batch 90 - Loss: 32.878235
[2025-05-11 02:54:59] Batch 95 - Loss: 32.805805
[2025-05-11 02:55:10] === Epoch 29/100 Summary ===
Training Loss: 32.804813
Validation Loss: 33.317760
Learning Rate: 0.00001000
[2025-05-11 02:55:12] Batch 0 - Loss: 32.405445
[2025-05-11 02:55:20] Batch 5 - Loss: 31.893866
[2025-05-11 02:55:27] Batch 10 - Loss: 31.771309
[2025-05-11 02:55:35] Batch 15 - Loss: 31.798298
[2025-05-11 02:55:43] Batch 20 - Loss: 31.722075
[2025-05-11 02:55:50] Batch 25 - Loss: 31.739784
[2025-05-11 02:55:58] Batch 30 - Loss: 31.790373
[2025-05-11 02:56:06] Batch 35 - Loss: 31.755001
[2025-05-11 02:56:13] Batch 40 - Loss: 31.772461
[2025-05-11 02:56:21] Batch 45 - Loss: 31.766817
[2025-05-11 02:56:29] Batch 50 - Loss: 31.724106
[2025-05-11 02:56:36] Batch 55 - Loss: 31.789789
[2025-05-11 02:56:44] Batch 60 - Loss: 31.896761
[2025-05-11 02:56:52] Batch 65 - Loss: 31.899347
[2025-05-11 02:57:00] Batch 70 - Loss: 31.867937
[2025-05-11 02:57:07] Batch 75 - Loss: 31.814457
[2025-05-11 02:57:15] Batch 80 - Loss: 31.777391
[2025-05-11 02:57:23] Batch 85 - Loss: 31.715185
[2025-05-11 02:57:30] Batch 90 - Loss: 31.652170
[2025-05-11 02:57:38] Batch 95 - Loss: 31.637405
[2025-05-11 02:57:49] === Epoch 30/100 Summary ===
Training Loss: 31.600716
Validation Loss: 32.376667
Learning Rate: 0.00001000
[2025-05-11 02:57:49] Generating detection visualizations for epoch 30...
[2025-05-11 02:57:50] Saved visualization to trained_models/custom_detector\visualizations\epoch_30\sample_1.png
[2025-05-11 02:57:50] Saved visualization to trained_models/custom_detector\visualizations\epoch_30\sample_2.png
[2025-05-11 02:57:50] Saved visualization to trained_models/custom_detector\visualizations\epoch_30\sample_3.png
[2025-05-11 02:57:52] Batch 0 - Loss: 30.399321
[2025-05-11 02:58:00] Batch 5 - Loss: 30.889589
[2025-05-11 02:58:07] Batch 10 - Loss: 31.195923
[2025-05-11 02:58:15] Batch 15 - Loss: 31.227705
[2025-05-11 02:58:23] Batch 20 - Loss: 31.181030
[2025-05-11 02:58:30] Batch 25 - Loss: 31.208397
[2025-05-11 02:58:38] Batch 30 - Loss: 31.294168
[2025-05-11 02:58:46] Batch 35 - Loss: 31.343880
[2025-05-11 02:58:54] Batch 40 - Loss: 31.337149
[2025-05-11 02:59:01] Batch 45 - Loss: 31.150614
[2025-05-11 02:59:09] Batch 50 - Loss: 31.183266
[2025-05-11 02:59:17] Batch 55 - Loss: 31.134296
[2025-05-11 02:59:24] Batch 60 - Loss: 31.169676
[2025-05-11 02:59:32] Batch 65 - Loss: 31.154636
[2025-05-11 02:59:40] Batch 70 - Loss: 31.169636
[2025-05-11 02:59:47] Batch 75 - Loss: 31.091028
[2025-05-11 02:59:55] Batch 80 - Loss: 31.025141
[2025-05-11 03:00:03] Batch 85 - Loss: 30.939117
[2025-05-11 03:00:10] Batch 90 - Loss: 30.846579
[2025-05-11 03:00:18] Batch 95 - Loss: 30.733444
[2025-05-11 03:00:30] === Epoch 31/100 Summary ===
Training Loss: 30.720249
Validation Loss: 31.219055
Learning Rate: 0.00001000
[2025-05-11 03:00:31] Batch 0 - Loss: 29.751461
[2025-05-11 03:00:39] Batch 5 - Loss: 29.352987
[2025-05-11 03:00:47] Batch 10 - Loss: 29.497683
[2025-05-11 03:00:54] Batch 15 - Loss: 29.549562
[2025-05-11 03:01:02] Batch 20 - Loss: 29.613647
[2025-05-11 03:01:09] Batch 25 - Loss: 29.578999
[2025-05-11 03:01:17] Batch 30 - Loss: 29.468925
[2025-05-11 03:01:25] Batch 35 - Loss: 29.350548
[2025-05-11 03:01:31] === Epoch 32/100 Summary ===
Training Loss: 29.369377
Validation Loss: 31.173050
Learning Rate: 0.00001000
[2025-05-11 03:01:33] Batch 0 - Loss: 29.771194
[2025-05-11 03:01:41] Batch 5 - Loss: 30.114527
[2025-05-11 03:01:48] Batch 10 - Loss: 30.275408
[2025-05-11 03:01:56] Batch 15 - Loss: 30.532818
[2025-05-11 03:02:04] Batch 20 - Loss: 30.454514
[2025-05-11 03:02:11] Batch 25 - Loss: 30.194510
[2025-05-11 03:02:19] Batch 30 - Loss: 30.245577
[2025-05-11 03:02:27] Batch 35 - Loss: 30.198479
[2025-05-11 03:02:34] Batch 40 - Loss: 30.170103
[2025-05-11 03:02:43] Batch 45 - Loss: 30.038193
[2025-05-11 03:02:50] Batch 50 - Loss: 30.105133
[2025-05-11 03:02:58] Batch 55 - Loss: 30.186605
[2025-05-11 03:03:06] Batch 60 - Loss: 30.079382
[2025-05-11 03:03:14] Batch 65 - Loss: 30.108828
[2025-05-11 03:03:22] Batch 70 - Loss: 30.008181
[2025-05-11 03:03:29] Batch 75 - Loss: 29.962561
[2025-05-11 03:03:37] Batch 80 - Loss: 29.893637
[2025-05-11 03:03:45] Batch 85 - Loss: 29.869099
[2025-05-11 03:03:52] Batch 90 - Loss: 29.786736
[2025-05-11 03:04:00] Batch 95 - Loss: 29.731592
[2025-05-11 03:04:11] === Epoch 33/100 Summary ===
Training Loss: 29.688972
Validation Loss: 29.706745
Learning Rate: 0.00001000
[2025-05-11 03:04:13] Batch 0 - Loss: 29.405804
[2025-05-11 03:04:21] Batch 5 - Loss: 28.566742
[2025-05-11 03:04:28] Batch 10 - Loss: 28.499645
[2025-05-11 03:04:36] Batch 15 - Loss: 28.613182
[2025-05-11 03:04:44] Batch 20 - Loss: 28.722208
[2025-05-11 03:04:51] Batch 25 - Loss: 28.866398
[2025-05-11 03:04:59] Batch 30 - Loss: 28.920959
[2025-05-11 03:05:07] Batch 35 - Loss: 28.837782
[2025-05-11 03:05:14] Batch 40 - Loss: 28.901251
[2025-05-11 03:05:22] Batch 45 - Loss: 28.950956
[2025-05-11 03:05:30] Batch 50 - Loss: 29.030767
[2025-05-11 03:05:38] Batch 55 - Loss: 29.031544
[2025-05-11 03:05:45] Batch 60 - Loss: 29.031561
[2025-05-11 03:05:53] Batch 65 - Loss: 28.999449
[2025-05-11 03:06:01] Batch 70 - Loss: 29.005869
[2025-05-11 03:06:08] Batch 75 - Loss: 28.947744
[2025-05-11 03:06:16] Batch 80 - Loss: 28.939781
[2025-05-11 03:06:24] Batch 85 - Loss: 28.911840
[2025-05-11 03:06:31] Batch 90 - Loss: 28.846355
[2025-05-11 03:06:39] Batch 95 - Loss: 28.836679
[2025-05-11 03:06:50] === Epoch 34/100 Summary ===
Training Loss: 28.767838
Validation Loss: 29.163425
Learning Rate: 0.00001000
[2025-05-11 03:06:52] Batch 0 - Loss: 27.367260
[2025-05-11 03:07:00] Batch 5 - Loss: 28.547112
[2025-05-11 03:07:08] Batch 10 - Loss: 28.337934
[2025-05-11 03:07:15] Batch 15 - Loss: 28.388027
[2025-05-11 03:07:23] Batch 20 - Loss: 28.365917
[2025-05-11 03:07:31] Batch 25 - Loss: 28.257406
[2025-05-11 03:07:38] Batch 30 - Loss: 28.285210
[2025-05-11 03:07:46] Batch 35 - Loss: 28.173336
[2025-05-11 03:07:54] Batch 40 - Loss: 28.119247
[2025-05-11 03:08:02] Batch 45 - Loss: 28.066387
[2025-05-11 03:08:09] Batch 50 - Loss: 28.032131
[2025-05-11 03:08:17] Batch 55 - Loss: 28.031515
[2025-05-11 03:08:25] Batch 60 - Loss: 28.059288
[2025-05-11 03:08:32] Batch 65 - Loss: 28.069372
[2025-05-11 03:08:40] Batch 70 - Loss: 27.996748
[2025-05-11 03:08:48] Batch 75 - Loss: 27.927378
[2025-05-11 03:08:55] Batch 80 - Loss: 27.864735
[2025-05-11 03:09:03] Batch 85 - Loss: 27.868267
[2025-05-11 03:09:11] Batch 90 - Loss: 27.813211
[2025-05-11 03:09:18] Batch 95 - Loss: 27.718979
[2025-05-11 03:09:30] === Epoch 35/100 Summary ===
Training Loss: 27.669201
Validation Loss: 28.090405
Learning Rate: 0.00001000
[2025-05-11 03:09:30] Generating detection visualizations for epoch 35...
[2025-05-11 03:09:30] Saved visualization to trained_models/custom_detector\visualizations\epoch_35\sample_1.png
[2025-05-11 03:09:31] Saved visualization to trained_models/custom_detector\visualizations\epoch_35\sample_2.png
[2025-05-11 03:09:31] Saved visualization to trained_models/custom_detector\visualizations\epoch_35\sample_3.png
[2025-05-11 03:09:33] Batch 0 - Loss: 27.233936
[2025-05-11 03:09:41] Batch 5 - Loss: 27.083513
[2025-05-11 03:09:49] Batch 10 - Loss: 27.084684
[2025-05-11 03:09:56] Batch 15 - Loss: 26.752470
[2025-05-11 03:10:04] Batch 20 - Loss: 26.755283
[2025-05-11 03:10:12] Batch 25 - Loss: 26.685738
[2025-05-11 03:10:20] Batch 30 - Loss: 26.691532
[2025-05-11 03:10:28] Batch 35 - Loss: 26.657845
[2025-05-11 03:10:34] === Epoch 36/100 Summary ===
Training Loss: 26.677320
Validation Loss: 27.790920
Learning Rate: 0.00001000
[2025-05-11 03:10:36] Batch 0 - Loss: 25.178980
[2025-05-11 03:10:44] Batch 5 - Loss: 26.536661
[2025-05-11 03:10:52] Batch 10 - Loss: 26.997938
[2025-05-11 03:10:59] Batch 15 - Loss: 27.318434
[2025-05-11 03:11:07] Batch 20 - Loss: 27.242170
[2025-05-11 03:11:15] Batch 25 - Loss: 27.240276
[2025-05-11 03:11:23] Batch 30 - Loss: 27.184242
[2025-05-11 03:11:30] Batch 35 - Loss: 27.349812
[2025-05-11 03:11:38] Batch 40 - Loss: 27.280905
[2025-05-11 03:11:46] Batch 45 - Loss: 27.285755
[2025-05-11 03:11:54] Batch 50 - Loss: 27.288527
[2025-05-11 03:12:01] Batch 55 - Loss: 27.339077
[2025-05-11 03:12:09] Batch 60 - Loss: 27.239408
[2025-05-11 03:12:17] Batch 65 - Loss: 27.195290
[2025-05-11 03:12:25] Batch 70 - Loss: 27.137960
[2025-05-11 03:12:32] Batch 75 - Loss: 27.077103
[2025-05-11 03:12:40] Batch 80 - Loss: 27.000389
[2025-05-11 03:12:48] Batch 85 - Loss: 26.983444
[2025-05-11 03:12:56] Batch 90 - Loss: 26.930624
[2025-05-11 03:13:03] Batch 95 - Loss: 26.899153
[2025-05-11 03:13:15] === Epoch 37/100 Summary ===
Training Loss: 26.867607
Validation Loss: 27.156647
Learning Rate: 0.00001000
[2025-05-11 03:13:17] Batch 0 - Loss: 26.883102
[2025-05-11 03:13:24] Batch 5 - Loss: 25.578796
[2025-05-11 03:13:32] Batch 10 - Loss: 25.535950
[2025-05-11 03:13:40] Batch 15 - Loss: 25.709904
[2025-05-11 03:13:48] Batch 20 - Loss: 25.937029
[2025-05-11 03:13:55] Batch 25 - Loss: 25.808178
[2025-05-11 03:14:03] Batch 30 - Loss: 25.888445
[2025-05-11 03:14:11] Batch 35 - Loss: 25.815693
[2025-05-11 03:14:19] Batch 40 - Loss: 26.023563
[2025-05-11 03:14:26] Batch 45 - Loss: 26.090916
[2025-05-11 03:14:34] Batch 50 - Loss: 26.172401
[2025-05-11 03:14:42] Batch 55 - Loss: 26.080641
[2025-05-11 03:14:50] Batch 60 - Loss: 26.035069
[2025-05-11 03:14:58] Batch 65 - Loss: 26.013844
[2025-05-11 03:15:05] Batch 70 - Loss: 25.954899
[2025-05-11 03:15:13] Batch 75 - Loss: 25.972591
[2025-05-11 03:15:21] Batch 80 - Loss: 25.938114
[2025-05-11 03:15:29] Batch 85 - Loss: 25.943771
[2025-05-11 03:15:36] Batch 90 - Loss: 25.911674
[2025-05-11 03:15:44] Batch 95 - Loss: 25.891535
[2025-05-11 03:15:56] === Epoch 38/100 Summary ===
Training Loss: 25.897350
Validation Loss: 26.624298
Learning Rate: 0.00001000
[2025-05-11 03:15:57] Batch 0 - Loss: 25.339432
[2025-05-11 03:16:05] Batch 5 - Loss: 24.780884
[2025-05-11 03:16:13] Batch 10 - Loss: 25.826246
[2025-05-11 03:16:21] Batch 15 - Loss: 25.670649
[2025-05-11 03:16:28] Batch 20 - Loss: 25.495388
[2025-05-11 03:16:36] Batch 25 - Loss: 25.668301
[2025-05-11 03:16:44] Batch 30 - Loss: 25.579079
[2025-05-11 03:16:52] Batch 35 - Loss: 25.475451
[2025-05-11 03:17:00] Batch 40 - Loss: 25.458353
[2025-05-11 03:17:07] Batch 45 - Loss: 25.482468
[2025-05-11 03:17:15] Batch 50 - Loss: 25.473719
[2025-05-11 03:17:23] Batch 55 - Loss: 25.383295
[2025-05-11 03:17:31] Batch 60 - Loss: 25.371513
[2025-05-11 03:17:38] Batch 65 - Loss: 25.344908
[2025-05-11 03:17:46] Batch 70 - Loss: 25.237677
[2025-05-11 03:17:54] Batch 75 - Loss: 25.176146
[2025-05-11 03:18:01] Batch 80 - Loss: 25.170866
[2025-05-11 03:18:09] Batch 85 - Loss: 25.123144
[2025-05-11 03:18:17] Batch 90 - Loss: 25.046171
[2025-05-11 03:18:25] Batch 95 - Loss: 24.996214
[2025-05-11 03:18:36] === Epoch 39/100 Summary ===
Training Loss: 24.951303
Validation Loss: 25.552731
Learning Rate: 0.00001000
[2025-05-11 03:18:38] Batch 0 - Loss: 22.314934
[2025-05-11 03:18:46] Batch 5 - Loss: 24.000536
[2025-05-11 03:18:53] Batch 10 - Loss: 23.954664
[2025-05-11 03:19:01] Batch 15 - Loss: 24.187847
[2025-05-11 03:19:09] Batch 20 - Loss: 24.255276
[2025-05-11 03:19:17] Batch 25 - Loss: 24.102806
[2025-05-11 03:19:24] Batch 30 - Loss: 24.131918
[2025-05-11 03:19:32] Batch 35 - Loss: 24.037405
[2025-05-11 03:19:38] === Epoch 40/100 Summary ===
Training Loss: 24.025072
Validation Loss: 25.253086
Learning Rate: 0.00001000
[2025-05-11 03:19:38] Generating detection visualizations for epoch 40...
[2025-05-11 03:19:39] Saved visualization to trained_models/custom_detector\visualizations\epoch_40\sample_1.png
[2025-05-11 03:19:39] Saved visualization to trained_models/custom_detector\visualizations\epoch_40\sample_2.png
[2025-05-11 03:19:40] Saved visualization to trained_models/custom_detector\visualizations\epoch_40\sample_3.png
[2025-05-11 03:19:42] Batch 0 - Loss: 24.957512
[2025-05-11 03:19:49] Batch 5 - Loss: 25.577141
[2025-05-11 03:19:57] Batch 10 - Loss: 25.357391
[2025-05-11 03:20:05] Batch 15 - Loss: 25.248701
[2025-05-11 03:20:13] Batch 20 - Loss: 24.930668
[2025-05-11 03:20:20] Batch 25 - Loss: 24.701962
[2025-05-11 03:20:28] Batch 30 - Loss: 24.738447
[2025-05-11 03:20:36] Batch 35 - Loss: 24.759697
[2025-05-11 03:20:44] Batch 40 - Loss: 24.716759
[2025-05-11 03:20:51] Batch 45 - Loss: 24.800482
[2025-05-11 03:20:59] Batch 50 - Loss: 24.736498
[2025-05-11 03:21:07] Batch 55 - Loss: 24.672060
[2025-05-11 03:21:15] Batch 60 - Loss: 24.654623
[2025-05-11 03:21:22] Batch 65 - Loss: 24.515835
[2025-05-11 03:21:30] Batch 70 - Loss: 24.470242
[2025-05-11 03:21:38] Batch 75 - Loss: 24.385904
[2025-05-11 03:21:46] Batch 80 - Loss: 24.367168
[2025-05-11 03:21:53] Batch 85 - Loss: 24.353777
[2025-05-11 03:22:01] Batch 90 - Loss: 24.263151
[2025-05-11 03:22:09] Batch 95 - Loss: 24.182037
[2025-05-11 03:22:21] === Epoch 41/100 Summary ===
Training Loss: 24.162954
Validation Loss: 24.166216
Learning Rate: 0.00001000
[2025-05-11 03:22:22] Batch 0 - Loss: 25.001240
[2025-05-11 03:22:30] Batch 5 - Loss: 23.305733
[2025-05-11 03:22:38] Batch 10 - Loss: 23.344942
[2025-05-11 03:22:45] Batch 15 - Loss: 23.553560
[2025-05-11 03:22:53] Batch 20 - Loss: 23.452847
[2025-05-11 03:23:01] Batch 25 - Loss: 23.615286
[2025-05-11 03:23:09] Batch 30 - Loss: 23.500662
[2025-05-11 03:23:16] Batch 35 - Loss: 23.398563
[2025-05-11 03:23:24] Batch 40 - Loss: 23.337444
[2025-05-11 03:23:32] Batch 45 - Loss: 23.528013
[2025-05-11 03:23:39] Batch 50 - Loss: 23.625544
[2025-05-11 03:23:47] Batch 55 - Loss: 23.607489
[2025-05-11 03:23:55] Batch 60 - Loss: 23.571026
[2025-05-11 03:24:03] Batch 65 - Loss: 23.520174
[2025-05-11 03:24:11] Batch 70 - Loss: 23.493742
[2025-05-11 03:24:18] Batch 75 - Loss: 23.496275
[2025-05-11 03:24:26] Batch 80 - Loss: 23.414635
[2025-05-11 03:24:34] Batch 85 - Loss: 23.430676
[2025-05-11 03:24:41] Batch 90 - Loss: 23.402634
[2025-05-11 03:24:49] Batch 95 - Loss: 23.403734
[2025-05-11 03:25:01] === Epoch 42/100 Summary ===
Training Loss: 23.354475
Validation Loss: 23.790470
Learning Rate: 0.00001000
[2025-05-11 03:25:02] Batch 0 - Loss: 23.046846
[2025-05-11 03:25:10] Batch 5 - Loss: 22.984596
[2025-05-11 03:25:18] Batch 10 - Loss: 23.411413
[2025-05-11 03:25:26] Batch 15 - Loss: 23.001595
[2025-05-11 03:25:33] Batch 20 - Loss: 22.994381
[2025-05-11 03:25:41] Batch 25 - Loss: 23.046236
[2025-05-11 03:25:49] Batch 30 - Loss: 23.052803
[2025-05-11 03:25:57] Batch 35 - Loss: 22.930765
[2025-05-11 03:26:05] Batch 40 - Loss: 22.978987
[2025-05-11 03:26:12] Batch 45 - Loss: 23.069214
[2025-05-11 03:26:20] Batch 50 - Loss: 22.914064
[2025-05-11 03:26:28] Batch 55 - Loss: 22.824724
[2025-05-11 03:26:36] Batch 60 - Loss: 22.777208
[2025-05-11 03:26:43] Batch 65 - Loss: 22.689423
[2025-05-11 03:26:51] Batch 70 - Loss: 22.668436
[2025-05-11 03:26:59] Batch 75 - Loss: 22.631544
[2025-05-11 03:27:07] Batch 80 - Loss: 22.604475
[2025-05-11 03:27:14] Batch 85 - Loss: 22.581160
[2025-05-11 03:27:22] Batch 90 - Loss: 22.573828
[2025-05-11 03:27:30] Batch 95 - Loss: 22.493408
[2025-05-11 03:27:41] === Epoch 43/100 Summary ===
Training Loss: 22.475569
Validation Loss: 23.149187
Learning Rate: 0.00001000
[2025-05-11 03:27:43] Batch 0 - Loss: 21.908394
[2025-05-11 03:27:51] Batch 5 - Loss: 21.230173
[2025-05-11 03:27:58] Batch 10 - Loss: 21.257807
[2025-05-11 03:28:06] Batch 15 - Loss: 21.599077
[2025-05-11 03:28:14] Batch 20 - Loss: 21.670862
[2025-05-11 03:28:22] Batch 25 - Loss: 21.663015
[2025-05-11 03:28:29] Batch 30 - Loss: 21.543344
[2025-05-11 03:28:37] Batch 35 - Loss: 21.538702
[2025-05-11 03:28:43] === Epoch 44/100 Summary ===
Training Loss: 21.480803
Validation Loss: 22.438065
Learning Rate: 0.00001000
[2025-05-11 03:28:45] Batch 0 - Loss: 20.459354
[2025-05-11 03:28:53] Batch 5 - Loss: 21.649649
[2025-05-11 03:29:01] Batch 10 - Loss: 22.269844
[2025-05-11 03:29:09] Batch 15 - Loss: 22.396778
[2025-05-11 03:29:17] Batch 20 - Loss: 22.240572
[2025-05-11 03:29:24] Batch 25 - Loss: 22.230457
[2025-05-11 03:29:32] Batch 30 - Loss: 22.345015
[2025-05-11 03:29:40] Batch 35 - Loss: 22.342140
[2025-05-11 03:29:48] Batch 40 - Loss: 22.313887
[2025-05-11 03:29:55] Batch 45 - Loss: 22.252018
[2025-05-11 03:30:03] Batch 50 - Loss: 22.127979
[2025-05-11 03:30:11] Batch 55 - Loss: 22.063459
[2025-05-11 03:30:19] Batch 60 - Loss: 22.031345
[2025-05-11 03:30:26] Batch 65 - Loss: 22.016901
[2025-05-11 03:30:34] Batch 70 - Loss: 21.964588
[2025-05-11 03:30:42] Batch 75 - Loss: 21.949682
[2025-05-11 03:30:50] Batch 80 - Loss: 21.873388
[2025-05-11 03:30:57] Batch 85 - Loss: 21.854950
[2025-05-11 03:31:05] Batch 90 - Loss: 21.811821
[2025-05-11 03:31:13] Batch 95 - Loss: 21.782755
[2025-05-11 03:31:24] === Epoch 45/100 Summary ===
Training Loss: 21.752256
Validation Loss: 21.747196
Learning Rate: 0.00001000
[2025-05-11 03:31:24] Generating detection visualizations for epoch 45...
[2025-05-11 03:31:25] Saved visualization to trained_models/custom_detector\visualizations\epoch_45\sample_1.png
[2025-05-11 03:31:25] Saved visualization to trained_models/custom_detector\visualizations\epoch_45\sample_2.png
[2025-05-11 03:31:26] Saved visualization to trained_models/custom_detector\visualizations\epoch_45\sample_3.png
[2025-05-11 03:31:27] Batch 0 - Loss: 20.208187
[2025-05-11 03:31:35] Batch 5 - Loss: 21.282026
[2025-05-11 03:31:43] Batch 10 - Loss: 21.041817
[2025-05-11 03:31:50] Batch 15 - Loss: 21.319529
[2025-05-11 03:31:58] Batch 20 - Loss: 21.011847
[2025-05-11 03:32:06] Batch 25 - Loss: 21.282824
[2025-05-11 03:32:13] Batch 30 - Loss: 21.181543
[2025-05-11 03:32:21] Batch 35 - Loss: 21.124437
[2025-05-11 03:32:29] Batch 40 - Loss: 21.296354
[2025-05-11 03:32:37] Batch 45 - Loss: 21.277287
[2025-05-11 03:32:44] Batch 50 - Loss: 21.255260
[2025-05-11 03:32:52] Batch 55 - Loss: 21.181286
[2025-05-11 03:33:00] Batch 60 - Loss: 21.174307
[2025-05-11 03:33:07] Batch 65 - Loss: 21.164070
[2025-05-11 03:33:15] Batch 70 - Loss: 21.103897
[2025-05-11 03:33:23] Batch 75 - Loss: 21.055750
[2025-05-11 03:33:31] Batch 80 - Loss: 21.033266
[2025-05-11 03:33:39] Batch 85 - Loss: 21.070461
[2025-05-11 03:33:46] Batch 90 - Loss: 21.063942
[2025-05-11 03:33:54] Batch 95 - Loss: 21.008909
[2025-05-11 03:34:05] === Epoch 46/100 Summary ===
Training Loss: 20.976736
Validation Loss: 21.448587
Learning Rate: 0.00001000
[2025-05-11 03:34:07] Batch 0 - Loss: 21.016949
[2025-05-11 03:34:15] Batch 5 - Loss: 21.803938
[2025-05-11 03:34:22] Batch 10 - Loss: 21.167549
[2025-05-11 03:34:30] Batch 15 - Loss: 21.310234
[2025-05-11 03:34:38] Batch 20 - Loss: 21.166042
[2025-05-11 03:34:46] Batch 25 - Loss: 21.112051
[2025-05-11 03:34:53] Batch 30 - Loss: 20.914509
[2025-05-11 03:35:01] Batch 35 - Loss: 20.778111
[2025-05-11 03:35:09] Batch 40 - Loss: 20.650034
[2025-05-11 03:35:17] Batch 45 - Loss: 20.590528
[2025-05-11 03:35:24] Batch 50 - Loss: 20.657671
[2025-05-11 03:35:32] Batch 55 - Loss: 20.566486
[2025-05-11 03:35:40] Batch 60 - Loss: 20.513536
[2025-05-11 03:35:48] Batch 65 - Loss: 20.527063
[2025-05-11 03:35:55] Batch 70 - Loss: 20.551218
[2025-05-11 03:36:03] Batch 75 - Loss: 20.497150
[2025-05-11 03:36:11] Batch 80 - Loss: 20.433784
[2025-05-11 03:36:18] Batch 85 - Loss: 20.401213
[2025-05-11 03:36:26] Batch 90 - Loss: 20.340918
[2025-05-11 03:36:34] Batch 95 - Loss: 20.307556
[2025-05-11 03:36:45] === Epoch 47/100 Summary ===
Training Loss: 20.280750
Validation Loss: 20.901499
Learning Rate: 0.00001000
[2025-05-11 03:36:47] Batch 0 - Loss: 19.988499
[2025-05-11 03:36:55] Batch 5 - Loss: 19.295351
[2025-05-11 03:37:03] Batch 10 - Loss: 19.219591
[2025-05-11 03:37:10] Batch 15 - Loss: 19.148186
[2025-05-11 03:37:18] Batch 20 - Loss: 19.120369
[2025-05-11 03:37:26] Batch 25 - Loss: 19.170071
[2025-05-11 03:37:33] Batch 30 - Loss: 19.235296
[2025-05-11 03:37:41] Batch 35 - Loss: 19.348301
[2025-05-11 03:37:47] === Epoch 48/100 Summary ===
Training Loss: 19.341894
Validation Loss: 20.555077
Learning Rate: 0.00001000
[2025-05-11 03:37:49] Batch 0 - Loss: 19.714855
[2025-05-11 03:37:57] Batch 5 - Loss: 19.677273
[2025-05-11 03:38:05] Batch 10 - Loss: 19.894098
[2025-05-11 03:38:13] Batch 15 - Loss: 19.946629
[2025-05-11 03:38:20] Batch 20 - Loss: 19.862494
[2025-05-11 03:38:28] Batch 25 - Loss: 19.921316
[2025-05-11 03:38:36] Batch 30 - Loss: 19.989950
[2025-05-11 03:38:43] Batch 35 - Loss: 20.018461
[2025-05-11 03:38:51] Batch 40 - Loss: 19.940138
[2025-05-11 03:38:59] Batch 45 - Loss: 19.971663
[2025-05-11 03:39:07] Batch 50 - Loss: 20.093273
[2025-05-11 03:39:14] Batch 55 - Loss: 20.060713
[2025-05-11 03:39:22] Batch 60 - Loss: 20.070805
[2025-05-11 03:39:30] Batch 65 - Loss: 20.103285
[2025-05-11 03:39:37] Batch 70 - Loss: 20.081587
[2025-05-11 03:39:45] Batch 75 - Loss: 20.002853
[2025-05-11 03:39:53] Batch 80 - Loss: 19.867880
[2025-05-11 03:40:01] Batch 85 - Loss: 19.801653
[2025-05-11 03:40:08] Batch 90 - Loss: 19.707798
[2025-05-11 03:40:16] Batch 95 - Loss: 19.680561
[2025-05-11 03:40:28] === Epoch 49/100 Summary ===
Training Loss: 19.632221
Validation Loss: 19.744617
Learning Rate: 0.00001000
[2025-05-11 03:40:29] Batch 0 - Loss: 19.645926
[2025-05-11 03:40:37] Batch 5 - Loss: 19.105986
[2025-05-11 03:40:45] Batch 10 - Loss: 19.431461
[2025-05-11 03:40:52] Batch 15 - Loss: 19.494827
[2025-05-11 03:41:00] Batch 20 - Loss: 19.168262
[2025-05-11 03:41:08] Batch 25 - Loss: 19.133101
[2025-05-11 03:41:15] Batch 30 - Loss: 19.122387
[2025-05-11 03:41:23] Batch 35 - Loss: 19.124889
[2025-05-11 03:41:31] Batch 40 - Loss: 19.032488
[2025-05-11 03:41:39] Batch 45 - Loss: 19.026081
[2025-05-11 03:41:46] Batch 50 - Loss: 19.016819
[2025-05-11 03:41:54] Batch 55 - Loss: 19.061407
[2025-05-11 03:42:02] Batch 60 - Loss: 19.086756
[2025-05-11 03:42:09] Batch 65 - Loss: 19.179688
[2025-05-11 03:42:17] Batch 70 - Loss: 19.119986
[2025-05-11 03:42:25] Batch 75 - Loss: 19.121170
[2025-05-11 03:42:33] Batch 80 - Loss: 19.114487
[2025-05-11 03:42:40] Batch 85 - Loss: 19.123997
[2025-05-11 03:42:48] Batch 90 - Loss: 19.068497
[2025-05-11 03:42:56] Batch 95 - Loss: 19.041973
[2025-05-11 03:43:07] === Epoch 50/100 Summary ===
Training Loss: 19.049599
Validation Loss: 19.434986
Learning Rate: 0.00001000
[2025-05-11 03:43:07] Generating detection visualizations for epoch 50...
[2025-05-11 03:43:08] Saved visualization to trained_models/custom_detector\visualizations\epoch_50\sample_1.png
[2025-05-11 03:43:08] Saved visualization to trained_models/custom_detector\visualizations\epoch_50\sample_2.png
[2025-05-11 03:43:08] Saved visualization to trained_models/custom_detector\visualizations\epoch_50\sample_3.png
[2025-05-11 03:43:10] Batch 0 - Loss: 19.015125
[2025-05-11 03:43:18] Batch 5 - Loss: 18.441133
[2025-05-11 03:43:25] Batch 10 - Loss: 19.263605
[2025-05-11 03:43:33] Batch 15 - Loss: 18.668804
[2025-05-11 03:43:41] Batch 20 - Loss: 18.734226
[2025-05-11 03:43:48] Batch 25 - Loss: 18.499166
[2025-05-11 03:43:56] Batch 30 - Loss: 18.445463
[2025-05-11 03:44:04] Batch 35 - Loss: 18.444101
[2025-05-11 03:44:11] Batch 40 - Loss: 18.401028
[2025-05-11 03:44:19] Batch 45 - Loss: 18.383743
[2025-05-11 03:44:27] Batch 50 - Loss: 18.389288
[2025-05-11 03:44:35] Batch 55 - Loss: 18.366240
[2025-05-11 03:44:42] Batch 60 - Loss: 18.443096
[2025-05-11 03:44:50] Batch 65 - Loss: 18.399282
[2025-05-11 03:44:58] Batch 70 - Loss: 18.298052
[2025-05-11 03:45:06] Batch 75 - Loss: 18.238737
[2025-05-11 03:45:13] Batch 80 - Loss: 18.178799
[2025-05-11 03:45:21] Batch 85 - Loss: 18.088787
[2025-05-11 03:45:29] Batch 90 - Loss: 18.088829
[2025-05-11 03:45:36] Batch 95 - Loss: 18.092306
[2025-05-11 03:45:48] === Epoch 51/100 Summary ===
Training Loss: 18.070354
Validation Loss: 18.630898
Learning Rate: 0.00001000
[2025-05-11 03:45:49] Batch 0 - Loss: 17.158421
[2025-05-11 03:45:57] Batch 5 - Loss: 17.447424
[2025-05-11 03:46:05] Batch 10 - Loss: 17.686270
[2025-05-11 03:46:12] Batch 15 - Loss: 17.653702
[2025-05-11 03:46:20] Batch 20 - Loss: 17.623886
[2025-05-11 03:46:28] Batch 25 - Loss: 17.488966
[2025-05-11 03:46:36] Batch 30 - Loss: 17.531443
[2025-05-11 03:46:43] Batch 35 - Loss: 17.465302
[2025-05-11 03:46:49] === Epoch 52/100 Summary ===
Training Loss: 17.478437
Validation Loss: 18.486511
Learning Rate: 0.00001000
[2025-05-11 03:46:51] Batch 0 - Loss: 18.559593
[2025-05-11 03:46:59] Batch 5 - Loss: 17.624022
[2025-05-11 03:47:07] Batch 10 - Loss: 17.338427
[2025-05-11 03:47:14] Batch 15 - Loss: 17.708994
[2025-05-11 03:47:22] Batch 20 - Loss: 17.901028
[2025-05-11 03:47:30] Batch 25 - Loss: 17.879398
[2025-05-11 03:47:38] Batch 30 - Loss: 17.905390
[2025-05-11 03:47:45] Batch 35 - Loss: 17.885139
[2025-05-11 03:47:53] Batch 40 - Loss: 17.958479
[2025-05-11 03:48:01] Batch 45 - Loss: 17.966063
[2025-05-11 03:48:08] Batch 50 - Loss: 18.005255
[2025-05-11 03:48:16] Batch 55 - Loss: 17.936348
[2025-05-11 03:48:24] Batch 60 - Loss: 17.963959
[2025-05-11 03:48:32] Batch 65 - Loss: 17.988714
[2025-05-11 03:48:39] Batch 70 - Loss: 17.991297
[2025-05-11 03:48:47] Batch 75 - Loss: 17.956890
[2025-05-11 03:48:55] Batch 80 - Loss: 17.901735
[2025-05-11 03:49:02] Batch 85 - Loss: 17.873175
[2025-05-11 03:49:10] Batch 90 - Loss: 17.810499
[2025-05-11 03:49:18] Batch 95 - Loss: 17.810780
[2025-05-11 03:49:30] === Epoch 53/100 Summary ===
Training Loss: 17.774971
Validation Loss: 17.882925
Learning Rate: 0.00001000
[2025-05-11 03:49:31] Batch 0 - Loss: 19.261658
[2025-05-11 03:49:39] Batch 5 - Loss: 17.396090
[2025-05-11 03:49:47] Batch 10 - Loss: 16.925381
[2025-05-11 03:49:54] Batch 15 - Loss: 16.839977
[2025-05-11 03:50:02] Batch 20 - Loss: 17.061562
[2025-05-11 03:50:10] Batch 25 - Loss: 16.957912
[2025-05-11 03:50:17] Batch 30 - Loss: 17.044596
[2025-05-11 03:50:25] Batch 35 - Loss: 16.961660
[2025-05-11 03:50:33] Batch 40 - Loss: 17.017319
[2025-05-11 03:50:40] Batch 45 - Loss: 17.044037
[2025-05-11 03:50:48] Batch 50 - Loss: 17.038025
[2025-05-11 03:50:56] Batch 55 - Loss: 17.016447
[2025-05-11 03:51:04] Batch 60 - Loss: 17.003277
[2025-05-11 03:51:11] Batch 65 - Loss: 17.039385
[2025-05-11 03:51:19] Batch 70 - Loss: 17.061131
[2025-05-11 03:51:27] Batch 75 - Loss: 17.042057
[2025-05-11 03:51:34] Batch 80 - Loss: 16.990253
[2025-05-11 03:51:42] Batch 85 - Loss: 17.088274
[2025-05-11 03:51:50] Batch 90 - Loss: 17.088675
[2025-05-11 03:51:58] Batch 95 - Loss: 17.118795
[2025-05-11 03:52:09] === Epoch 54/100 Summary ===
Training Loss: 17.116928
Validation Loss: 17.639820
Learning Rate: 0.00001000
[2025-05-11 03:52:11] Batch 0 - Loss: 18.122128
[2025-05-11 03:52:18] Batch 5 - Loss: 17.948244
[2025-05-11 03:52:26] Batch 10 - Loss: 17.409971
[2025-05-11 03:52:34] Batch 15 - Loss: 17.483669
[2025-05-11 03:52:41] Batch 20 - Loss: 17.088259
[2025-05-11 03:52:49] Batch 25 - Loss: 16.957626
[2025-05-11 03:52:57] Batch 30 - Loss: 16.935713
[2025-05-11 03:53:05] Batch 35 - Loss: 16.928957
[2025-05-11 03:53:12] Batch 40 - Loss: 16.796528
[2025-05-11 03:53:20] Batch 45 - Loss: 16.740751
[2025-05-11 03:53:28] Batch 50 - Loss: 16.724091
[2025-05-11 03:53:36] Batch 55 - Loss: 16.736296
[2025-05-11 03:53:43] Batch 60 - Loss: 16.665497
[2025-05-11 03:53:51] Batch 65 - Loss: 16.667505
[2025-05-11 03:53:59] Batch 70 - Loss: 16.632803
[2025-05-11 03:54:07] Batch 75 - Loss: 16.581514
[2025-05-11 03:54:14] Batch 80 - Loss: 16.525715
[2025-05-11 03:54:22] Batch 85 - Loss: 16.453695
[2025-05-11 03:54:30] Batch 90 - Loss: 16.399193
[2025-05-11 03:54:37] Batch 95 - Loss: 16.333632
[2025-05-11 03:54:49] === Epoch 55/100 Summary ===
Training Loss: 16.334961
Validation Loss: 17.143435
Learning Rate: 0.00001000
[2025-05-11 03:54:49] Generating detection visualizations for epoch 55...
[2025-05-11 03:54:49] Saved visualization to trained_models/custom_detector\visualizations\epoch_55\sample_1.png
[2025-05-11 03:54:50] Saved visualization to trained_models/custom_detector\visualizations\epoch_55\sample_2.png
[2025-05-11 03:54:50] Saved visualization to trained_models/custom_detector\visualizations\epoch_55\sample_3.png
[2025-05-11 03:54:52] Batch 0 - Loss: 17.422726
[2025-05-11 03:54:59] Batch 5 - Loss: 16.084608
[2025-05-11 03:55:07] Batch 10 - Loss: 15.988689
[2025-05-11 03:55:15] Batch 15 - Loss: 16.372971
[2025-05-11 03:55:22] Batch 20 - Loss: 16.278271
[2025-05-11 03:55:30] Batch 25 - Loss: 16.161585
[2025-05-11 03:55:38] Batch 30 - Loss: 16.055691
[2025-05-11 03:55:45] Batch 35 - Loss: 16.213615
[2025-05-11 03:55:52] === Epoch 56/100 Summary ===
Training Loss: 16.206381
Validation Loss: 16.900330
Learning Rate: 0.00001000
[2025-05-11 03:55:54] Batch 0 - Loss: 16.400841
[2025-05-11 03:56:01] Batch 5 - Loss: 17.171976
[2025-05-11 03:56:09] Batch 10 - Loss: 16.845945
[2025-05-11 03:56:17] Batch 15 - Loss: 16.763409
[2025-05-11 03:56:24] Batch 20 - Loss: 16.757460
[2025-05-11 03:56:32] Batch 25 - Loss: 16.720303
[2025-05-11 03:56:40] Batch 30 - Loss: 16.531979
[2025-05-11 03:56:48] Batch 35 - Loss: 16.598022
[2025-05-11 03:56:55] Batch 40 - Loss: 16.604849
[2025-05-11 03:57:03] Batch 45 - Loss: 16.457111
[2025-05-11 03:57:11] Batch 50 - Loss: 16.474310
[2025-05-11 03:57:18] Batch 55 - Loss: 16.358578
[2025-05-11 03:57:26] Batch 60 - Loss: 16.350399
[2025-05-11 03:57:34] Batch 65 - Loss: 16.341377
[2025-05-11 03:57:42] Batch 70 - Loss: 16.273596
[2025-05-11 03:57:49] Batch 75 - Loss: 16.177282
[2025-05-11 03:57:57] Batch 80 - Loss: 16.190584
[2025-05-11 03:58:05] Batch 85 - Loss: 16.142981
[2025-05-11 03:58:12] Batch 90 - Loss: 16.101469
[2025-05-11 03:58:20] Batch 95 - Loss: 16.095739
[2025-05-11 03:58:32] === Epoch 57/100 Summary ===
Training Loss: 16.077003
Validation Loss: 16.450731
Learning Rate: 0.00001000
[2025-05-11 03:58:33] Batch 0 - Loss: 14.942648
[2025-05-11 03:58:41] Batch 5 - Loss: 15.397003
[2025-05-11 03:58:49] Batch 10 - Loss: 15.427447
[2025-05-11 03:58:56] Batch 15 - Loss: 15.210420
[2025-05-11 03:59:04] Batch 20 - Loss: 15.441581
[2025-05-11 03:59:12] Batch 25 - Loss: 15.567469
[2025-05-11 03:59:20] Batch 30 - Loss: 15.567338
[2025-05-11 03:59:27] Batch 35 - Loss: 15.693185
[2025-05-11 03:59:35] Batch 40 - Loss: 15.661207
[2025-05-11 03:59:43] Batch 45 - Loss: 15.853944
[2025-05-11 03:59:50] Batch 50 - Loss: 15.881393
[2025-05-11 03:59:58] Batch 55 - Loss: 15.861554
[2025-05-11 04:00:06] Batch 60 - Loss: 15.896490
[2025-05-11 04:00:14] Batch 65 - Loss: 15.870265
[2025-05-11 04:00:21] Batch 70 - Loss: 15.857869
[2025-05-11 04:00:29] Batch 75 - Loss: 15.802842
[2025-05-11 04:00:37] Batch 80 - Loss: 15.789018
[2025-05-11 04:00:44] Batch 85 - Loss: 15.723825
[2025-05-11 04:00:52] Batch 90 - Loss: 15.725121
[2025-05-11 04:01:00] Batch 95 - Loss: 15.767116
[2025-05-11 04:01:11] === Epoch 58/100 Summary ===
Training Loss: 15.765975
Validation Loss: 16.112034
Learning Rate: 0.00001000
[2025-05-11 04:01:13] Batch 0 - Loss: 15.760761
[2025-05-11 04:01:21] Batch 5 - Loss: 14.944745
[2025-05-11 04:01:28] Batch 10 - Loss: 15.437857
[2025-05-11 04:01:36] Batch 15 - Loss: 15.074546
[2025-05-11 04:01:44] Batch 20 - Loss: 15.007401
[2025-05-11 04:01:51] Batch 25 - Loss: 14.816672
[2025-05-11 04:01:59] Batch 30 - Loss: 14.982121
[2025-05-11 04:02:07] Batch 35 - Loss: 14.939365
[2025-05-11 04:02:15] Batch 40 - Loss: 14.956290
[2025-05-11 04:02:22] Batch 45 - Loss: 15.023265
[2025-05-11 04:02:30] Batch 50 - Loss: 14.941895
[2025-05-11 04:02:38] Batch 55 - Loss: 15.046075
[2025-05-11 04:02:45] Batch 60 - Loss: 15.110993
[2025-05-11 04:02:53] Batch 65 - Loss: 15.116460
[2025-05-11 04:03:01] Batch 70 - Loss: 15.042195
[2025-05-11 04:03:09] Batch 75 - Loss: 15.010416
[2025-05-11 04:03:16] Batch 80 - Loss: 14.954920
[2025-05-11 04:03:24] Batch 85 - Loss: 14.931464
[2025-05-11 04:03:32] Batch 90 - Loss: 14.884245
[2025-05-11 04:03:40] Batch 95 - Loss: 14.892800
[2025-05-11 04:03:51] === Epoch 59/100 Summary ===
Training Loss: 14.922058
Validation Loss: 15.738292
Learning Rate: 0.00001000
[2025-05-11 04:03:53] Batch 0 - Loss: 16.251945
[2025-05-11 04:04:00] Batch 5 - Loss: 14.512971
[2025-05-11 04:04:08] Batch 10 - Loss: 14.528955
[2025-05-11 04:04:16] Batch 15 - Loss: 14.560702
[2025-05-11 04:04:23] Batch 20 - Loss: 14.633909
[2025-05-11 04:04:31] Batch 25 - Loss: 14.482119
[2025-05-11 04:04:39] Batch 30 - Loss: 14.359639
[2025-05-11 04:04:47] Batch 35 - Loss: 14.376875
[2025-05-11 04:04:53] === Epoch 60/100 Summary ===
Training Loss: 14.392077
Validation Loss: 15.740491
Learning Rate: 0.00001000
[2025-05-11 04:04:53] Generating detection visualizations for epoch 60...
[2025-05-11 04:04:53] Saved visualization to trained_models/custom_detector\visualizations\epoch_60\sample_1.png
[2025-05-11 04:04:53] Saved visualization to trained_models/custom_detector\visualizations\epoch_60\sample_2.png
[2025-05-11 04:04:54] Saved visualization to trained_models/custom_detector\visualizations\epoch_60\sample_3.png
[2025-05-11 04:04:56] Batch 0 - Loss: 13.755690
[2025-05-11 04:05:03] Batch 5 - Loss: 15.199505
[2025-05-11 04:05:11] Batch 10 - Loss: 15.367547
[2025-05-11 04:05:19] Batch 15 - Loss: 15.116715
[2025-05-11 04:05:26] Batch 20 - Loss: 14.963187
[2025-05-11 04:05:34] Batch 25 - Loss: 15.115565
[2025-05-11 04:05:42] Batch 30 - Loss: 15.173044
[2025-05-11 04:05:50] Batch 35 - Loss: 15.160832
[2025-05-11 04:05:57] Batch 40 - Loss: 15.202528
[2025-05-11 04:06:05] Batch 45 - Loss: 15.196427
[2025-05-11 04:06:13] Batch 50 - Loss: 15.231005
[2025-05-11 04:06:20] Batch 55 - Loss: 15.137930
[2025-05-11 04:06:28] Batch 60 - Loss: 15.052805
[2025-05-11 04:06:36] Batch 65 - Loss: 14.951912
[2025-05-11 04:06:43] Batch 70 - Loss: 14.909584
[2025-05-11 04:06:51] Batch 75 - Loss: 14.822421
[2025-05-11 04:06:59] Batch 80 - Loss: 14.750556
[2025-05-11 04:07:07] Batch 85 - Loss: 14.693197
[2025-05-11 04:07:14] Batch 90 - Loss: 14.684871
[2025-05-11 04:07:22] Batch 95 - Loss: 14.669528
[2025-05-11 04:07:34] === Epoch 61/100 Summary ===
Training Loss: 14.667716
Validation Loss: 15.160672
Learning Rate: 0.00001000
[2025-05-11 04:07:35] Batch 0 - Loss: 14.383873
[2025-05-11 04:07:43] Batch 5 - Loss: 14.372375
[2025-05-11 04:07:51] Batch 10 - Loss: 14.107046
[2025-05-11 04:07:58] Batch 15 - Loss: 14.065376
[2025-05-11 04:08:06] Batch 20 - Loss: 13.992535
[2025-05-11 04:08:14] Batch 25 - Loss: 14.149618
[2025-05-11 04:08:21] Batch 30 - Loss: 14.194708
[2025-05-11 04:08:29] Batch 35 - Loss: 14.267323
[2025-05-11 04:08:37] Batch 40 - Loss: 14.453449
[2025-05-11 04:08:44] Batch 45 - Loss: 14.462864
[2025-05-11 04:08:52] Batch 50 - Loss: 14.441843
[2025-05-11 04:09:00] Batch 55 - Loss: 14.411486
[2025-05-11 04:09:08] Batch 60 - Loss: 14.360847
[2025-05-11 04:09:15] Batch 65 - Loss: 14.352832
[2025-05-11 04:09:23] Batch 70 - Loss: 14.290503
[2025-05-11 04:09:31] Batch 75 - Loss: 14.277204
[2025-05-11 04:09:38] Batch 80 - Loss: 14.306579
[2025-05-11 04:09:46] Batch 85 - Loss: 14.302696
[2025-05-11 04:09:54] Batch 90 - Loss: 14.310415
[2025-05-11 04:10:02] Batch 95 - Loss: 14.363561
[2025-05-11 04:10:13] === Epoch 62/100 Summary ===
Training Loss: 14.365021
Validation Loss: 15.072725
Learning Rate: 0.00001000
[2025-05-11 04:10:15] Batch 0 - Loss: 13.379634
[2025-05-11 04:10:22] Batch 5 - Loss: 16.193731
[2025-05-11 04:10:30] Batch 10 - Loss: 15.261953
[2025-05-11 04:10:38] Batch 15 - Loss: 14.805350
[2025-05-11 04:10:45] Batch 20 - Loss: 14.691801
[2025-05-11 04:10:53] Batch 25 - Loss: 14.494395
[2025-05-11 04:11:01] Batch 30 - Loss: 14.330876
[2025-05-11 04:11:08] Batch 35 - Loss: 14.417960
[2025-05-11 04:11:16] Batch 40 - Loss: 14.341215
[2025-05-11 04:11:24] Batch 45 - Loss: 14.273338
[2025-05-11 04:11:32] Batch 50 - Loss: 14.212850
[2025-05-11 04:11:39] Batch 55 - Loss: 14.154802
[2025-05-11 04:11:47] Batch 60 - Loss: 14.166106
[2025-05-11 04:11:55] Batch 65 - Loss: 14.100904
[2025-05-11 04:12:02] Batch 70 - Loss: 13.984336
[2025-05-11 04:12:10] Batch 75 - Loss: 13.972755
[2025-05-11 04:12:18] Batch 80 - Loss: 13.913594
[2025-05-11 04:12:25] Batch 85 - Loss: 13.920015
[2025-05-11 04:12:33] Batch 90 - Loss: 13.862329
[2025-05-11 04:12:41] Batch 95 - Loss: 13.848587
[2025-05-11 04:12:52] === Epoch 63/100 Summary ===
Training Loss: 13.812164
Validation Loss: 14.738533
Learning Rate: 0.00001000
[2025-05-11 04:12:54] Batch 0 - Loss: 13.039387
[2025-05-11 04:13:01] Batch 5 - Loss: 14.013977
[2025-05-11 04:13:09] Batch 10 - Loss: 13.582651
[2025-05-11 04:13:17] Batch 15 - Loss: 13.541772
[2025-05-11 04:13:24] Batch 20 - Loss: 13.278590
[2025-05-11 04:13:32] Batch 25 - Loss: 13.255342
[2025-05-11 04:13:40] Batch 30 - Loss: 13.280480
[2025-05-11 04:13:48] Batch 35 - Loss: 13.272283
[2025-05-11 04:13:54] === Epoch 64/100 Summary ===
Training Loss: 13.335192
Validation Loss: 14.462519
Learning Rate: 0.00001000
[2025-05-11 04:13:56] Batch 0 - Loss: 14.249103
[2025-05-11 04:14:03] Batch 5 - Loss: 13.157792
[2025-05-11 04:14:11] Batch 10 - Loss: 13.803564
[2025-05-11 04:14:19] Batch 15 - Loss: 13.541352
[2025-05-11 04:14:27] Batch 20 - Loss: 13.567158
[2025-05-11 04:14:34] Batch 25 - Loss: 13.711791
[2025-05-11 04:14:42] Batch 30 - Loss: 13.871538
[2025-05-11 04:14:50] Batch 35 - Loss: 13.952484
[2025-05-11 04:14:57] Batch 40 - Loss: 14.003641
[2025-05-11 04:15:05] Batch 45 - Loss: 13.999164
[2025-05-11 04:15:13] Batch 50 - Loss: 14.028331
[2025-05-11 04:15:20] Batch 55 - Loss: 13.931397
[2025-05-11 04:15:28] Batch 60 - Loss: 13.909115
[2025-05-11 04:15:36] Batch 65 - Loss: 13.910368
[2025-05-11 04:15:44] Batch 70 - Loss: 13.826626
[2025-05-11 04:15:51] Batch 75 - Loss: 13.816566
[2025-05-11 04:15:59] Batch 80 - Loss: 13.774411
[2025-05-11 04:16:07] Batch 85 - Loss: 13.695086
[2025-05-11 04:16:14] Batch 90 - Loss: 13.659019
[2025-05-11 04:16:22] Batch 95 - Loss: 13.625023
[2025-05-11 04:16:34] === Epoch 65/100 Summary ===
Training Loss: 13.585572
Validation Loss: 14.125082
Learning Rate: 0.00001000
[2025-05-11 04:16:34] Generating detection visualizations for epoch 65...
[2025-05-11 04:16:34] Saved visualization to trained_models/custom_detector\visualizations\epoch_65\sample_1.png
[2025-05-11 04:16:34] Saved visualization to trained_models/custom_detector\visualizations\epoch_65\sample_2.png
[2025-05-11 04:16:35] Saved visualization to trained_models/custom_detector\visualizations\epoch_65\sample_3.png
[2025-05-11 04:16:36] Batch 0 - Loss: 13.894760
[2025-05-11 04:16:44] Batch 5 - Loss: 13.849250
[2025-05-11 04:16:52] Batch 10 - Loss: 13.625341
[2025-05-11 04:16:59] Batch 15 - Loss: 13.483708
[2025-05-11 04:17:07] Batch 20 - Loss: 13.383551
[2025-05-11 04:17:15] Batch 25 - Loss: 13.368804
[2025-05-11 04:17:23] Batch 30 - Loss: 13.447631
[2025-05-11 04:17:30] Batch 35 - Loss: 13.468965
[2025-05-11 04:17:38] Batch 40 - Loss: 13.525187
[2025-05-11 04:17:46] Batch 45 - Loss: 13.602098
[2025-05-11 04:17:53] Batch 50 - Loss: 13.501684
[2025-05-11 04:18:01] Batch 55 - Loss: 13.557823
[2025-05-11 04:18:09] Batch 60 - Loss: 13.592066
[2025-05-11 04:18:16] Batch 65 - Loss: 13.641270
[2025-05-11 04:18:24] Batch 70 - Loss: 13.599277
[2025-05-11 04:18:32] Batch 75 - Loss: 13.580292
[2025-05-11 04:18:40] Batch 80 - Loss: 13.550550
[2025-05-11 04:18:47] Batch 85 - Loss: 13.530580
[2025-05-11 04:18:55] Batch 90 - Loss: 13.513504
[2025-05-11 04:19:03] Batch 95 - Loss: 13.519878
[2025-05-11 04:19:14] === Epoch 66/100 Summary ===
Training Loss: 13.506531
Validation Loss: 13.994481
Learning Rate: 0.00001000
[2025-05-11 04:19:16] Batch 0 - Loss: 14.956209
[2025-05-11 04:19:23] Batch 5 - Loss: 13.103858
[2025-05-11 04:19:31] Batch 10 - Loss: 13.104476
[2025-05-11 04:19:39] Batch 15 - Loss: 13.480322
[2025-05-11 04:19:46] Batch 20 - Loss: 13.170006
[2025-05-11 04:19:54] Batch 25 - Loss: 13.051870
[2025-05-11 04:20:02] Batch 30 - Loss: 13.100532
[2025-05-11 04:20:10] Batch 35 - Loss: 13.059109
[2025-05-11 04:20:17] Batch 40 - Loss: 13.007646
[2025-05-11 04:20:25] Batch 45 - Loss: 13.091495
[2025-05-11 04:20:33] Batch 50 - Loss: 13.054243
[2025-05-11 04:20:40] Batch 55 - Loss: 13.027315
[2025-05-11 04:20:48] Batch 60 - Loss: 13.037417
[2025-05-11 04:20:56] Batch 65 - Loss: 13.020058
[2025-05-11 04:21:04] Batch 70 - Loss: 12.988697
[2025-05-11 04:21:11] Batch 75 - Loss: 12.904158
[2025-05-11 04:21:19] Batch 80 - Loss: 12.855832
[2025-05-11 04:21:27] Batch 85 - Loss: 12.778119
[2025-05-11 04:21:34] Batch 90 - Loss: 12.751575
[2025-05-11 04:21:42] Batch 95 - Loss: 12.687717
[2025-05-11 04:21:54] === Epoch 67/100 Summary ===
Training Loss: 12.695906
Validation Loss: 13.773966
Learning Rate: 0.00001000
[2025-05-11 04:21:55] Batch 0 - Loss: 13.080549
[2025-05-11 04:22:03] Batch 5 - Loss: 11.936596
[2025-05-11 04:22:11] Batch 10 - Loss: 12.218762
[2025-05-11 04:22:18] Batch 15 - Loss: 12.178307
[2025-05-11 04:22:26] Batch 20 - Loss: 12.162980
[2025-05-11 04:22:34] Batch 25 - Loss: 12.356581
[2025-05-11 04:22:41] Batch 30 - Loss: 12.219189
[2025-05-11 04:22:49] Batch 35 - Loss: 12.209682
[2025-05-11 04:22:55] === Epoch 68/100 Summary ===
Training Loss: 12.205037
Validation Loss: 13.660048
Learning Rate: 0.00001000
[2025-05-11 04:22:57] Batch 0 - Loss: 13.737128
[2025-05-11 04:23:05] Batch 5 - Loss: 12.734942
[2025-05-11 04:23:13] Batch 10 - Loss: 13.381577
[2025-05-11 04:23:20] Batch 15 - Loss: 13.632961
[2025-05-11 04:23:28] Batch 20 - Loss: 13.334062
[2025-05-11 04:23:36] Batch 25 - Loss: 13.308086
[2025-05-11 04:23:43] Batch 30 - Loss: 13.167712
[2025-05-11 04:23:51] Batch 35 - Loss: 12.940197
[2025-05-11 04:23:59] Batch 40 - Loss: 13.002880
[2025-05-11 04:24:07] Batch 45 - Loss: 12.916710
[2025-05-11 04:24:14] Batch 50 - Loss: 12.872990
[2025-05-11 04:24:22] Batch 55 - Loss: 12.799964
[2025-05-11 04:24:30] Batch 60 - Loss: 12.798357
[2025-05-11 04:24:37] Batch 65 - Loss: 12.835084
[2025-05-11 04:24:45] Batch 70 - Loss: 12.871518
[2025-05-11 04:24:53] Batch 75 - Loss: 12.830411
[2025-05-11 04:25:01] Batch 80 - Loss: 12.771001
[2025-05-11 04:25:08] Batch 85 - Loss: 12.770725
[2025-05-11 04:25:16] Batch 90 - Loss: 12.736494
[2025-05-11 04:25:24] Batch 95 - Loss: 12.728169
[2025-05-11 04:25:35] === Epoch 69/100 Summary ===
Training Loss: 12.690274
Validation Loss: 13.451731
Learning Rate: 0.00001000
[2025-05-11 04:25:37] Batch 0 - Loss: 13.123026
[2025-05-11 04:25:45] Batch 5 - Loss: 12.411693
[2025-05-11 04:25:53] Batch 10 - Loss: 12.093525
[2025-05-11 04:26:00] Batch 15 - Loss: 12.581955
[2025-05-11 04:26:08] Batch 20 - Loss: 12.427241
[2025-05-11 04:26:16] Batch 25 - Loss: 12.480247
[2025-05-11 04:26:23] Batch 30 - Loss: 12.548457
[2025-05-11 04:26:31] Batch 35 - Loss: 12.487146
[2025-05-11 04:26:39] Batch 40 - Loss: 12.500445
[2025-05-11 04:26:46] Batch 45 - Loss: 12.623572
[2025-05-11 04:26:54] Batch 50 - Loss: 12.514320
[2025-05-11 04:27:02] Batch 55 - Loss: 12.430161
[2025-05-11 04:27:10] Batch 60 - Loss: 12.463692
[2025-05-11 04:27:17] Batch 65 - Loss: 12.473910
[2025-05-11 04:27:25] Batch 70 - Loss: 12.485740
[2025-05-11 04:27:33] Batch 75 - Loss: 12.538699
[2025-05-11 04:27:40] Batch 80 - Loss: 12.582180
[2025-05-11 04:27:48] Batch 85 - Loss: 12.575361
[2025-05-11 04:27:56] Batch 90 - Loss: 12.563797
[2025-05-11 04:28:03] Batch 95 - Loss: 12.574215
[2025-05-11 04:28:15] === Epoch 70/100 Summary ===
Training Loss: 12.557398
Validation Loss: 13.220419
Learning Rate: 0.00001000
[2025-05-11 04:28:15] Generating detection visualizations for epoch 70...
[2025-05-11 04:28:15] Saved visualization to trained_models/custom_detector\visualizations\epoch_70\sample_1.png
[2025-05-11 04:28:16] Saved visualization to trained_models/custom_detector\visualizations\epoch_70\sample_2.png
[2025-05-11 04:28:16] Saved visualization to trained_models/custom_detector\visualizations\epoch_70\sample_3.png
[2025-05-11 04:28:18] Batch 0 - Loss: 11.696653
[2025-05-11 04:28:25] Batch 5 - Loss: 12.971667
[2025-05-11 04:28:33] Batch 10 - Loss: 12.090035
[2025-05-11 04:28:41] Batch 15 - Loss: 12.030015
[2025-05-11 04:28:48] Batch 20 - Loss: 12.098259
[2025-05-11 04:28:56] Batch 25 - Loss: 12.136825
[2025-05-11 04:29:04] Batch 30 - Loss: 12.232764
[2025-05-11 04:29:11] Batch 35 - Loss: 12.260976
[2025-05-11 04:29:19] Batch 40 - Loss: 12.158830
[2025-05-11 04:29:27] Batch 45 - Loss: 12.229200
[2025-05-11 04:29:34] Batch 50 - Loss: 12.201119
[2025-05-11 04:29:42] Batch 55 - Loss: 12.151900
[2025-05-11 04:29:50] Batch 60 - Loss: 12.083402
[2025-05-11 04:29:58] Batch 65 - Loss: 12.065899
[2025-05-11 04:30:05] Batch 70 - Loss: 12.045592
[2025-05-11 04:30:13] Batch 75 - Loss: 12.020060
[2025-05-11 04:30:21] Batch 80 - Loss: 12.092291
[2025-05-11 04:30:28] Batch 85 - Loss: 12.096024
[2025-05-11 04:30:36] Batch 90 - Loss: 12.096049
[2025-05-11 04:30:43] Batch 95 - Loss: 12.055553
[2025-05-11 04:30:55] === Epoch 71/100 Summary ===
Training Loss: 11.986503
Validation Loss: 13.029569
Learning Rate: 0.00001000
[2025-05-11 04:30:57] Batch 0 - Loss: 11.523330
[2025-05-11 04:31:04] Batch 5 - Loss: 11.368773
[2025-05-11 04:31:12] Batch 10 - Loss: 11.610340
[2025-05-11 04:31:20] Batch 15 - Loss: 11.609999
[2025-05-11 04:31:27] Batch 20 - Loss: 11.688659
[2025-05-11 04:31:35] Batch 25 - Loss: 11.623663
[2025-05-11 04:31:43] Batch 30 - Loss: 11.653902
[2025-05-11 04:31:50] Batch 35 - Loss: 11.627743
[2025-05-11 04:31:57] === Epoch 72/100 Summary ===
Training Loss: 11.633572
Validation Loss: 12.970011
Learning Rate: 0.00001000
[2025-05-11 04:31:59] Batch 0 - Loss: 11.239030
[2025-05-11 04:32:06] Batch 5 - Loss: 11.970670
[2025-05-11 04:32:14] Batch 10 - Loss: 11.715567
[2025-05-11 04:32:22] Batch 15 - Loss: 11.615671
[2025-05-11 04:32:29] Batch 20 - Loss: 11.859715
[2025-05-11 04:32:37] Batch 25 - Loss: 11.960764
[2025-05-11 04:32:45] Batch 30 - Loss: 12.084537
[2025-05-11 04:32:52] Batch 35 - Loss: 12.077023
[2025-05-11 04:33:00] Batch 40 - Loss: 12.125900
[2025-05-11 04:33:08] Batch 45 - Loss: 12.232882
[2025-05-11 04:33:15] Batch 50 - Loss: 12.237785
[2025-05-11 04:33:23] Batch 55 - Loss: 12.304141
[2025-05-11 04:33:31] Batch 60 - Loss: 12.330090
[2025-05-11 04:33:39] Batch 65 - Loss: 12.244227
[2025-05-11 04:33:46] Batch 70 - Loss: 12.225060
[2025-05-11 04:33:54] Batch 75 - Loss: 12.169355
[2025-05-11 04:34:02] Batch 80 - Loss: 12.120959
[2025-05-11 04:34:09] Batch 85 - Loss: 12.060780
[2025-05-11 04:34:17] Batch 90 - Loss: 12.026188
[2025-05-11 04:34:25] Batch 95 - Loss: 12.048705
[2025-05-11 04:34:36] === Epoch 73/100 Summary ===
Training Loss: 12.031678
Validation Loss: 12.836243
Learning Rate: 0.00001000
[2025-05-11 04:34:38] Batch 0 - Loss: 11.654517
[2025-05-11 04:34:45] Batch 5 - Loss: 11.902974
[2025-05-11 04:34:53] Batch 10 - Loss: 11.689273
[2025-05-11 04:35:01] Batch 15 - Loss: 11.670393
[2025-05-11 04:35:08] Batch 20 - Loss: 11.723961
[2025-05-11 04:35:16] Batch 25 - Loss: 11.646879
[2025-05-11 04:35:24] Batch 30 - Loss: 11.517186
[2025-05-11 04:35:32] Batch 35 - Loss: 11.565874
[2025-05-11 04:35:39] Batch 40 - Loss: 11.701649
[2025-05-11 04:35:47] Batch 45 - Loss: 11.881185
[2025-05-11 04:35:55] Batch 50 - Loss: 11.845861
[2025-05-11 04:36:02] Batch 55 - Loss: 11.902003
[2025-05-11 04:36:10] Batch 60 - Loss: 11.927587
[2025-05-11 04:36:18] Batch 65 - Loss: 11.915362
[2025-05-11 04:36:25] Batch 70 - Loss: 11.932033
[2025-05-11 04:36:33] Batch 75 - Loss: 11.947106
[2025-05-11 04:36:41] Batch 80 - Loss: 11.975192
[2025-05-11 04:36:49] Batch 85 - Loss: 11.895552
[2025-05-11 04:36:56] Batch 90 - Loss: 11.857715
[2025-05-11 04:37:04] Batch 95 - Loss: 11.791322
[2025-05-11 04:37:16] === Epoch 74/100 Summary ===
Training Loss: 11.769161
Validation Loss: 12.663351
Learning Rate: 0.00001000
[2025-05-11 04:37:17] Batch 0 - Loss: 10.590264
[2025-05-11 04:37:25] Batch 5 - Loss: 11.723674
[2025-05-11 04:37:33] Batch 10 - Loss: 11.556624
[2025-05-11 04:37:40] Batch 15 - Loss: 11.530693
[2025-05-11 04:37:48] Batch 20 - Loss: 11.440142
[2025-05-11 04:37:56] Batch 25 - Loss: 11.546681
[2025-05-11 04:38:03] Batch 30 - Loss: 11.722971
[2025-05-11 04:38:11] Batch 35 - Loss: 11.842874
[2025-05-11 04:38:19] Batch 40 - Loss: 11.707194
[2025-05-11 04:38:27] Batch 45 - Loss: 11.704173
[2025-05-11 04:38:34] Batch 50 - Loss: 11.698030
[2025-05-11 04:38:42] Batch 55 - Loss: 11.637626
[2025-05-11 04:38:50] Batch 60 - Loss: 11.630767
[2025-05-11 04:38:57] Batch 65 - Loss: 11.603354
[2025-05-11 04:39:05] Batch 70 - Loss: 11.562475
[2025-05-11 04:39:13] Batch 75 - Loss: 11.575810
[2025-05-11 04:39:20] Batch 80 - Loss: 11.561235
[2025-05-11 04:39:28] Batch 85 - Loss: 11.542450
[2025-05-11 04:39:36] Batch 90 - Loss: 11.484297
[2025-05-11 04:39:44] Batch 95 - Loss: 11.498874
[2025-05-11 04:39:55] === Epoch 75/100 Summary ===
Training Loss: 11.497780
Validation Loss: 12.519516
Learning Rate: 0.00001000
[2025-05-11 04:39:55] Generating detection visualizations for epoch 75...
[2025-05-11 04:39:56] Saved visualization to trained_models/custom_detector\visualizations\epoch_75\sample_1.png
[2025-05-11 04:39:56] Saved visualization to trained_models/custom_detector\visualizations\epoch_75\sample_2.png
[2025-05-11 04:39:56] Saved visualization to trained_models/custom_detector\visualizations\epoch_75\sample_3.png
[2025-05-11 04:39:58] Batch 0 - Loss: 10.289197
[2025-05-11 04:40:05] Batch 5 - Loss: 11.328526
[2025-05-11 04:40:13] Batch 10 - Loss: 11.013332
[2025-05-11 04:40:21] Batch 15 - Loss: 11.063066
[2025-05-11 04:40:28] Batch 20 - Loss: 10.972316
[2025-05-11 04:40:36] Batch 25 - Loss: 10.847503
[2025-05-11 04:40:44] Batch 30 - Loss: 10.855776
[2025-05-11 04:40:51] Batch 35 - Loss: 10.896223
[2025-05-11 04:40:58] === Epoch 76/100 Summary ===
Training Loss: 10.885853
Validation Loss: 12.448828
Learning Rate: 0.00001000
[2025-05-11 04:41:00] Batch 0 - Loss: 13.259941
[2025-05-11 04:41:07] Batch 5 - Loss: 11.739349
[2025-05-11 04:41:15] Batch 10 - Loss: 11.525860
[2025-05-11 04:41:23] Batch 15 - Loss: 11.429811
[2025-05-11 04:41:30] Batch 20 - Loss: 11.648132
[2025-05-11 04:41:38] Batch 25 - Loss: 11.523244
[2025-05-11 04:41:46] Batch 30 - Loss: 11.440946
[2025-05-11 04:41:54] Batch 35 - Loss: 11.436704
[2025-05-11 04:42:01] Batch 40 - Loss: 11.425343
[2025-05-11 04:42:09] Batch 45 - Loss: 11.440436
[2025-05-11 04:42:17] Batch 50 - Loss: 11.470083
[2025-05-11 04:42:24] Batch 55 - Loss: 11.411904
[2025-05-11 04:42:32] Batch 60 - Loss: 11.452013
[2025-05-11 04:42:40] Batch 65 - Loss: 11.497678
[2025-05-11 04:42:47] Batch 70 - Loss: 11.431610
[2025-05-11 04:42:55] Batch 75 - Loss: 11.341063
[2025-05-11 04:43:03] Batch 80 - Loss: 11.305781
[2025-05-11 04:43:10] Batch 85 - Loss: 11.273911
[2025-05-11 04:43:18] Batch 90 - Loss: 11.259201
[2025-05-11 04:43:26] Batch 95 - Loss: 11.269490
[2025-05-11 04:43:37] === Epoch 77/100 Summary ===
Training Loss: 11.341408
Validation Loss: 12.345988
Learning Rate: 0.00001000
[2025-05-11 04:43:39] Batch 0 - Loss: 9.874464
[2025-05-11 04:43:47] Batch 5 - Loss: 10.789065
[2025-05-11 04:43:54] Batch 10 - Loss: 10.926533
[2025-05-11 04:44:02] Batch 15 - Loss: 10.891888
[2025-05-11 04:44:10] Batch 20 - Loss: 11.096636
[2025-05-11 04:44:18] Batch 25 - Loss: 11.220358
[2025-05-11 04:44:25] Batch 30 - Loss: 11.191808
[2025-05-11 04:44:33] Batch 35 - Loss: 11.124005
[2025-05-11 04:44:41] Batch 40 - Loss: 11.227121
[2025-05-11 04:44:48] Batch 45 - Loss: 11.352105
[2025-05-11 04:44:56] Batch 50 - Loss: 11.418216
[2025-05-11 04:45:04] Batch 55 - Loss: 11.478581
[2025-05-11 04:45:12] Batch 60 - Loss: 11.456547
[2025-05-11 04:45:19] Batch 65 - Loss: 11.428319
[2025-05-11 04:45:27] Batch 70 - Loss: 11.445557
[2025-05-11 04:45:35] Batch 75 - Loss: 11.410785
[2025-05-11 04:45:42] Batch 80 - Loss: 11.390182
[2025-05-11 04:45:50] Batch 85 - Loss: 11.344198
[2025-05-11 04:45:58] Batch 90 - Loss: 11.333117
[2025-05-11 04:46:05] Batch 95 - Loss: 11.371750
[2025-05-11 04:46:17] === Epoch 78/100 Summary ===
Training Loss: 11.330256
Validation Loss: 12.224460
Learning Rate: 0.00001000
[2025-05-11 04:46:18] Batch 0 - Loss: 8.903752
[2025-05-11 04:46:26] Batch 5 - Loss: 11.005333
[2025-05-11 04:46:34] Batch 10 - Loss: 10.938691
[2025-05-11 04:46:42] Batch 15 - Loss: 11.246408
[2025-05-11 04:46:49] Batch 20 - Loss: 11.323955
[2025-05-11 04:46:57] Batch 25 - Loss: 11.246367
[2025-05-11 04:47:05] Batch 30 - Loss: 11.129490
[2025-05-11 04:47:12] Batch 35 - Loss: 11.242604
[2025-05-11 04:47:20] Batch 40 - Loss: 11.162766
[2025-05-11 04:47:28] Batch 45 - Loss: 11.231256
[2025-05-11 04:47:36] Batch 50 - Loss: 11.301672
[2025-05-11 04:47:43] Batch 55 - Loss: 11.271407
[2025-05-11 04:47:51] Batch 60 - Loss: 11.221805
[2025-05-11 04:47:59] Batch 65 - Loss: 11.188862
[2025-05-11 04:48:06] Batch 70 - Loss: 11.213432
[2025-05-11 04:48:14] Batch 75 - Loss: 11.177866
[2025-05-11 04:48:22] Batch 80 - Loss: 11.147476
[2025-05-11 04:48:29] Batch 85 - Loss: 11.108682
[2025-05-11 04:48:37] Batch 90 - Loss: 11.030057
[2025-05-11 04:48:45] Batch 95 - Loss: 11.011901
[2025-05-11 04:48:56] === Epoch 79/100 Summary ===
Training Loss: 10.991548
Validation Loss: 12.097463
Learning Rate: 0.00001000
[2025-05-11 04:48:58] Batch 0 - Loss: 11.792298
[2025-05-11 04:49:06] Batch 5 - Loss: 11.324036
[2025-05-11 04:49:13] Batch 10 - Loss: 10.720161
[2025-05-11 04:49:21] Batch 15 - Loss: 10.553102
[2025-05-11 04:49:29] Batch 20 - Loss: 10.619270
[2025-05-11 04:49:36] Batch 25 - Loss: 10.627376
[2025-05-11 04:49:44] Batch 30 - Loss: 10.708774
[2025-05-11 04:49:52] Batch 35 - Loss: 10.711180
[2025-05-11 04:49:58] === Epoch 80/100 Summary ===
Training Loss: 10.747861
Validation Loss: 12.082153
Learning Rate: 0.00001000
[2025-05-11 04:49:58] Generating detection visualizations for epoch 80...
[2025-05-11 04:49:58] Saved visualization to trained_models/custom_detector\visualizations\epoch_80\sample_1.png
[2025-05-11 04:49:59] Saved visualization to trained_models/custom_detector\visualizations\epoch_80\sample_2.png
[2025-05-11 04:49:59] Saved visualization to trained_models/custom_detector\visualizations\epoch_80\sample_3.png
[2025-05-11 04:50:01] Batch 0 - Loss: 11.923935
[2025-05-11 04:50:09] Batch 5 - Loss: 10.976158
[2025-05-11 04:50:16] Batch 10 - Loss: 11.455598
[2025-05-11 04:50:24] Batch 15 - Loss: 11.607048
[2025-05-11 04:50:32] Batch 20 - Loss: 11.630419
[2025-05-11 04:50:39] Batch 25 - Loss: 11.366909
[2025-05-11 04:50:47] Batch 30 - Loss: 11.286840
[2025-05-11 04:50:55] Batch 35 - Loss: 11.338677
[2025-05-11 04:51:02] Batch 40 - Loss: 11.365270
[2025-05-11 04:51:10] Batch 45 - Loss: 11.390445
[2025-05-11 04:51:18] Batch 50 - Loss: 11.303941
[2025-05-11 04:51:26] Batch 55 - Loss: 11.216891
[2025-05-11 04:51:33] Batch 60 - Loss: 11.207668
[2025-05-11 04:51:41] Batch 65 - Loss: 11.236264
[2025-05-11 04:51:49] Batch 70 - Loss: 11.246547
[2025-05-11 04:51:56] Batch 75 - Loss: 11.111092
[2025-05-11 04:52:04] Batch 80 - Loss: 11.113060
[2025-05-11 04:52:12] Batch 85 - Loss: 11.061051
[2025-05-11 04:52:20] Batch 90 - Loss: 11.050447
[2025-05-11 04:52:27] Batch 95 - Loss: 11.095380
[2025-05-11 04:52:39] === Epoch 81/100 Summary ===
Training Loss: 11.065668
Validation Loss: 11.988911
Learning Rate: 0.00001000
[2025-05-11 04:52:40] Batch 0 - Loss: 10.107660
[2025-05-11 04:52:48] Batch 5 - Loss: 10.999099
[2025-05-11 04:52:56] Batch 10 - Loss: 11.029428
[2025-05-11 04:53:03] Batch 15 - Loss: 11.289810
[2025-05-11 04:53:11] Batch 20 - Loss: 10.853298
[2025-05-11 04:53:19] Batch 25 - Loss: 10.741564
[2025-05-11 04:53:27] Batch 30 - Loss: 10.626455
[2025-05-11 04:53:34] Batch 35 - Loss: 10.496492
[2025-05-11 04:53:42] Batch 40 - Loss: 10.623962
[2025-05-11 04:53:50] Batch 45 - Loss: 10.701212
[2025-05-11 04:53:58] Batch 50 - Loss: 10.779397
[2025-05-11 04:54:05] Batch 55 - Loss: 10.873195
[2025-05-11 04:54:13] Batch 60 - Loss: 10.822999
[2025-05-11 04:54:21] Batch 65 - Loss: 10.765718
[2025-05-11 04:54:28] Batch 70 - Loss: 10.724549
[2025-05-11 04:54:36] Batch 75 - Loss: 10.717441
[2025-05-11 04:54:44] Batch 80 - Loss: 10.768600
[2025-05-11 04:54:52] Batch 85 - Loss: 10.748893
[2025-05-11 04:54:59] Batch 90 - Loss: 10.764146
[2025-05-11 04:55:07] Batch 95 - Loss: 10.771554
[2025-05-11 04:55:18] === Epoch 82/100 Summary ===
Training Loss: 10.786761
Validation Loss: 11.908139
Learning Rate: 0.00001000
[2025-05-11 04:55:20] Batch 0 - Loss: 9.555992
[2025-05-11 04:55:28] Batch 5 - Loss: 10.951660
[2025-05-11 04:55:35] Batch 10 - Loss: 11.216170
[2025-05-11 04:55:43] Batch 15 - Loss: 11.195463
[2025-05-11 04:55:51] Batch 20 - Loss: 11.356994
[2025-05-11 04:55:59] Batch 25 - Loss: 11.355626
[2025-05-11 04:56:06] Batch 30 - Loss: 11.263055
[2025-05-11 04:56:14] Batch 35 - Loss: 11.163080
[2025-05-11 04:56:22] Batch 40 - Loss: 11.035359
[2025-05-11 04:56:29] Batch 45 - Loss: 10.965450
[2025-05-11 04:56:37] Batch 50 - Loss: 10.974450
[2025-05-11 04:56:45] Batch 55 - Loss: 10.887197
[2025-05-11 04:56:53] Batch 60 - Loss: 10.834439
[2025-05-11 04:57:00] Batch 65 - Loss: 10.766530
[2025-05-11 04:57:08] Batch 70 - Loss: 10.698946
[2025-05-11 04:57:16] Batch 75 - Loss: 10.647462
[2025-05-11 04:57:23] Batch 80 - Loss: 10.659177
[2025-05-11 04:57:31] Batch 85 - Loss: 10.667891
[2025-05-11 04:57:39] Batch 90 - Loss: 10.647753
[2025-05-11 04:57:47] Batch 95 - Loss: 10.623078
[2025-05-11 04:57:58] === Epoch 83/100 Summary ===
Training Loss: 10.598396
Validation Loss: 11.866144
Learning Rate: 0.00001000
[2025-05-11 04:58:00] Batch 0 - Loss: 11.194950
[2025-05-11 04:58:07] Batch 5 - Loss: 10.062851
[2025-05-11 04:58:15] Batch 10 - Loss: 10.169365
[2025-05-11 04:58:23] Batch 15 - Loss: 10.201402
[2025-05-11 04:58:30] Batch 20 - Loss: 10.281620
[2025-05-11 04:58:38] Batch 25 - Loss: 10.413303
[2025-05-11 04:58:46] Batch 30 - Loss: 10.382771
[2025-05-11 04:58:54] Batch 35 - Loss: 10.302999
[2025-05-11 04:59:00] === Epoch 84/100 Summary ===
Training Loss: 10.316218
Validation Loss: 11.828841
Learning Rate: 0.00001000
[2025-05-11 04:59:02] Batch 0 - Loss: 10.865202
[2025-05-11 04:59:09] Batch 5 - Loss: 10.333821
[2025-05-11 04:59:17] Batch 10 - Loss: 10.923234
[2025-05-11 04:59:25] Batch 15 - Loss: 10.745195
[2025-05-11 04:59:33] Batch 20 - Loss: 10.865459
[2025-05-11 04:59:40] Batch 25 - Loss: 11.036799
[2025-05-11 04:59:48] Batch 30 - Loss: 10.988867
[2025-05-11 04:59:56] Batch 35 - Loss: 10.975099
[2025-05-11 05:00:03] Batch 40 - Loss: 10.990453
[2025-05-11 05:00:11] Batch 45 - Loss: 10.955641
[2025-05-11 05:00:19] Batch 50 - Loss: 11.010442
[2025-05-11 05:00:27] Batch 55 - Loss: 11.022890
[2025-05-11 05:00:34] Batch 60 - Loss: 11.045840
[2025-05-11 05:00:42] Batch 65 - Loss: 11.097996
[2025-05-11 05:00:50] Batch 70 - Loss: 11.061115
[2025-05-11 05:00:57] Batch 75 - Loss: 10.956780
[2025-05-11 05:01:05] Batch 80 - Loss: 10.906200
[2025-05-11 05:01:13] Batch 85 - Loss: 10.872363
[2025-05-11 05:01:20] Batch 90 - Loss: 10.877769
[2025-05-11 05:01:28] Batch 95 - Loss: 10.817608
[2025-05-11 05:01:40] === Epoch 85/100 Summary ===
Training Loss: 10.766768
Validation Loss: 11.771883
Learning Rate: 0.00001000
[2025-05-11 05:01:40] Generating detection visualizations for epoch 85...
[2025-05-11 05:01:40] Saved visualization to trained_models/custom_detector\visualizations\epoch_85\sample_1.png
[2025-05-11 05:01:41] Saved visualization to trained_models/custom_detector\visualizations\epoch_85\sample_2.png
[2025-05-11 05:01:41] Saved visualization to trained_models/custom_detector\visualizations\epoch_85\sample_3.png
[2025-05-11 05:01:43] Batch 0 - Loss: 10.433327
[2025-05-11 05:01:50] Batch 5 - Loss: 10.555020
[2025-05-11 05:01:58] Batch 10 - Loss: 10.808474
[2025-05-11 05:02:06] Batch 15 - Loss: 10.764472
[2025-05-11 05:02:14] Batch 20 - Loss: 10.593007
[2025-05-11 05:02:22] Batch 25 - Loss: 10.551901
[2025-05-11 05:02:29] Batch 30 - Loss: 10.405858
[2025-05-11 05:02:37] Batch 35 - Loss: 10.435024
[2025-05-11 05:02:45] Batch 40 - Loss: 10.510090
[2025-05-11 05:02:53] Batch 45 - Loss: 10.498273
[2025-05-11 05:03:00] Batch 50 - Loss: 10.513943
[2025-05-11 05:03:08] Batch 55 - Loss: 10.538161
[2025-05-11 05:03:16] Batch 60 - Loss: 10.485375
[2025-05-11 05:03:24] Batch 65 - Loss: 10.490414
[2025-05-11 05:03:32] Batch 70 - Loss: 10.499451
[2025-05-11 05:03:39] Batch 75 - Loss: 10.505201
[2025-05-11 05:03:47] Batch 80 - Loss: 10.465599
[2025-05-11 05:03:55] Batch 85 - Loss: 10.500609
[2025-05-11 05:04:03] Batch 90 - Loss: 10.478243
[2025-05-11 05:04:10] Batch 95 - Loss: 10.441401
[2025-05-11 05:04:22] === Epoch 86/100 Summary ===
Training Loss: 10.463140
Validation Loss: 11.705599
Learning Rate: 0.00001000
[2025-05-11 05:04:24] Batch 0 - Loss: 10.404639
[2025-05-11 05:04:31] Batch 5 - Loss: 10.520236
[2025-05-11 05:04:39] Batch 10 - Loss: 10.460373
[2025-05-11 05:04:47] Batch 15 - Loss: 10.468379
[2025-05-11 05:04:55] Batch 20 - Loss: 10.443105
[2025-05-11 05:05:02] Batch 25 - Loss: 10.753664
[2025-05-11 05:05:10] Batch 30 - Loss: 10.847576
[2025-05-11 05:05:18] Batch 35 - Loss: 10.724563
[2025-05-11 05:05:26] Batch 40 - Loss: 10.672579
[2025-05-11 05:05:33] Batch 45 - Loss: 10.614858
[2025-05-11 05:05:41] Batch 50 - Loss: 10.678381
[2025-05-11 05:05:49] Batch 55 - Loss: 10.608160
[2025-05-11 05:05:57] Batch 60 - Loss: 10.539574
[2025-05-11 05:06:04] Batch 65 - Loss: 10.558775
[2025-05-11 05:06:12] Batch 70 - Loss: 10.533479
[2025-05-11 05:06:20] Batch 75 - Loss: 10.497625
[2025-05-11 05:06:28] Batch 80 - Loss: 10.493086
[2025-05-11 05:06:35] Batch 85 - Loss: 10.424368
[2025-05-11 05:06:43] Batch 90 - Loss: 10.370189
[2025-05-11 05:06:51] Batch 95 - Loss: 10.361133
[2025-05-11 05:07:03] === Epoch 87/100 Summary ===
Training Loss: 10.302495
Validation Loss: 11.641685
Learning Rate: 0.00001000
[2025-05-11 05:07:04] Batch 0 - Loss: 9.250145
[2025-05-11 05:07:12] Batch 5 - Loss: 10.004647
[2025-05-11 05:07:20] Batch 10 - Loss: 9.988380
[2025-05-11 05:07:28] Batch 15 - Loss: 10.166488
[2025-05-11 05:07:35] Batch 20 - Loss: 10.000534
[2025-05-11 05:07:43] Batch 25 - Loss: 10.087366
[2025-05-11 05:07:51] Batch 30 - Loss: 9.995520
[2025-05-11 05:07:58] Batch 35 - Loss: 9.970097
[2025-05-11 05:08:05] === Epoch 88/100 Summary ===
Training Loss: 10.011526
Validation Loss: 11.597228
Learning Rate: 0.00001000
[2025-05-11 05:08:07] Batch 0 - Loss: 9.716487
[2025-05-11 05:08:14] Batch 5 - Loss: 10.253274
[2025-05-11 05:08:22] Batch 10 - Loss: 10.293736
[2025-05-11 05:08:30] Batch 15 - Loss: 10.394504
[2025-05-11 05:08:38] Batch 20 - Loss: 10.278399
[2025-05-11 05:08:45] Batch 25 - Loss: 10.264159
[2025-05-11 05:08:53] Batch 30 - Loss: 10.289888
[2025-05-11 05:09:01] Batch 35 - Loss: 10.306470
[2025-05-11 05:09:09] Batch 40 - Loss: 10.314304
[2025-05-11 05:09:17] Batch 45 - Loss: 10.357148
[2025-05-11 05:09:24] Batch 50 - Loss: 10.362015
[2025-05-11 05:09:32] Batch 55 - Loss: 10.392252
[2025-05-11 05:09:40] Batch 60 - Loss: 10.371009
[2025-05-11 05:09:48] Batch 65 - Loss: 10.415724
[2025-05-11 05:09:55] Batch 70 - Loss: 10.408707
[2025-05-11 05:10:03] Batch 75 - Loss: 10.368911
[2025-05-11 05:10:11] Batch 80 - Loss: 10.374282
[2025-05-11 05:10:19] Batch 85 - Loss: 10.395884
[2025-05-11 05:10:26] Batch 90 - Loss: 10.340107
[2025-05-11 05:10:34] Batch 95 - Loss: 10.301702
[2025-05-11 05:10:46] === Epoch 89/100 Summary ===
Training Loss: 10.303189
Validation Loss: 11.578838
Learning Rate: 0.00001000
[2025-05-11 05:10:47] Batch 0 - Loss: 9.035833
[2025-05-11 05:10:55] Batch 5 - Loss: 10.281119
[2025-05-11 05:11:03] Batch 10 - Loss: 10.626384
[2025-05-11 05:11:11] Batch 15 - Loss: 10.620650
[2025-05-11 05:11:18] Batch 20 - Loss: 10.584819
[2025-05-11 05:11:26] Batch 25 - Loss: 10.711806
[2025-05-11 05:11:34] Batch 30 - Loss: 10.505567
[2025-05-11 05:11:42] Batch 35 - Loss: 10.418777
[2025-05-11 05:11:49] Batch 40 - Loss: 10.402203
[2025-05-11 05:11:57] Batch 45 - Loss: 10.553029
[2025-05-11 05:12:05] Batch 50 - Loss: 10.601425
[2025-05-11 05:12:13] Batch 55 - Loss: 10.643243
[2025-05-11 05:12:20] Batch 60 - Loss: 10.665848
[2025-05-11 05:12:28] Batch 65 - Loss: 10.657840
[2025-05-11 05:12:36] Batch 70 - Loss: 10.715816
[2025-05-11 05:12:44] Batch 75 - Loss: 10.684208
[2025-05-11 05:12:51] Batch 80 - Loss: 10.672295
[2025-05-11 05:12:59] Batch 85 - Loss: 10.673808
[2025-05-11 05:13:07] Batch 90 - Loss: 10.603311
[2025-05-11 05:13:15] Batch 95 - Loss: 10.669854
[2025-05-11 05:13:26] === Epoch 90/100 Summary ===
Training Loss: 10.676998
Validation Loss: 11.492731
Learning Rate: 0.00001000
[2025-05-11 05:13:26] Generating detection visualizations for epoch 90...
[2025-05-11 05:13:27] Saved visualization to trained_models/custom_detector\visualizations\epoch_90\sample_1.png
[2025-05-11 05:13:27] Saved visualization to trained_models/custom_detector\visualizations\epoch_90\sample_2.png
[2025-05-11 05:13:27] Saved visualization to trained_models/custom_detector\visualizations\epoch_90\sample_3.png
[2025-05-11 05:13:29] Batch 0 - Loss: 10.092944
[2025-05-11 05:13:37] Batch 5 - Loss: 10.419163
[2025-05-11 05:13:44] Batch 10 - Loss: 11.151339
[2025-05-11 05:13:52] Batch 15 - Loss: 10.905178
[2025-05-11 05:14:00] Batch 20 - Loss: 10.800917
[2025-05-11 05:14:07] Batch 25 - Loss: 10.596493
[2025-05-11 05:14:15] Batch 30 - Loss: 10.650621
[2025-05-11 05:14:23] Batch 35 - Loss: 10.594057
[2025-05-11 05:14:31] Batch 40 - Loss: 10.413187
[2025-05-11 05:14:38] Batch 45 - Loss: 10.410100
[2025-05-11 05:14:46] Batch 50 - Loss: 10.411607
[2025-05-11 05:14:54] Batch 55 - Loss: 10.387678
[2025-05-11 05:15:02] Batch 60 - Loss: 10.284527
[2025-05-11 05:15:09] Batch 65 - Loss: 10.230466
[2025-05-11 05:15:17] Batch 70 - Loss: 10.240680
[2025-05-11 05:15:25] Batch 75 - Loss: 10.229920
[2025-05-11 05:15:33] Batch 80 - Loss: 10.167023
[2025-05-11 05:15:41] Batch 85 - Loss: 10.126756
[2025-05-11 05:15:48] Batch 90 - Loss: 10.063584
[2025-05-11 05:15:56] Batch 95 - Loss: 9.999769
[2025-05-11 05:16:08] === Epoch 91/100 Summary ===
Training Loss: 10.008195
Validation Loss: 11.510802
Learning Rate: 0.00001000
[2025-05-11 05:16:09] Batch 0 - Loss: 10.063855
[2025-05-11 05:16:17] Batch 5 - Loss: 9.335170
[2025-05-11 05:16:25] Batch 10 - Loss: 9.244149
[2025-05-11 05:16:32] Batch 15 - Loss: 9.265979
[2025-05-11 05:16:40] Batch 20 - Loss: 9.230428
[2025-05-11 05:16:48] Batch 25 - Loss: 9.345786
[2025-05-11 05:16:56] Batch 30 - Loss: 9.387500
[2025-05-11 05:17:03] Batch 35 - Loss: 9.399584
[2025-05-11 05:17:10] === Epoch 92/100 Summary ===
Training Loss: 9.412597
Validation Loss: 11.454114
Learning Rate: 0.00001000
[2025-05-11 05:17:12] Batch 0 - Loss: 10.729588
[2025-05-11 05:17:19] Batch 5 - Loss: 11.355640
[2025-05-11 05:17:27] Batch 10 - Loss: 11.059430
[2025-05-11 05:17:35] Batch 15 - Loss: 10.879362
[2025-05-11 05:17:43] Batch 20 - Loss: 10.508672
[2025-05-11 05:17:50] Batch 25 - Loss: 10.449869
[2025-05-11 05:17:58] Batch 30 - Loss: 10.352837
[2025-05-11 05:18:06] Batch 35 - Loss: 10.276999
[2025-05-11 05:18:14] Batch 40 - Loss: 10.314948
[2025-05-11 05:18:21] Batch 45 - Loss: 10.408649
[2025-05-11 05:18:29] Batch 50 - Loss: 10.493019
[2025-05-11 05:18:37] Batch 55 - Loss: 10.550225
[2025-05-11 05:18:45] Batch 60 - Loss: 10.536600
[2025-05-11 05:18:52] Batch 65 - Loss: 10.466257
[2025-05-11 05:19:00] Batch 70 - Loss: 10.420177
[2025-05-11 05:19:08] Batch 75 - Loss: 10.378271
[2025-05-11 05:19:16] Batch 80 - Loss: 10.323582
[2025-05-11 05:19:23] Batch 85 - Loss: 10.260218
[2025-05-11 05:19:31] Batch 90 - Loss: 10.272258
[2025-05-11 05:19:39] Batch 95 - Loss: 10.250306
[2025-05-11 05:19:50] === Epoch 93/100 Summary ===
Training Loss: 10.240685
Validation Loss: 11.443118
Learning Rate: 0.00001000
[2025-05-11 05:19:52] Batch 0 - Loss: 8.863268
[2025-05-11 05:20:00] Batch 5 - Loss: 10.333907
[2025-05-11 05:20:08] Batch 10 - Loss: 10.077157
[2025-05-11 05:20:15] Batch 15 - Loss: 9.993652
[2025-05-11 05:20:23] Batch 20 - Loss: 10.019691
[2025-05-11 05:20:31] Batch 25 - Loss: 9.969070
[2025-05-11 05:20:39] Batch 30 - Loss: 9.832845
[2025-05-11 05:20:46] Batch 35 - Loss: 9.928083
[2025-05-11 05:20:54] Batch 40 - Loss: 10.003494
[2025-05-11 05:21:02] Batch 45 - Loss: 10.292245
[2025-05-11 05:21:10] Batch 50 - Loss: 10.428233
[2025-05-11 05:21:17] Batch 55 - Loss: 10.442214
[2025-05-11 05:21:25] Batch 60 - Loss: 10.334542
[2025-05-11 05:21:33] Batch 65 - Loss: 10.313857
[2025-05-11 05:21:41] Batch 70 - Loss: 10.299933
[2025-05-11 05:21:48] Batch 75 - Loss: 10.313491
[2025-05-11 05:21:56] Batch 80 - Loss: 10.260147
[2025-05-11 05:22:04] Batch 85 - Loss: 10.308828
[2025-05-11 05:22:12] Batch 90 - Loss: 10.279549
[2025-05-11 05:22:19] Batch 95 - Loss: 10.263867
[2025-05-11 05:22:31] === Epoch 94/100 Summary ===
Training Loss: 10.251830
Validation Loss: 11.426226
Learning Rate: 0.00001000
[2025-05-11 05:22:33] Batch 0 - Loss: 10.985539
[2025-05-11 05:22:40] Batch 5 - Loss: 10.286304
[2025-05-11 05:22:48] Batch 10 - Loss: 9.879068
[2025-05-11 05:22:56] Batch 15 - Loss: 10.052873
[2025-05-11 05:23:04] Batch 20 - Loss: 10.091993
[2025-05-11 05:23:11] Batch 25 - Loss: 10.087708
[2025-05-11 05:23:19] Batch 30 - Loss: 10.033979
[2025-05-11 05:23:27] Batch 35 - Loss: 10.020601
[2025-05-11 05:23:35] Batch 40 - Loss: 10.063314
[2025-05-11 05:23:42] Batch 45 - Loss: 10.016892
[2025-05-11 05:23:50] Batch 50 - Loss: 10.027162
[2025-05-11 05:23:58] Batch 55 - Loss: 10.071091
[2025-05-11 05:24:06] Batch 60 - Loss: 10.050232
[2025-05-11 05:24:13] Batch 65 - Loss: 10.028607
[2025-05-11 05:24:21] Batch 70 - Loss: 9.994241
[2025-05-11 05:24:29] Batch 75 - Loss: 9.978062
[2025-05-11 05:24:37] Batch 80 - Loss: 9.932801
[2025-05-11 05:24:44] Batch 85 - Loss: 9.879330
[2025-05-11 05:24:52] Batch 90 - Loss: 9.878435
[2025-05-11 05:25:00] Batch 95 - Loss: 9.856347
[2025-05-11 05:25:12] === Epoch 95/100 Summary ===
Training Loss: 9.842676
Validation Loss: 11.372524
Learning Rate: 0.00001000
[2025-05-11 05:25:12] Generating detection visualizations for epoch 95...
[2025-05-11 05:25:12] Saved visualization to trained_models/custom_detector\visualizations\epoch_95\sample_1.png
[2025-05-11 05:25:12] Saved visualization to trained_models/custom_detector\visualizations\epoch_95\sample_2.png
[2025-05-11 05:25:13] Saved visualization to trained_models/custom_detector\visualizations\epoch_95\sample_3.png
[2025-05-11 05:25:14] Batch 0 - Loss: 8.206625
[2025-05-11 05:25:22] Batch 5 - Loss: 10.900249
[2025-05-11 05:25:30] Batch 10 - Loss: 10.326559
[2025-05-11 05:25:37] Batch 15 - Loss: 9.860729
[2025-05-11 05:25:45] Batch 20 - Loss: 9.898252
[2025-05-11 05:25:53] Batch 25 - Loss: 9.889481
[2025-05-11 05:26:00] Batch 30 - Loss: 9.705421
[2025-05-11 05:26:08] Batch 35 - Loss: 9.606705
[2025-05-11 05:26:14] === Epoch 96/100 Summary ===
Training Loss: 9.622086
Validation Loss: 11.382120
Learning Rate: 0.00001000
[2025-05-11 05:26:16] Batch 0 - Loss: 10.532847
[2025-05-11 05:26:24] Batch 5 - Loss: 10.615849
[2025-05-11 05:26:32] Batch 10 - Loss: 10.463494
[2025-05-11 05:26:39] Batch 15 - Loss: 10.238967
[2025-05-11 05:26:47] Batch 20 - Loss: 10.106697
[2025-05-11 05:26:55] Batch 25 - Loss: 10.361648
[2025-05-11 05:27:03] Batch 30 - Loss: 10.465923
[2025-05-11 05:27:10] Batch 35 - Loss: 10.440766
[2025-05-11 05:27:18] Batch 40 - Loss: 10.400798
[2025-05-11 05:27:26] Batch 45 - Loss: 10.352098
[2025-05-11 05:27:34] Batch 50 - Loss: 10.378568
[2025-05-11 05:27:41] Batch 55 - Loss: 10.429380
[2025-05-11 05:27:49] Batch 60 - Loss: 10.360151
[2025-05-11 05:27:57] Batch 65 - Loss: 10.286057
[2025-05-11 05:28:04] Batch 70 - Loss: 10.227395
[2025-05-11 05:28:12] Batch 75 - Loss: 10.169353
[2025-05-11 05:28:20] Batch 80 - Loss: 10.153875
[2025-05-11 05:28:28] Batch 85 - Loss: 10.080784
[2025-05-11 05:28:35] Batch 90 - Loss: 9.999617
[2025-05-11 05:28:43] Batch 95 - Loss: 9.956658
[2025-05-11 05:28:54] === Epoch 97/100 Summary ===
Training Loss: 9.968700
Validation Loss: 11.388308
Learning Rate: 0.00001000
[2025-05-11 05:28:56] Batch 0 - Loss: 9.658660
[2025-05-11 05:29:04] Batch 5 - Loss: 9.845134
[2025-05-11 05:29:11] Batch 10 - Loss: 10.244430
[2025-05-11 05:29:19] Batch 15 - Loss: 10.181924
[2025-05-11 05:29:27] Batch 20 - Loss: 10.095573
[2025-05-11 05:29:35] Batch 25 - Loss: 9.959133
[2025-05-11 05:29:42] Batch 30 - Loss: 9.927438
[2025-05-11 05:29:50] Batch 35 - Loss: 10.027671
[2025-05-11 05:29:58] Batch 40 - Loss: 10.094864
[2025-05-11 05:30:06] Batch 45 - Loss: 10.155630
[2025-05-11 05:30:13] Batch 50 - Loss: 10.211337
[2025-05-11 05:30:21] Batch 55 - Loss: 10.131358
[2025-05-11 05:30:29] Batch 60 - Loss: 10.151641
[2025-05-11 05:30:36] Batch 65 - Loss: 10.100260
[2025-05-11 05:30:44] Batch 70 - Loss: 10.072564
[2025-05-11 05:30:52] Batch 75 - Loss: 10.085463
[2025-05-11 05:31:00] Batch 80 - Loss: 10.244171
[2025-05-11 05:31:07] Batch 85 - Loss: 10.212570
[2025-05-11 05:31:15] Batch 90 - Loss: 10.194950
[2025-05-11 05:31:23] Batch 95 - Loss: 10.160823
[2025-05-11 05:31:34] === Epoch 98/100 Summary ===
Training Loss: 10.168341
Validation Loss: 11.421968
Learning Rate: 0.00001000
[2025-05-11 05:31:36] Batch 0 - Loss: 9.464846
[2025-05-11 05:31:43] Batch 5 - Loss: 10.526563
[2025-05-11 05:31:51] Batch 10 - Loss: 10.207237
[2025-05-11 05:31:59] Batch 15 - Loss: 10.071318
[2025-05-11 05:32:07] Batch 20 - Loss: 10.123179
[2025-05-11 05:32:14] Batch 25 - Loss: 10.194488
[2025-05-11 05:32:22] Batch 30 - Loss: 10.233029
[2025-05-11 05:32:30] Batch 35 - Loss: 10.262888
[2025-05-11 05:32:37] Batch 40 - Loss: 10.297973
[2025-05-11 05:32:45] Batch 45 - Loss: 10.220316
[2025-05-11 05:32:53] Batch 50 - Loss: 10.253859
[2025-05-11 05:33:01] Batch 55 - Loss: 10.089201
[2025-05-11 05:33:08] Batch 60 - Loss: 10.073687
[2025-05-11 05:33:16] Batch 65 - Loss: 10.037294
[2025-05-11 05:33:24] Batch 70 - Loss: 9.993068
[2025-05-11 05:33:32] Batch 75 - Loss: 10.023315
[2025-05-11 05:33:39] Batch 80 - Loss: 9.985265
[2025-05-11 05:33:47] Batch 85 - Loss: 9.975597
[2025-05-11 05:33:55] Batch 90 - Loss: 9.897521
[2025-05-11 05:34:03] Batch 95 - Loss: 9.842454
[2025-05-11 05:34:14] === Epoch 99/100 Summary ===
Training Loss: 9.833963
Validation Loss: 11.353695
Learning Rate: 0.00001000
[2025-05-11 05:34:16] Batch 0 - Loss: 9.585871
[2025-05-11 05:34:23] Batch 5 - Loss: 9.445193
[2025-05-11 05:34:31] Batch 10 - Loss: 9.666862
[2025-05-11 05:34:39] Batch 15 - Loss: 9.428256
[2025-05-11 05:34:47] Batch 20 - Loss: 9.328335
[2025-05-11 05:34:54] Batch 25 - Loss: 9.302970
[2025-05-11 05:35:02] Batch 30 - Loss: 9.305188
[2025-05-11 05:35:10] Batch 35 - Loss: 9.359989
[2025-05-11 05:35:16] === Epoch 100/100 Summary ===
Training Loss: 9.328536
Validation Loss: 11.335880
Learning Rate: 0.00001000
[2025-05-11 05:35:16] Generating detection visualizations for epoch 100...
[2025-05-11 05:35:16] Saved visualization to trained_models/custom_detector\visualizations\epoch_100\sample_1.png
[2025-05-11 05:35:17] Saved visualization to trained_models/custom_detector\visualizations\epoch_100\sample_2.png
[2025-05-11 05:35:17] Saved visualization to trained_models/custom_detector\visualizations\epoch_100\sample_3.png
[2025-05-11 05:35:17] === Training Completed ===
[2025-05-11 05:35:17] Best epoch: 100
[2025-05-11 05:35:17] Best validation loss: 11.335880
[2025-05-11 05:35:17] Model training completed in 225.93 minutes
[2025-05-11 05:35:17] Model saved to trained_models/custom_detector\final_model.keras
[2025-05-11 05:35:17] Training history plot saved to trained_models/custom_detector\training_history.png
[2025-05-11 05:35:17] === Valorant Detector Completed ===
