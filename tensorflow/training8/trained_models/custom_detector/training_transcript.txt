[2025-05-11 13:50:44] Starting training session
Model configuration: IMAGE_SIZE=(416, 416), BATCH_SIZE=16, EPOCHS=100, LEARNING_RATE=0.0001

[2025-05-11 13:50:44] === Starting Valorany Detector ===
[2025-05-11 13:50:44] Training data: ../tf_files/train.record
[2025-05-11 13:50:44] Validation data: ../tf_files/valid.record
[2025-05-11 13:50:44] Loading data from ../tf_files/train.record and ../tf_files/valid.record
[2025-05-11 13:50:45] Estimating dataset sizes...
[2025-05-11 13:50:45] Train dataset size: ~337 batches (approx. 5385 examples)
[2025-05-11 13:50:45] Validation dataset size: ~33 batches (approx. 514 examples)
[2025-05-11 13:50:45] Creating model...
[2025-05-11 13:50:45] Starting model training...
[2025-05-11 13:50:46] === Training Started ===
[2025-05-11 13:50:46] Model structure:
[2025-05-11 13:50:46] Total layers: 160
[2025-05-11 13:50:46] Total parameters: 9,344,081
[2025-05-11 13:50:46] Trainable parameters: 7,084,561
[2025-05-11 13:50:46] Non-trainable parameters: 2,259,520
[2025-05-11 13:50:50] Batch 0 - Loss: 514.630066
[2025-05-11 13:50:52] Batch 5 - Loss: 444.246094
[2025-05-11 13:50:53] Batch 10 - Loss: 420.271667
[2025-05-11 13:50:55] Batch 15 - Loss: 407.922882
[2025-05-11 13:50:57] Batch 20 - Loss: 385.198242
[2025-05-11 13:50:59] Batch 25 - Loss: 367.336365
[2025-05-11 13:51:00] Batch 30 - Loss: 350.765717
[2025-05-11 13:51:02] Batch 35 - Loss: 335.635620
[2025-05-11 13:51:04] Batch 40 - Loss: 322.185944
[2025-05-11 13:51:06] Batch 45 - Loss: 309.661713
[2025-05-11 13:51:07] Batch 50 - Loss: 298.063751
[2025-05-11 13:51:09] Batch 55 - Loss: 290.464722
[2025-05-11 13:51:11] Batch 60 - Loss: 280.829437
[2025-05-11 13:51:13] Batch 65 - Loss: 271.445312
[2025-05-11 13:51:14] Batch 70 - Loss: 262.356445
[2025-05-11 13:51:16] Batch 75 - Loss: 253.418274
[2025-05-11 13:51:18] Batch 80 - Loss: 244.849899
[2025-05-11 13:51:20] Batch 85 - Loss: 237.218521
[2025-05-11 13:51:21] Batch 90 - Loss: 230.893250
[2025-05-11 13:51:23] Batch 95 - Loss: 223.434464
[2025-05-11 13:51:31] === Epoch 1/100 Summary ===
Training Loss: 219.599609
Validation Loss: 253.271774
Learning Rate: 0.00003000
[2025-05-11 13:51:32] Batch 0 - Loss: 70.774681
[2025-05-11 13:51:34] Batch 5 - Loss: 70.511528
[2025-05-11 13:51:35] Batch 10 - Loss: 67.300812
[2025-05-11 13:51:37] Batch 15 - Loss: 64.187294
[2025-05-11 13:51:39] Batch 20 - Loss: 61.104103
[2025-05-11 13:51:41] Batch 25 - Loss: 58.435894
[2025-05-11 13:51:42] Batch 30 - Loss: 56.516933
[2025-05-11 13:51:44] Batch 35 - Loss: 54.181366
[2025-05-11 13:51:46] Batch 40 - Loss: 53.760231
[2025-05-11 13:51:48] Batch 45 - Loss: 51.851700
[2025-05-11 13:51:49] Batch 50 - Loss: 50.099697
[2025-05-11 13:51:51] Batch 55 - Loss: 48.439972
[2025-05-11 13:51:53] Batch 60 - Loss: 46.979710
[2025-05-11 13:51:55] Batch 65 - Loss: 45.585461
[2025-05-11 13:51:57] Batch 70 - Loss: 44.376682
[2025-05-11 13:51:58] Batch 75 - Loss: 43.095573
[2025-05-11 13:52:00] Batch 80 - Loss: 41.895126
[2025-05-11 13:52:02] Batch 85 - Loss: 40.843567
[2025-05-11 13:52:04] Batch 90 - Loss: 39.825825
[2025-05-11 13:52:05] Batch 95 - Loss: 38.811848
[2025-05-11 13:52:13] === Epoch 2/100 Summary ===
Training Loss: 38.136047
Validation Loss: 59.531181
Learning Rate: 0.00003000
[2025-05-11 13:52:13] Batch 0 - Loss: 23.335756
[2025-05-11 13:52:15] Batch 5 - Loss: 20.577955
[2025-05-11 13:52:17] Batch 10 - Loss: 19.828957
[2025-05-11 13:52:18] Batch 15 - Loss: 19.775167
[2025-05-11 13:52:20] Batch 20 - Loss: 19.509083
[2025-05-11 13:52:22] Batch 25 - Loss: 19.493366
[2025-05-11 13:52:24] Batch 30 - Loss: 19.368864
[2025-05-11 13:52:26] Batch 35 - Loss: 19.111494
[2025-05-11 13:52:27] Batch 40 - Loss: 18.969994
[2025-05-11 13:52:29] Batch 45 - Loss: 18.702272
[2025-05-11 13:52:31] Batch 50 - Loss: 18.592464
[2025-05-11 13:52:33] Batch 55 - Loss: 18.430151
[2025-05-11 13:52:34] Batch 60 - Loss: 18.204817
[2025-05-11 13:52:36] Batch 65 - Loss: 17.963953
[2025-05-11 13:52:38] Batch 70 - Loss: 17.882420
[2025-05-11 13:52:40] Batch 75 - Loss: 17.632284
[2025-05-11 13:52:41] Batch 80 - Loss: 17.487717
[2025-05-11 13:52:43] Batch 85 - Loss: 17.328445
[2025-05-11 13:52:45] Batch 90 - Loss: 17.206188
[2025-05-11 13:52:47] Batch 95 - Loss: 17.060444
[2025-05-11 13:52:54] === Epoch 3/100 Summary ===
Training Loss: 17.073706
Validation Loss: 24.617853
Learning Rate: 0.00003000
[2025-05-11 13:52:54] Batch 0 - Loss: 16.506456
[2025-05-11 13:52:56] Batch 5 - Loss: 13.710280
[2025-05-11 13:52:58] Batch 10 - Loss: 13.853503
[2025-05-11 13:52:59] Batch 15 - Loss: 13.802052
[2025-05-11 13:53:01] Batch 20 - Loss: 13.644661
[2025-05-11 13:53:03] Batch 25 - Loss: 13.511918
[2025-05-11 13:53:05] Batch 30 - Loss: 13.482781
[2025-05-11 13:53:06] Batch 35 - Loss: 13.593358
[2025-05-11 13:53:12] === Epoch 4/100 Summary ===
Training Loss: 13.589865
Validation Loss: 25.920050
Learning Rate: 0.00003000
[2025-05-11 13:53:13] Batch 0 - Loss: 14.713705
[2025-05-11 13:53:15] Batch 5 - Loss: 15.744605
[2025-05-11 13:53:17] Batch 10 - Loss: 15.329556
[2025-05-11 13:53:19] Batch 15 - Loss: 14.753582
[2025-05-11 13:53:20] Batch 20 - Loss: 14.622999
[2025-05-11 13:53:22] Batch 25 - Loss: 14.548121
[2025-05-11 13:53:24] Batch 30 - Loss: 14.787000
[2025-05-11 13:53:26] Batch 35 - Loss: 14.705843
[2025-05-11 13:53:27] Batch 40 - Loss: 14.625189
[2025-05-11 13:53:29] Batch 45 - Loss: 14.591452
[2025-05-11 13:53:31] Batch 50 - Loss: 14.584331
[2025-05-11 13:53:33] Batch 55 - Loss: 14.607793
[2025-05-11 13:53:35] Batch 60 - Loss: 14.459909
[2025-05-11 13:53:36] Batch 65 - Loss: 14.387144
[2025-05-11 13:53:38] Batch 70 - Loss: 14.400909
[2025-05-11 13:53:40] Batch 75 - Loss: 14.282938
[2025-05-11 13:53:42] Batch 80 - Loss: 14.180853
[2025-05-11 13:53:44] Batch 85 - Loss: 14.394635
[2025-05-11 13:53:45] Batch 90 - Loss: 14.452378
[2025-05-11 13:53:47] Batch 95 - Loss: 14.372294
[2025-05-11 13:53:54] === Epoch 5/100 Summary ===
Training Loss: 14.306594
Validation Loss: 21.529526
Learning Rate: 0.00003000
[2025-05-11 13:53:54] Generating detection visualizations for epoch 5...
[2025-05-11 13:53:55] Saved visualization to trained_models/custom_detector\visualizations\epoch_5\sample_1.png
[2025-05-11 13:53:56] Saved visualization to trained_models/custom_detector\visualizations\epoch_5\sample_2.png
[2025-05-11 13:53:56] Saved visualization to trained_models/custom_detector\visualizations\epoch_5\sample_3.png
[2025-05-11 13:53:56] Batch 0 - Loss: 12.536966
[2025-05-11 13:53:58] Batch 5 - Loss: 12.471293
[2025-05-11 13:54:00] Batch 10 - Loss: 12.229431
[2025-05-11 13:54:02] Batch 15 - Loss: 12.559893
[2025-05-11 13:54:03] Batch 20 - Loss: 12.202743
[2025-05-11 13:54:05] Batch 25 - Loss: 11.986415
[2025-05-11 13:54:07] Batch 30 - Loss: 11.952702
[2025-05-11 13:54:09] Batch 35 - Loss: 11.973699
[2025-05-11 13:54:10] Batch 40 - Loss: 12.105774
[2025-05-11 13:54:12] Batch 45 - Loss: 12.081076
[2025-05-11 13:54:14] Batch 50 - Loss: 12.356487
[2025-05-11 13:54:16] Batch 55 - Loss: 12.325814
[2025-05-11 13:54:18] Batch 60 - Loss: 12.344037
[2025-05-11 13:54:19] Batch 65 - Loss: 12.384050
[2025-05-11 13:54:21] Batch 70 - Loss: 12.365757
[2025-05-11 13:54:23] Batch 75 - Loss: 12.403021
[2025-05-11 13:54:25] Batch 80 - Loss: 12.396593
[2025-05-11 13:54:26] Batch 85 - Loss: 12.360252
[2025-05-11 13:54:28] Batch 90 - Loss: 12.299479
[2025-05-11 13:54:30] Batch 95 - Loss: 12.309257
[2025-05-11 13:54:37] === Epoch 6/100 Summary ===
Training Loss: 12.310221
Validation Loss: 14.828520
Learning Rate: 0.00003000
[2025-05-11 13:54:38] Batch 0 - Loss: 12.619403
[2025-05-11 13:54:39] Batch 5 - Loss: 12.473460
[2025-05-11 13:54:41] Batch 10 - Loss: 11.710738
[2025-05-11 13:54:43] Batch 15 - Loss: 12.099632
[2025-05-11 13:54:45] Batch 20 - Loss: 11.829638
[2025-05-11 13:54:46] Batch 25 - Loss: 11.833998
[2025-05-11 13:54:48] Batch 30 - Loss: 11.913197
[2025-05-11 13:54:50] Batch 35 - Loss: 12.033114
[2025-05-11 13:54:52] Batch 40 - Loss: 11.989290
[2025-05-11 13:54:53] Batch 45 - Loss: 11.895376
[2025-05-11 13:54:55] Batch 50 - Loss: 11.774466
[2025-05-11 13:54:57] Batch 55 - Loss: 11.801566
[2025-05-11 13:54:59] Batch 60 - Loss: 11.793239
[2025-05-11 13:55:00] Batch 65 - Loss: 11.721987
[2025-05-11 13:55:02] Batch 70 - Loss: 11.701169
[2025-05-11 13:55:04] Batch 75 - Loss: 11.703261
[2025-05-11 13:55:06] Batch 80 - Loss: 11.605570
[2025-05-11 13:55:07] Batch 85 - Loss: 11.569459
[2025-05-11 13:55:09] Batch 90 - Loss: 11.562064
[2025-05-11 13:55:11] Batch 95 - Loss: 11.494834
[2025-05-11 13:55:18] === Epoch 7/100 Summary ===
Training Loss: 11.551832
Validation Loss: 15.904648
Learning Rate: 0.00003000
[2025-05-11 13:55:18] Batch 0 - Loss: 12.971569
[2025-05-11 13:55:20] Batch 5 - Loss: 11.880882
[2025-05-11 13:55:22] Batch 10 - Loss: 11.542496
[2025-05-11 13:55:24] Batch 15 - Loss: 11.478596
[2025-05-11 13:55:25] Batch 20 - Loss: 11.394338
[2025-05-11 13:55:27] Batch 25 - Loss: 11.356456
[2025-05-11 13:55:29] Batch 30 - Loss: 11.379752
[2025-05-11 13:55:31] Batch 35 - Loss: 11.356447
[2025-05-11 13:55:37] === Epoch 8/100 Summary ===
Training Loss: 11.332391
Validation Loss: 14.749255
Learning Rate: 0.00003000
[2025-05-11 13:55:38] Batch 0 - Loss: 11.250989
[2025-05-11 13:55:39] Batch 5 - Loss: 12.373664
[2025-05-11 13:55:41] Batch 10 - Loss: 12.378107
[2025-05-11 13:55:43] Batch 15 - Loss: 12.106855
[2025-05-11 13:55:45] Batch 20 - Loss: 11.810431
[2025-05-11 13:55:46] Batch 25 - Loss: 11.806927
[2025-05-11 13:55:48] Batch 30 - Loss: 11.793143
[2025-05-11 13:55:50] Batch 35 - Loss: 11.887272
[2025-05-11 13:55:52] Batch 40 - Loss: 12.041489
[2025-05-11 13:55:53] Batch 45 - Loss: 11.973911
[2025-05-11 13:55:55] Batch 50 - Loss: 12.015427
[2025-05-11 13:55:57] Batch 55 - Loss: 12.253701
[2025-05-11 13:55:59] Batch 60 - Loss: 12.322870
[2025-05-11 13:56:01] Batch 65 - Loss: 12.241087
[2025-05-11 13:56:02] Batch 70 - Loss: 12.183398
[2025-05-11 13:56:04] Batch 75 - Loss: 12.121604
[2025-05-11 13:56:06] Batch 80 - Loss: 12.021621
[2025-05-11 13:56:08] Batch 85 - Loss: 12.041944
[2025-05-11 13:56:09] Batch 90 - Loss: 11.985549
[2025-05-11 13:56:11] Batch 95 - Loss: 11.901092
[2025-05-11 13:56:18] === Epoch 9/100 Summary ===
Training Loss: 11.915874
Validation Loss: 13.568962
Learning Rate: 0.00003000
[2025-05-11 13:56:19] Batch 0 - Loss: 11.427225
[2025-05-11 13:56:21] Batch 5 - Loss: 11.523403
[2025-05-11 13:56:22] Batch 10 - Loss: 12.241583
[2025-05-11 13:56:24] Batch 15 - Loss: 11.876435
[2025-05-11 13:56:26] Batch 20 - Loss: 12.046168
[2025-05-11 13:56:28] Batch 25 - Loss: 11.817489
[2025-05-11 13:56:30] Batch 30 - Loss: 11.762759
[2025-05-11 13:56:31] Batch 35 - Loss: 11.631256
[2025-05-11 13:56:33] Batch 40 - Loss: 11.651584
[2025-05-11 13:56:35] Batch 45 - Loss: 11.603425
[2025-05-11 13:56:37] Batch 50 - Loss: 11.720861
[2025-05-11 13:56:38] Batch 55 - Loss: 11.765563
[2025-05-11 13:56:40] Batch 60 - Loss: 11.678737
[2025-05-11 13:56:42] Batch 65 - Loss: 11.665283
[2025-05-11 13:56:44] Batch 70 - Loss: 11.617614
[2025-05-11 13:56:45] Batch 75 - Loss: 11.663378
[2025-05-11 13:56:47] Batch 80 - Loss: 11.637753
[2025-05-11 13:56:49] Batch 85 - Loss: 11.616357
[2025-05-11 13:56:51] Batch 90 - Loss: 11.526158
[2025-05-11 13:56:53] Batch 95 - Loss: 11.498371
[2025-05-11 13:57:00] === Epoch 10/100 Summary ===
Training Loss: 11.474730
Validation Loss: 13.254492
Learning Rate: 0.00003000
[2025-05-11 13:57:00] Generating detection visualizations for epoch 10...
[2025-05-11 13:57:00] Saved visualization to trained_models/custom_detector\visualizations\epoch_10\sample_1.png
[2025-05-11 13:57:01] Saved visualization to trained_models/custom_detector\visualizations\epoch_10\sample_2.png
[2025-05-11 13:57:01] Saved visualization to trained_models/custom_detector\visualizations\epoch_10\sample_3.png
[2025-05-11 13:57:01] Batch 0 - Loss: 9.361231
[2025-05-11 13:57:03] Batch 5 - Loss: 10.744171
[2025-05-11 13:57:05] Batch 10 - Loss: 11.522190
[2025-05-11 13:57:07] Batch 15 - Loss: 11.166192
[2025-05-11 13:57:08] Batch 20 - Loss: 11.356951
[2025-05-11 13:57:10] Batch 25 - Loss: 11.190842
[2025-05-11 13:57:12] Batch 30 - Loss: 12.155634
[2025-05-11 13:57:14] Batch 35 - Loss: 11.955449
[2025-05-11 13:57:16] Batch 40 - Loss: 11.771692
[2025-05-11 13:57:17] Batch 45 - Loss: 11.781303
[2025-05-11 13:57:19] Batch 50 - Loss: 11.657625
[2025-05-11 13:57:21] Batch 55 - Loss: 11.706090
[2025-05-11 13:57:23] Batch 60 - Loss: 11.588364
[2025-05-11 13:57:24] Batch 65 - Loss: 11.524100
[2025-05-11 13:57:26] Batch 70 - Loss: 11.460078
[2025-05-11 13:57:28] Batch 75 - Loss: 11.382441
[2025-05-11 13:57:30] Batch 80 - Loss: 11.407059
[2025-05-11 13:57:31] Batch 85 - Loss: 11.382267
[2025-05-11 13:57:33] Batch 90 - Loss: 11.364884
[2025-05-11 13:57:35] Batch 95 - Loss: 11.311816
[2025-05-11 13:57:42] === Epoch 11/100 Summary ===
Training Loss: 11.277721
Validation Loss: 12.654790
Learning Rate: 0.00003000
[2025-05-11 13:57:42] Batch 0 - Loss: 10.268022
[2025-05-11 13:57:44] Batch 5 - Loss: 10.103412
[2025-05-11 13:57:46] Batch 10 - Loss: 10.593951
[2025-05-11 13:57:48] Batch 15 - Loss: 10.768246
[2025-05-11 13:57:49] Batch 20 - Loss: 10.776518
[2025-05-11 13:57:51] Batch 25 - Loss: 10.770595
[2025-05-11 13:57:53] Batch 30 - Loss: 10.730258
[2025-05-11 13:57:55] Batch 35 - Loss: 10.584852
[2025-05-11 13:58:01] === Epoch 12/100 Summary ===
Training Loss: 10.559301
Validation Loss: 12.897074
Learning Rate: 0.00003000
[2025-05-11 13:58:01] Batch 0 - Loss: 10.817409
[2025-05-11 13:58:03] Batch 5 - Loss: 14.982884
[2025-05-11 13:58:05] Batch 10 - Loss: 14.051589
[2025-05-11 13:58:07] Batch 15 - Loss: 13.157760
[2025-05-11 13:58:09] Batch 20 - Loss: 12.881920
[2025-05-11 13:58:11] Batch 25 - Loss: 12.559520
[2025-05-11 13:58:13] Batch 30 - Loss: 12.368939
[2025-05-11 13:58:15] Batch 35 - Loss: 12.199566
[2025-05-11 13:58:16] Batch 40 - Loss: 12.024692
[2025-05-11 13:58:18] Batch 45 - Loss: 11.972412
[2025-05-11 13:58:20] Batch 50 - Loss: 11.935181
[2025-05-11 13:58:22] Batch 55 - Loss: 11.911673
[2025-05-11 13:58:24] Batch 60 - Loss: 11.923701
[2025-05-11 13:58:26] Batch 65 - Loss: 11.829145
[2025-05-11 13:58:27] Batch 70 - Loss: 11.752715
[2025-05-11 13:58:29] Batch 75 - Loss: 11.674128
[2025-05-11 13:58:31] Batch 80 - Loss: 11.635032
[2025-05-11 13:58:33] Batch 85 - Loss: 11.601555
[2025-05-11 13:58:35] Batch 90 - Loss: 11.556493
[2025-05-11 13:58:37] Batch 95 - Loss: 11.504220
[2025-05-11 13:58:45] === Epoch 13/100 Summary ===
Training Loss: 11.490702
Validation Loss: 12.729248
Learning Rate: 0.00003000
[2025-05-11 13:58:45] Batch 0 - Loss: 10.171545
[2025-05-11 13:58:47] Batch 5 - Loss: 11.169579
[2025-05-11 13:58:48] Batch 10 - Loss: 10.700982
[2025-05-11 13:58:50] Batch 15 - Loss: 10.754947
[2025-05-11 13:58:52] Batch 20 - Loss: 10.797930
[2025-05-11 13:58:54] Batch 25 - Loss: 10.714501
[2025-05-11 13:58:55] Batch 30 - Loss: 10.631207
[2025-05-11 13:58:57] Batch 35 - Loss: 10.666005
[2025-05-11 13:58:59] Batch 40 - Loss: 10.666484
[2025-05-11 13:59:01] Batch 45 - Loss: 10.791281
[2025-05-11 13:59:03] Batch 50 - Loss: 10.938004
[2025-05-11 13:59:04] Batch 55 - Loss: 10.945429
[2025-05-11 13:59:06] Batch 60 - Loss: 10.952923
[2025-05-11 13:59:08] Batch 65 - Loss: 10.838708
[2025-05-11 13:59:10] Batch 70 - Loss: 10.886909
[2025-05-11 13:59:12] Batch 75 - Loss: 10.923249
[2025-05-11 13:59:14] Batch 80 - Loss: 10.868265
[2025-05-11 13:59:15] Batch 85 - Loss: 10.914128
[2025-05-11 13:59:17] Batch 90 - Loss: 10.909509
[2025-05-11 13:59:19] Batch 95 - Loss: 10.959239
[2025-05-11 13:59:27] === Epoch 14/100 Summary ===
Training Loss: 10.931245
Validation Loss: 12.488500
Learning Rate: 0.00003000
[2025-05-11 13:59:27] Batch 0 - Loss: 11.660579
[2025-05-11 13:59:29] Batch 5 - Loss: 10.957385
[2025-05-11 13:59:31] Batch 10 - Loss: 11.018975
[2025-05-11 13:59:33] Batch 15 - Loss: 11.289223
[2025-05-11 13:59:35] Batch 20 - Loss: 11.310752
[2025-05-11 13:59:37] Batch 25 - Loss: 11.315938
[2025-05-11 13:59:39] Batch 30 - Loss: 11.215120
[2025-05-11 13:59:41] Batch 35 - Loss: 11.307278
[2025-05-11 13:59:42] Batch 40 - Loss: 11.278969
[2025-05-11 13:59:44] Batch 45 - Loss: 11.146152
[2025-05-11 13:59:46] Batch 50 - Loss: 11.056447
[2025-05-11 13:59:48] Batch 55 - Loss: 11.020849
[2025-05-11 13:59:50] Batch 60 - Loss: 11.000014
[2025-05-11 13:59:51] Batch 65 - Loss: 10.982081
[2025-05-11 13:59:53] Batch 70 - Loss: 10.939844
[2025-05-11 13:59:55] Batch 75 - Loss: 10.905533
[2025-05-11 13:59:57] Batch 80 - Loss: 10.929561
[2025-05-11 13:59:59] Batch 85 - Loss: 10.902705
[2025-05-11 14:00:01] Batch 90 - Loss: 10.885543
[2025-05-11 14:00:02] Batch 95 - Loss: 10.861531
[2025-05-11 14:00:10] === Epoch 15/100 Summary ===
Training Loss: 10.859093
Validation Loss: 12.464302
Learning Rate: 0.00003000
[2025-05-11 14:00:10] Generating detection visualizations for epoch 15...
[2025-05-11 14:00:10] Saved visualization to trained_models/custom_detector\visualizations\epoch_15\sample_1.png
[2025-05-11 14:00:11] Saved visualization to trained_models/custom_detector\visualizations\epoch_15\sample_2.png
[2025-05-11 14:00:11] Saved visualization to trained_models/custom_detector\visualizations\epoch_15\sample_3.png
[2025-05-11 14:00:11] Batch 0 - Loss: 10.389107
[2025-05-11 14:00:13] Batch 5 - Loss: 9.454591
[2025-05-11 14:00:15] Batch 10 - Loss: 10.013508
[2025-05-11 14:00:17] Batch 15 - Loss: 10.034209
[2025-05-11 14:00:19] Batch 20 - Loss: 9.846757
[2025-05-11 14:00:21] Batch 25 - Loss: 10.706838
[2025-05-11 14:00:22] Batch 30 - Loss: 10.721839
[2025-05-11 14:00:24] Batch 35 - Loss: 10.563709
[2025-05-11 14:00:31] === Epoch 16/100 Summary ===
Training Loss: 10.529935
Validation Loss: 12.293856
Learning Rate: 0.00003000
[2025-05-11 14:00:32] Batch 0 - Loss: 9.260464
[2025-05-11 14:00:33] Batch 5 - Loss: 11.263112
[2025-05-11 14:00:35] Batch 10 - Loss: 11.014779
[2025-05-11 14:00:37] Batch 15 - Loss: 10.737462
[2025-05-11 14:00:39] Batch 20 - Loss: 10.907283
[2025-05-11 14:00:41] Batch 25 - Loss: 11.110480
[2025-05-11 14:00:43] Batch 30 - Loss: 11.130010
[2025-05-11 14:00:44] Batch 35 - Loss: 11.184384
[2025-05-11 14:00:46] Batch 40 - Loss: 11.150424
[2025-05-11 14:00:48] Batch 45 - Loss: 11.113828
[2025-05-11 14:00:50] Batch 50 - Loss: 11.079826
[2025-05-11 14:00:52] Batch 55 - Loss: 11.121694
[2025-05-11 14:00:54] Batch 60 - Loss: 11.124577
[2025-05-11 14:00:56] Batch 65 - Loss: 11.097834
[2025-05-11 14:00:58] Batch 70 - Loss: 11.126961
[2025-05-11 14:00:59] Batch 75 - Loss: 11.103737
[2025-05-11 14:01:01] Batch 80 - Loss: 11.001409
[2025-05-11 14:01:03] Batch 85 - Loss: 10.982798
[2025-05-11 14:01:05] Batch 90 - Loss: 10.938182
[2025-05-11 14:01:07] Batch 95 - Loss: 10.907875
[2025-05-11 14:01:14] === Epoch 17/100 Summary ===
Training Loss: 10.942996
Validation Loss: 12.412845
Learning Rate: 0.00003000
[2025-05-11 14:01:14] Batch 0 - Loss: 10.878893
[2025-05-11 14:01:16] Batch 5 - Loss: 10.449525
[2025-05-11 14:01:18] Batch 10 - Loss: 10.735310
[2025-05-11 14:01:20] Batch 15 - Loss: 10.847593
[2025-05-11 14:01:22] Batch 20 - Loss: 10.799269
[2025-05-11 14:01:23] Batch 25 - Loss: 10.838049
[2025-05-11 14:01:25] Batch 30 - Loss: 10.751386
[2025-05-11 14:01:27] Batch 35 - Loss: 10.702624
[2025-05-11 14:01:29] Batch 40 - Loss: 10.746748
[2025-05-11 14:01:31] Batch 45 - Loss: 10.867614
[2025-05-11 14:01:32] Batch 50 - Loss: 10.795846
[2025-05-11 14:01:34] Batch 55 - Loss: 10.765521
[2025-05-11 14:01:36] Batch 60 - Loss: 10.722397
[2025-05-11 14:01:38] Batch 65 - Loss: 10.774988
[2025-05-11 14:01:39] Batch 70 - Loss: 10.758159
[2025-05-11 14:01:41] Batch 75 - Loss: 10.753936
[2025-05-11 14:01:43] Batch 80 - Loss: 10.730481
[2025-05-11 14:01:45] Batch 85 - Loss: 10.698144
[2025-05-11 14:01:47] Batch 90 - Loss: 10.746483
[2025-05-11 14:01:48] Batch 95 - Loss: 10.804486
[2025-05-11 14:01:56] === Epoch 18/100 Summary ===
Training Loss: 10.798009
Validation Loss: 12.222692
Learning Rate: 0.00003000
[2025-05-11 14:01:56] Batch 0 - Loss: 9.274220
[2025-05-11 14:01:58] Batch 5 - Loss: 9.956571
[2025-05-11 14:02:00] Batch 10 - Loss: 10.319486
[2025-05-11 14:02:02] Batch 15 - Loss: 10.707164
[2025-05-11 14:02:03] Batch 20 - Loss: 10.653191
[2025-05-11 14:02:05] Batch 25 - Loss: 10.799134
[2025-05-11 14:02:07] Batch 30 - Loss: 10.802959
[2025-05-11 14:02:09] Batch 35 - Loss: 10.709462
[2025-05-11 14:02:11] Batch 40 - Loss: 10.681829
[2025-05-11 14:02:12] Batch 45 - Loss: 10.677705
[2025-05-11 14:02:14] Batch 50 - Loss: 10.694374
[2025-05-11 14:02:16] Batch 55 - Loss: 10.623439
[2025-05-11 14:02:18] Batch 60 - Loss: 10.590288
[2025-05-11 14:02:19] Batch 65 - Loss: 10.576607
[2025-05-11 14:02:21] Batch 70 - Loss: 10.525062
[2025-05-11 14:02:23] Batch 75 - Loss: 10.471530
[2025-05-11 14:02:25] Batch 80 - Loss: 10.532358
[2025-05-11 14:02:26] Batch 85 - Loss: 10.497717
[2025-05-11 14:02:28] Batch 90 - Loss: 10.510564
[2025-05-11 14:02:30] Batch 95 - Loss: 10.428022
[2025-05-11 14:02:37] === Epoch 19/100 Summary ===
Training Loss: 10.454787
Validation Loss: 12.135632
Learning Rate: 0.00003000
[2025-05-11 14:02:38] Batch 0 - Loss: 12.669205
[2025-05-11 14:02:39] Batch 5 - Loss: 10.338560
[2025-05-11 14:02:41] Batch 10 - Loss: 10.094049
[2025-05-11 14:02:43] Batch 15 - Loss: 9.946501
[2025-05-11 14:02:45] Batch 20 - Loss: 10.098539
[2025-05-11 14:02:46] Batch 25 - Loss: 10.121929
[2025-05-11 14:02:48] Batch 30 - Loss: 10.202098
[2025-05-11 14:02:50] Batch 35 - Loss: 10.219742
[2025-05-11 14:02:56] === Epoch 20/100 Summary ===
Training Loss: 10.267318
Validation Loss: 12.111051
Learning Rate: 0.00003000
[2025-05-11 14:02:56] Generating detection visualizations for epoch 20...
[2025-05-11 14:02:56] Saved visualization to trained_models/custom_detector\visualizations\epoch_20\sample_1.png
[2025-05-11 14:02:57] Saved visualization to trained_models/custom_detector\visualizations\epoch_20\sample_2.png
[2025-05-11 14:02:57] Saved visualization to trained_models/custom_detector\visualizations\epoch_20\sample_3.png
[2025-05-11 14:02:58] Batch 0 - Loss: 10.919647
[2025-05-11 14:03:00] Batch 5 - Loss: 10.525854
[2025-05-11 14:03:01] Batch 10 - Loss: 11.001815
[2025-05-11 14:03:03] Batch 15 - Loss: 10.960982
[2025-05-11 14:03:05] Batch 20 - Loss: 10.957855
[2025-05-11 14:03:07] Batch 25 - Loss: 10.805710
[2025-05-11 14:03:08] Batch 30 - Loss: 10.771276
[2025-05-11 14:03:10] Batch 35 - Loss: 10.887845
[2025-05-11 14:03:12] Batch 40 - Loss: 10.844839
[2025-05-11 14:03:14] Batch 45 - Loss: 10.787879
[2025-05-11 14:03:15] Batch 50 - Loss: 10.862125
[2025-05-11 14:03:17] Batch 55 - Loss: 10.852700
[2025-05-11 14:03:19] Batch 60 - Loss: 10.809803
[2025-05-11 14:03:21] Batch 65 - Loss: 10.808123
[2025-05-11 14:03:23] Batch 70 - Loss: 10.755280
[2025-05-11 14:03:24] Batch 75 - Loss: 10.753658
[2025-05-11 14:03:26] Batch 80 - Loss: 10.736294
[2025-05-11 14:03:28] Batch 85 - Loss: 10.742616
[2025-05-11 14:03:30] Batch 90 - Loss: 10.714886
[2025-05-11 14:03:32] Batch 95 - Loss: 10.682019
[2025-05-11 14:03:39] === Epoch 21/100 Summary ===
Training Loss: 10.613018
Validation Loss: 12.095513
Learning Rate: 0.00003000
[2025-05-11 14:03:39] Batch 0 - Loss: 9.057753
[2025-05-11 14:03:41] Batch 5 - Loss: 10.729775
[2025-05-11 14:03:43] Batch 10 - Loss: 10.451995
[2025-05-11 14:03:45] Batch 15 - Loss: 10.487320
[2025-05-11 14:03:46] Batch 20 - Loss: 10.524031
[2025-05-11 14:03:48] Batch 25 - Loss: 10.489007
[2025-05-11 14:03:50] Batch 30 - Loss: 10.367208
[2025-05-11 14:03:52] Batch 35 - Loss: 10.428586
[2025-05-11 14:03:54] Batch 40 - Loss: 10.671579
[2025-05-11 14:03:55] Batch 45 - Loss: 10.695360
[2025-05-11 14:03:57] Batch 50 - Loss: 10.754220
[2025-05-11 14:03:59] Batch 55 - Loss: 10.764748
[2025-05-11 14:04:01] Batch 60 - Loss: 10.730923
[2025-05-11 14:04:02] Batch 65 - Loss: 10.708181
[2025-05-11 14:04:04] Batch 70 - Loss: 10.644536
[2025-05-11 14:04:06] Batch 75 - Loss: 10.674998
[2025-05-11 14:04:08] Batch 80 - Loss: 10.666100
[2025-05-11 14:04:09] Batch 85 - Loss: 10.650229
[2025-05-11 14:04:11] Batch 90 - Loss: 10.626084
[2025-05-11 14:04:13] Batch 95 - Loss: 10.643758
[2025-05-11 14:04:20] === Epoch 22/100 Summary ===
Training Loss: 10.628113
Validation Loss: 12.023277
Learning Rate: 0.00003000
[2025-05-11 14:04:21] Batch 0 - Loss: 10.665865
[2025-05-11 14:04:22] Batch 5 - Loss: 11.757022
[2025-05-11 14:04:24] Batch 10 - Loss: 11.390766
[2025-05-11 14:04:26] Batch 15 - Loss: 11.241834
[2025-05-11 14:04:28] Batch 20 - Loss: 11.182429
[2025-05-11 14:04:29] Batch 25 - Loss: 11.125737
[2025-05-11 14:04:31] Batch 30 - Loss: 10.889592
[2025-05-11 14:04:33] Batch 35 - Loss: 10.770924
[2025-05-11 14:04:35] Batch 40 - Loss: 10.954215
[2025-05-11 14:04:36] Batch 45 - Loss: 10.888218
[2025-05-11 14:04:38] Batch 50 - Loss: 10.864580
[2025-05-11 14:04:40] Batch 55 - Loss: 10.783210
[2025-05-11 14:04:42] Batch 60 - Loss: 10.694363
[2025-05-11 14:04:43] Batch 65 - Loss: 10.629437
[2025-05-11 14:04:45] Batch 70 - Loss: 10.664505
[2025-05-11 14:04:47] Batch 75 - Loss: 10.597376
[2025-05-11 14:04:49] Batch 80 - Loss: 10.531899
[2025-05-11 14:04:51] Batch 85 - Loss: 10.532260
[2025-05-11 14:04:52] Batch 90 - Loss: 10.546569
[2025-05-11 14:04:54] Batch 95 - Loss: 10.528115
[2025-05-11 14:05:01] === Epoch 23/100 Summary ===
Training Loss: 10.512280
Validation Loss: 12.045570
Learning Rate: 0.00003000
[2025-05-11 14:05:02] Batch 0 - Loss: 8.283761
[2025-05-11 14:05:03] Batch 5 - Loss: 10.437446
[2025-05-11 14:05:05] Batch 10 - Loss: 10.318124
[2025-05-11 14:05:07] Batch 15 - Loss: 10.127913
[2025-05-11 14:05:08] Batch 20 - Loss: 10.186399
[2025-05-11 14:05:10] Batch 25 - Loss: 10.080950
[2025-05-11 14:05:12] Batch 30 - Loss: 9.991144
[2025-05-11 14:05:14] Batch 35 - Loss: 9.945492
[2025-05-11 14:05:20] === Epoch 24/100 Summary ===
Training Loss: 9.960550
Validation Loss: 12.014670
Learning Rate: 0.00003000
[2025-05-11 14:05:21] Batch 0 - Loss: 10.781527
[2025-05-11 14:05:22] Batch 5 - Loss: 9.982421
[2025-05-11 14:05:24] Batch 10 - Loss: 10.589957
[2025-05-11 14:05:26] Batch 15 - Loss: 10.424490
[2025-05-11 14:05:28] Batch 20 - Loss: 10.423464
[2025-05-11 14:05:29] Batch 25 - Loss: 10.569290
[2025-05-11 14:05:31] Batch 30 - Loss: 10.677320
[2025-05-11 14:05:33] Batch 35 - Loss: 10.793018
[2025-05-11 14:05:35] Batch 40 - Loss: 10.806093
[2025-05-11 14:05:36] Batch 45 - Loss: 10.835130
[2025-05-11 14:05:38] Batch 50 - Loss: 10.860357
[2025-05-11 14:05:40] Batch 55 - Loss: 10.867496
[2025-05-11 14:05:42] Batch 60 - Loss: 10.888957
[2025-05-11 14:05:43] Batch 65 - Loss: 10.809254
[2025-05-11 14:05:45] Batch 70 - Loss: 10.745678
[2025-05-11 14:05:47] Batch 75 - Loss: 10.739408
[2025-05-11 14:05:49] Batch 80 - Loss: 10.636542
[2025-05-11 14:05:51] Batch 85 - Loss: 10.628083
[2025-05-11 14:05:52] Batch 90 - Loss: 10.572618
[2025-05-11 14:05:54] Batch 95 - Loss: 10.593528
[2025-05-11 14:06:02] === Epoch 25/100 Summary ===
Training Loss: 10.596966
Validation Loss: 11.974344
Learning Rate: 0.00003000
[2025-05-11 14:06:02] Generating detection visualizations for epoch 25...
[2025-05-11 14:06:02] Saved visualization to trained_models/custom_detector\visualizations\epoch_25\sample_1.png
[2025-05-11 14:06:02] Saved visualization to trained_models/custom_detector\visualizations\epoch_25\sample_2.png
[2025-05-11 14:06:03] Saved visualization to trained_models/custom_detector\visualizations\epoch_25\sample_3.png
[2025-05-11 14:06:03] Batch 0 - Loss: 9.856408
[2025-05-11 14:06:05] Batch 5 - Loss: 9.736073
[2025-05-11 14:06:07] Batch 10 - Loss: 10.234088
[2025-05-11 14:06:08] Batch 15 - Loss: 10.404267
[2025-05-11 14:06:10] Batch 20 - Loss: 10.350685
[2025-05-11 14:06:12] Batch 25 - Loss: 10.353989
[2025-05-11 14:06:14] Batch 30 - Loss: 10.485666
[2025-05-11 14:06:15] Batch 35 - Loss: 10.433996
[2025-05-11 14:06:17] Batch 40 - Loss: 10.375327
[2025-05-11 14:06:19] Batch 45 - Loss: 10.340999
[2025-05-11 14:06:21] Batch 50 - Loss: 10.477984
[2025-05-11 14:06:22] Batch 55 - Loss: 10.459093
[2025-05-11 14:06:24] Batch 60 - Loss: 10.539594
[2025-05-11 14:06:26] Batch 65 - Loss: 10.480321
[2025-05-11 14:06:28] Batch 70 - Loss: 10.481517
[2025-05-11 14:06:29] Batch 75 - Loss: 10.537985
[2025-05-11 14:06:31] Batch 80 - Loss: 10.549386
[2025-05-11 14:06:33] Batch 85 - Loss: 10.535722
[2025-05-11 14:06:35] Batch 90 - Loss: 10.621656
[2025-05-11 14:06:37] Batch 95 - Loss: 10.581413
[2025-05-11 14:06:44] === Epoch 26/100 Summary ===
Training Loss: 10.556979
Validation Loss: 11.945892
Learning Rate: 0.00003000
[2025-05-11 14:06:44] Batch 0 - Loss: 10.040706
[2025-05-11 14:06:46] Batch 5 - Loss: 10.506124
[2025-05-11 14:06:48] Batch 10 - Loss: 10.369830
[2025-05-11 14:06:50] Batch 15 - Loss: 10.365369
[2025-05-11 14:06:51] Batch 20 - Loss: 10.264910
[2025-05-11 14:06:53] Batch 25 - Loss: 10.319081
[2025-05-11 14:06:55] Batch 30 - Loss: 10.330256
[2025-05-11 14:06:57] Batch 35 - Loss: 10.412327
[2025-05-11 14:06:59] Batch 40 - Loss: 10.366660
[2025-05-11 14:07:00] Batch 45 - Loss: 10.335366
[2025-05-11 14:07:02] Batch 50 - Loss: 10.282171
[2025-05-11 14:07:04] Batch 55 - Loss: 10.291071
[2025-05-11 14:07:06] Batch 60 - Loss: 10.260436
[2025-05-11 14:07:07] Batch 65 - Loss: 10.231472
[2025-05-11 14:07:09] Batch 70 - Loss: 10.207970
[2025-05-11 14:07:11] Batch 75 - Loss: 10.183151
[2025-05-11 14:07:13] Batch 80 - Loss: 10.201894
[2025-05-11 14:07:14] Batch 85 - Loss: 10.224341
[2025-05-11 14:07:16] Batch 90 - Loss: 10.226995
[2025-05-11 14:07:18] Batch 95 - Loss: 10.190563
[2025-05-11 14:07:25] === Epoch 27/100 Summary ===
Training Loss: 10.179037
Validation Loss: 11.903381
Learning Rate: 0.00003000
[2025-05-11 14:07:25] Batch 0 - Loss: 10.973164
[2025-05-11 14:07:27] Batch 5 - Loss: 10.008431
[2025-05-11 14:07:29] Batch 10 - Loss: 9.883340
[2025-05-11 14:07:31] Batch 15 - Loss: 9.864531
[2025-05-11 14:07:32] Batch 20 - Loss: 9.700709
[2025-05-11 14:07:34] Batch 25 - Loss: 9.703040
[2025-05-11 14:07:36] Batch 30 - Loss: 9.754169
[2025-05-11 14:07:38] Batch 35 - Loss: 9.887897
[2025-05-11 14:07:44] === Epoch 28/100 Summary ===
Training Loss: 9.875016
Validation Loss: 11.916129
Learning Rate: 0.00003000
[2025-05-11 14:07:45] Batch 0 - Loss: 11.498011
[2025-05-11 14:07:46] Batch 5 - Loss: 10.517847
[2025-05-11 14:07:48] Batch 10 - Loss: 10.826386
[2025-05-11 14:07:50] Batch 15 - Loss: 10.628284
[2025-05-11 14:07:52] Batch 20 - Loss: 10.686680
[2025-05-11 14:07:54] Batch 25 - Loss: 10.724945
[2025-05-11 14:07:55] Batch 30 - Loss: 10.704134
[2025-05-11 14:07:57] Batch 35 - Loss: 10.628851
[2025-05-11 14:07:59] Batch 40 - Loss: 10.689455
[2025-05-11 14:08:01] Batch 45 - Loss: 10.652495
[2025-05-11 14:08:02] Batch 50 - Loss: 10.675297
[2025-05-11 14:08:04] Batch 55 - Loss: 10.600538
[2025-05-11 14:08:06] Batch 60 - Loss: 10.613558
[2025-05-11 14:08:08] Batch 65 - Loss: 10.595322
[2025-05-11 14:08:09] Batch 70 - Loss: 10.566507
[2025-05-11 14:08:11] Batch 75 - Loss: 10.525719
[2025-05-11 14:08:13] Batch 80 - Loss: 10.481256
[2025-05-11 14:08:15] Batch 85 - Loss: 10.488847
[2025-05-11 14:08:16] Batch 90 - Loss: 10.421498
[2025-05-11 14:08:18] Batch 95 - Loss: 10.418684
[2025-05-11 14:08:25] === Epoch 29/100 Summary ===
Training Loss: 10.431314
Validation Loss: 11.918802
Learning Rate: 0.00003000
[2025-05-11 14:08:26] Batch 0 - Loss: 10.078736
[2025-05-11 14:08:27] Batch 5 - Loss: 10.609458
[2025-05-11 14:08:29] Batch 10 - Loss: 11.122933
[2025-05-11 14:08:31] Batch 15 - Loss: 10.841434
[2025-05-11 14:08:33] Batch 20 - Loss: 10.518131
[2025-05-11 14:08:34] Batch 25 - Loss: 10.326504
[2025-05-11 14:08:36] Batch 30 - Loss: 10.338806
[2025-05-11 14:08:38] Batch 35 - Loss: 10.292363
[2025-05-11 14:08:40] Batch 40 - Loss: 10.166600
[2025-05-11 14:08:41] Batch 45 - Loss: 10.261584
[2025-05-11 14:08:43] Batch 50 - Loss: 10.446303
[2025-05-11 14:08:45] Batch 55 - Loss: 10.414811
[2025-05-11 14:08:47] Batch 60 - Loss: 10.421543
[2025-05-11 14:08:49] Batch 65 - Loss: 10.390457
[2025-05-11 14:08:51] Batch 70 - Loss: 10.403308
[2025-05-11 14:08:53] Batch 75 - Loss: 10.444822
[2025-05-11 14:08:54] Batch 80 - Loss: 10.473056
[2025-05-11 14:08:56] Batch 85 - Loss: 10.536673
[2025-05-11 14:08:58] Batch 90 - Loss: 10.589673
[2025-05-11 14:09:00] Batch 95 - Loss: 10.555754
[2025-05-11 14:09:08] === Epoch 30/100 Summary ===
Training Loss: 10.524706
Validation Loss: 11.922600
Learning Rate: 0.00003000
[2025-05-11 14:09:08] Generating detection visualizations for epoch 30...
[2025-05-11 14:09:09] Saved visualization to trained_models/custom_detector\visualizations\epoch_30\sample_1.png
[2025-05-11 14:09:09] Saved visualization to trained_models/custom_detector\visualizations\epoch_30\sample_2.png
[2025-05-11 14:09:09] Saved visualization to trained_models/custom_detector\visualizations\epoch_30\sample_3.png
[2025-05-11 14:09:10] Batch 0 - Loss: 10.162657
[2025-05-11 14:09:12] Batch 5 - Loss: 11.374906
[2025-05-11 14:09:13] Batch 10 - Loss: 10.349481
[2025-05-11 14:09:15] Batch 15 - Loss: 10.198007
[2025-05-11 14:09:17] Batch 20 - Loss: 10.236712
[2025-05-11 14:09:19] Batch 25 - Loss: 10.397476
[2025-05-11 14:09:21] Batch 30 - Loss: 10.408065
[2025-05-11 14:09:22] Batch 35 - Loss: 10.355571
[2025-05-11 14:09:24] Batch 40 - Loss: 10.434536
[2025-05-11 14:09:26] Batch 45 - Loss: 10.392113
[2025-05-11 14:09:28] Batch 50 - Loss: 10.361690
[2025-05-11 14:09:30] Batch 55 - Loss: 10.328901
[2025-05-11 14:09:32] Batch 60 - Loss: 10.317707
[2025-05-11 14:09:33] Batch 65 - Loss: 10.266861
[2025-05-11 14:09:35] Batch 70 - Loss: 10.247255
[2025-05-11 14:09:37] Batch 75 - Loss: 10.225352
[2025-05-11 14:09:39] Batch 80 - Loss: 10.203319
[2025-05-11 14:09:41] Batch 85 - Loss: 10.213401
[2025-05-11 14:09:42] Batch 90 - Loss: 10.198037
[2025-05-11 14:09:44] Batch 95 - Loss: 10.155565
[2025-05-11 14:09:52] === Epoch 31/100 Summary ===
Training Loss: 10.132565
Validation Loss: 11.880003
Learning Rate: 0.00003000
[2025-05-11 14:09:52] Batch 0 - Loss: 8.541871
[2025-05-11 14:09:54] Batch 5 - Loss: 9.212584
[2025-05-11 14:09:56] Batch 10 - Loss: 9.133804
[2025-05-11 14:09:58] Batch 15 - Loss: 9.268794
[2025-05-11 14:10:00] Batch 20 - Loss: 9.346230
[2025-05-11 14:10:02] Batch 25 - Loss: 9.314109
[2025-05-11 14:10:03] Batch 30 - Loss: 9.396020
[2025-05-11 14:10:05] Batch 35 - Loss: 9.442666
[2025-05-11 14:10:12] === Epoch 32/100 Summary ===
Training Loss: 9.458942
Validation Loss: 11.852714
Learning Rate: 0.00003000
[2025-05-11 14:10:12] Batch 0 - Loss: 10.995710
[2025-05-11 14:10:14] Batch 5 - Loss: 10.140365
[2025-05-11 14:10:16] Batch 10 - Loss: 10.264567
[2025-05-11 14:10:18] Batch 15 - Loss: 10.177509
[2025-05-11 14:10:20] Batch 20 - Loss: 10.819695
[2025-05-11 14:10:21] Batch 25 - Loss: 10.859246
[2025-05-11 14:10:23] Batch 30 - Loss: 10.817513
[2025-05-11 14:10:25] Batch 35 - Loss: 10.709797
[2025-05-11 14:10:27] Batch 40 - Loss: 10.756533
[2025-05-11 14:10:29] Batch 45 - Loss: 10.780935
[2025-05-11 14:10:30] Batch 50 - Loss: 10.778225
[2025-05-11 14:10:32] Batch 55 - Loss: 10.768393
[2025-05-11 14:10:34] Batch 60 - Loss: 10.700793
[2025-05-11 14:10:36] Batch 65 - Loss: 10.624542
[2025-05-11 14:10:37] Batch 70 - Loss: 10.613936
[2025-05-11 14:10:39] Batch 75 - Loss: 10.588056
[2025-05-11 14:10:41] Batch 80 - Loss: 10.553135
[2025-05-11 14:10:43] Batch 85 - Loss: 10.524261
[2025-05-11 14:10:44] Batch 90 - Loss: 10.460474
[2025-05-11 14:10:46] Batch 95 - Loss: 10.401544
[2025-05-11 14:10:53] === Epoch 33/100 Summary ===
Training Loss: 10.373096
Validation Loss: 11.919193
Learning Rate: 0.00003000
[2025-05-11 14:10:54] Batch 0 - Loss: 10.688320
[2025-05-11 14:10:55] Batch 5 - Loss: 10.196673
[2025-05-11 14:10:57] Batch 10 - Loss: 9.599944
[2025-05-11 14:10:59] Batch 15 - Loss: 9.664997
[2025-05-11 14:11:01] Batch 20 - Loss: 9.655874
[2025-05-11 14:11:02] Batch 25 - Loss: 9.862446
[2025-05-11 14:11:04] Batch 30 - Loss: 9.824155
[2025-05-11 14:11:06] Batch 35 - Loss: 9.923641
[2025-05-11 14:11:08] Batch 40 - Loss: 10.007184
[2025-05-11 14:11:09] Batch 45 - Loss: 10.143209
[2025-05-11 14:11:11] Batch 50 - Loss: 10.178518
[2025-05-11 14:11:13] Batch 55 - Loss: 10.218903
[2025-05-11 14:11:15] Batch 60 - Loss: 10.259391
[2025-05-11 14:11:17] Batch 65 - Loss: 10.335501
[2025-05-11 14:11:18] Batch 70 - Loss: 10.316825
[2025-05-11 14:11:20] Batch 75 - Loss: 10.293494
[2025-05-11 14:11:22] Batch 80 - Loss: 10.309579
[2025-05-11 14:11:24] Batch 85 - Loss: 10.356346
[2025-05-11 14:11:26] Batch 90 - Loss: 10.386433
[2025-05-11 14:11:27] Batch 95 - Loss: 10.364123
[2025-05-11 14:11:35] === Epoch 34/100 Summary ===
Training Loss: 10.376035
Validation Loss: 11.838758
Learning Rate: 0.00003000
[2025-05-11 14:11:35] Batch 0 - Loss: 9.775308
[2025-05-11 14:11:37] Batch 5 - Loss: 10.639182
[2025-05-11 14:11:39] Batch 10 - Loss: 10.373590
[2025-05-11 14:11:41] Batch 15 - Loss: 10.530221
[2025-05-11 14:11:42] Batch 20 - Loss: 10.347753
[2025-05-11 14:11:44] Batch 25 - Loss: 10.310074
[2025-05-11 14:11:46] Batch 30 - Loss: 10.206697
[2025-05-11 14:11:48] Batch 35 - Loss: 10.180076
[2025-05-11 14:11:50] Batch 40 - Loss: 10.098607
[2025-05-11 14:11:51] Batch 45 - Loss: 10.232800
[2025-05-11 14:11:53] Batch 50 - Loss: 10.227815
[2025-05-11 14:11:55] Batch 55 - Loss: 10.139710
[2025-05-11 14:11:57] Batch 60 - Loss: 10.050420
[2025-05-11 14:11:59] Batch 65 - Loss: 9.994261
[2025-05-11 14:12:01] Batch 70 - Loss: 9.981511
[2025-05-11 14:12:03] Batch 75 - Loss: 9.925029
[2025-05-11 14:12:04] Batch 80 - Loss: 9.944548
[2025-05-11 14:12:06] Batch 85 - Loss: 9.907548
[2025-05-11 14:12:08] Batch 90 - Loss: 9.911915
[2025-05-11 14:12:10] Batch 95 - Loss: 9.848314
[2025-05-11 14:12:19] === Epoch 35/100 Summary ===
Training Loss: 9.854498
Validation Loss: 11.814322
Learning Rate: 0.00003000
[2025-05-11 14:12:19] Generating detection visualizations for epoch 35...
[2025-05-11 14:12:19] Saved visualization to trained_models/custom_detector\visualizations\epoch_35\sample_1.png
[2025-05-11 14:12:20] Saved visualization to trained_models/custom_detector\visualizations\epoch_35\sample_2.png
[2025-05-11 14:12:20] Saved visualization to trained_models/custom_detector\visualizations\epoch_35\sample_3.png
[2025-05-11 14:12:20] Batch 0 - Loss: 10.440570
[2025-05-11 14:12:22] Batch 5 - Loss: 9.172929
[2025-05-11 14:12:24] Batch 10 - Loss: 9.172947
[2025-05-11 14:12:26] Batch 15 - Loss: 9.539406
[2025-05-11 14:12:28] Batch 20 - Loss: 9.364825
[2025-05-11 14:12:30] Batch 25 - Loss: 9.423079
[2025-05-11 14:12:32] Batch 30 - Loss: 9.480235
[2025-05-11 14:12:33] Batch 35 - Loss: 9.494587
[2025-05-11 14:12:40] === Epoch 36/100 Summary ===
Training Loss: 9.528419
Validation Loss: 11.866525
Learning Rate: 0.00003000
[2025-05-11 14:12:41] Batch 0 - Loss: 10.099531
[2025-05-11 14:12:43] Batch 5 - Loss: 9.679357
[2025-05-11 14:12:45] Batch 10 - Loss: 9.812341
[2025-05-11 14:12:47] Batch 15 - Loss: 9.743039
[2025-05-11 14:12:49] Batch 20 - Loss: 9.717943
[2025-05-11 14:12:51] Batch 25 - Loss: 9.895807
[2025-05-11 14:12:52] Batch 30 - Loss: 10.064242
[2025-05-11 14:12:54] Batch 35 - Loss: 10.142365
[2025-05-11 14:12:56] Batch 40 - Loss: 10.333871
[2025-05-11 14:12:58] Batch 45 - Loss: 10.316137
[2025-05-11 14:13:00] Batch 50 - Loss: 10.362347
[2025-05-11 14:13:02] Batch 55 - Loss: 10.329457
[2025-05-11 14:13:03] Batch 60 - Loss: 10.252544
[2025-05-11 14:13:05] Batch 65 - Loss: 10.232420
[2025-05-11 14:13:07] Batch 70 - Loss: 10.297264
[2025-05-11 14:13:09] Batch 75 - Loss: 10.233804
[2025-05-11 14:13:10] Batch 80 - Loss: 10.251678
[2025-05-11 14:13:12] Batch 85 - Loss: 10.249533
[2025-05-11 14:13:14] Batch 90 - Loss: 10.260310
[2025-05-11 14:13:16] Batch 95 - Loss: 10.270310
[2025-05-11 14:13:23] === Epoch 37/100 Summary ===
Training Loss: 10.241512
Validation Loss: 11.824556
Learning Rate: 0.00003000
[2025-05-11 14:13:24] Batch 0 - Loss: 11.460780
[2025-05-11 14:13:25] Batch 5 - Loss: 10.442060
[2025-05-11 14:13:27] Batch 10 - Loss: 10.056026
[2025-05-11 14:13:29] Batch 15 - Loss: 9.886992
[2025-05-11 14:13:31] Batch 20 - Loss: 9.796296
[2025-05-11 14:13:32] Batch 25 - Loss: 9.812326
[2025-05-11 14:13:34] Batch 30 - Loss: 9.897656
[2025-05-11 14:13:36] Batch 35 - Loss: 9.802317
[2025-05-11 14:13:38] Batch 40 - Loss: 9.795480
[2025-05-11 14:13:40] Batch 45 - Loss: 9.747917
[2025-05-11 14:13:41] Batch 50 - Loss: 9.712744
[2025-05-11 14:13:43] Batch 55 - Loss: 9.744957
[2025-05-11 14:13:45] Batch 60 - Loss: 9.823133
[2025-05-11 14:13:47] Batch 65 - Loss: 9.966846
[2025-05-11 14:13:49] Batch 70 - Loss: 10.012819
[2025-05-11 14:13:50] Batch 75 - Loss: 10.082778
[2025-05-11 14:13:52] Batch 80 - Loss: 10.108616
[2025-05-11 14:13:54] Batch 85 - Loss: 10.120978
[2025-05-11 14:13:56] Batch 90 - Loss: 10.172186
[2025-05-11 14:13:58] Batch 95 - Loss: 10.205277
[2025-05-11 14:14:05] === Epoch 38/100 Summary ===
Training Loss: 10.177387
Validation Loss: 11.792567
Learning Rate: 0.00003000
[2025-05-11 14:14:06] Batch 0 - Loss: 7.517548
[2025-05-11 14:14:08] Batch 5 - Loss: 9.692732
[2025-05-11 14:14:10] Batch 10 - Loss: 9.590244
[2025-05-11 14:14:12] Batch 15 - Loss: 10.045903
[2025-05-11 14:14:13] Batch 20 - Loss: 9.766885
[2025-05-11 14:14:15] Batch 25 - Loss: 9.872622
[2025-05-11 14:14:17] Batch 30 - Loss: 9.744891
[2025-05-11 14:14:19] Batch 35 - Loss: 9.831386
[2025-05-11 14:14:21] Batch 40 - Loss: 9.927289
[2025-05-11 14:14:23] Batch 45 - Loss: 10.032195
[2025-05-11 14:14:24] Batch 50 - Loss: 10.050078
[2025-05-11 14:14:26] Batch 55 - Loss: 10.039100
[2025-05-11 14:14:28] Batch 60 - Loss: 10.013172
[2025-05-11 14:14:30] Batch 65 - Loss: 10.025059
[2025-05-11 14:14:31] Batch 70 - Loss: 9.999470
[2025-05-11 14:14:33] Batch 75 - Loss: 9.996799
[2025-05-11 14:14:35] Batch 80 - Loss: 9.933024
[2025-05-11 14:14:37] Batch 85 - Loss: 9.947032
[2025-05-11 14:14:38] Batch 90 - Loss: 9.907803
[2025-05-11 14:14:40] Batch 95 - Loss: 9.875057
[2025-05-11 14:14:47] === Epoch 39/100 Summary ===
Training Loss: 9.831776
Validation Loss: 11.854322
Learning Rate: 0.00003000
[2025-05-11 14:14:47] Batch 0 - Loss: 8.174088
[2025-05-11 14:14:49] Batch 5 - Loss: 9.101364
[2025-05-11 14:14:51] Batch 10 - Loss: 9.799578
[2025-05-11 14:14:52] Batch 15 - Loss: 9.951885
[2025-05-11 14:14:54] Batch 20 - Loss: 9.659508
[2025-05-11 14:14:56] Batch 25 - Loss: 9.548477
[2025-05-11 14:14:58] Batch 30 - Loss: 9.776877
[2025-05-11 14:14:59] Batch 35 - Loss: 9.680691
[2025-05-11 14:15:05] === Epoch 40/100 Summary ===
Training Loss: 9.667231
Validation Loss: 11.906127
Learning Rate: 0.00003000
[2025-05-11 14:15:05] Generating detection visualizations for epoch 40...
[2025-05-11 14:15:06] Saved visualization to trained_models/custom_detector\visualizations\epoch_40\sample_1.png
[2025-05-11 14:15:06] Saved visualization to trained_models/custom_detector\visualizations\epoch_40\sample_2.png
[2025-05-11 14:15:06] Saved visualization to trained_models/custom_detector\visualizations\epoch_40\sample_3.png
[2025-05-11 14:15:07] Batch 0 - Loss: 11.267483
[2025-05-11 14:15:09] Batch 5 - Loss: 9.695461
[2025-05-11 14:15:11] Batch 10 - Loss: 10.463932
[2025-05-11 14:15:12] Batch 15 - Loss: 10.545684
[2025-05-11 14:15:14] Batch 20 - Loss: 10.370851
[2025-05-11 14:15:16] Batch 25 - Loss: 10.316397
[2025-05-11 14:15:17] Batch 30 - Loss: 10.331451
[2025-05-11 14:15:19] Batch 35 - Loss: 10.293010
[2025-05-11 14:15:21] Batch 40 - Loss: 10.216640
[2025-05-11 14:15:23] Batch 45 - Loss: 10.194000
[2025-05-11 14:15:24] Batch 50 - Loss: 10.171296
[2025-05-11 14:15:26] Batch 55 - Loss: 10.150867
[2025-05-11 14:15:28] Batch 60 - Loss: 10.139375
[2025-05-11 14:15:30] Batch 65 - Loss: 10.112104
[2025-05-11 14:15:31] Batch 70 - Loss: 10.055659
[2025-05-11 14:15:33] Batch 75 - Loss: 10.047405
[2025-05-11 14:15:35] Batch 80 - Loss: 10.082796
[2025-05-11 14:15:37] Batch 85 - Loss: 10.118202
[2025-05-11 14:15:38] Batch 90 - Loss: 10.129027
[2025-05-11 14:15:40] Batch 95 - Loss: 10.130214
[2025-05-11 14:15:47] === Epoch 41/100 Summary ===
Training Loss: 10.101145
Validation Loss: 11.856436
Learning Rate: 0.00003000
[2025-05-11 14:15:48] Batch 0 - Loss: 11.229792
[2025-05-11 14:15:49] Batch 5 - Loss: 9.912711
[2025-05-11 14:15:51] Batch 10 - Loss: 9.547981
[2025-05-11 14:15:53] Batch 15 - Loss: 9.704746
[2025-05-11 14:15:54] Batch 20 - Loss: 9.822746
[2025-05-11 14:15:56] Batch 25 - Loss: 9.845675
[2025-05-11 14:15:58] Batch 30 - Loss: 9.848849
[2025-05-11 14:16:00] Batch 35 - Loss: 9.882786
[2025-05-11 14:16:01] Batch 40 - Loss: 9.973326
[2025-05-11 14:16:03] Batch 45 - Loss: 10.146488
[2025-05-11 14:16:05] Batch 50 - Loss: 10.146284
[2025-05-11 14:16:07] Batch 55 - Loss: 10.084918
[2025-05-11 14:16:08] Batch 60 - Loss: 10.078513
[2025-05-11 14:16:10] Batch 65 - Loss: 10.098391
[2025-05-11 14:16:12] Batch 70 - Loss: 10.003879
[2025-05-11 14:16:13] Batch 75 - Loss: 10.069540
[2025-05-11 14:16:15] Batch 80 - Loss: 10.079631
[2025-05-11 14:16:17] Batch 85 - Loss: 10.049466
[2025-05-11 14:16:19] Batch 90 - Loss: 10.097257
[2025-05-11 14:16:20] Batch 95 - Loss: 10.136925
[2025-05-11 14:16:28] === Epoch 42/100 Summary ===
Training Loss: 10.102174
Validation Loss: 11.780572
Learning Rate: 0.00003000
[2025-05-11 14:16:28] Batch 0 - Loss: 12.231030
[2025-05-11 14:16:30] Batch 5 - Loss: 10.508292
[2025-05-11 14:16:31] Batch 10 - Loss: 10.310790
[2025-05-11 14:16:33] Batch 15 - Loss: 10.785986
[2025-05-11 14:16:35] Batch 20 - Loss: 10.641938
[2025-05-11 14:16:37] Batch 25 - Loss: 10.395818
[2025-05-11 14:16:38] Batch 30 - Loss: 10.417683
[2025-05-11 14:16:40] Batch 35 - Loss: 10.262115
[2025-05-11 14:16:42] Batch 40 - Loss: 10.193173
[2025-05-11 14:16:44] Batch 45 - Loss: 10.120530
[2025-05-11 14:16:45] Batch 50 - Loss: 10.109236
[2025-05-11 14:16:47] Batch 55 - Loss: 10.047140
[2025-05-11 14:16:49] Batch 60 - Loss: 10.085050
[2025-05-11 14:16:51] Batch 65 - Loss: 10.034871
[2025-05-11 14:16:52] Batch 70 - Loss: 10.009245
[2025-05-11 14:16:54] Batch 75 - Loss: 9.957492
[2025-05-11 14:16:56] Batch 80 - Loss: 9.916531
[2025-05-11 14:16:57] Batch 85 - Loss: 9.889786
[2025-05-11 14:16:59] Batch 90 - Loss: 9.864973
[2025-05-11 14:17:01] Batch 95 - Loss: 9.845361
[2025-05-11 14:17:08] === Epoch 43/100 Summary ===
Training Loss: 9.867785
Validation Loss: 11.772696
Learning Rate: 0.00003000
[2025-05-11 14:17:08] Batch 0 - Loss: 8.406743
[2025-05-11 14:17:10] Batch 5 - Loss: 8.993892
[2025-05-11 14:17:12] Batch 10 - Loss: 9.381638
[2025-05-11 14:17:13] Batch 15 - Loss: 9.234767
[2025-05-11 14:17:15] Batch 20 - Loss: 9.392604
[2025-05-11 14:17:17] Batch 25 - Loss: 9.360328
[2025-05-11 14:17:19] Batch 30 - Loss: 9.337456
[2025-05-11 14:17:20] Batch 35 - Loss: 9.295424
[2025-05-11 14:17:26] === Epoch 44/100 Summary ===
Training Loss: 9.281066
Validation Loss: 11.767137
Learning Rate: 0.00003000
[2025-05-11 14:17:27] Batch 0 - Loss: 10.356543
[2025-05-11 14:17:29] Batch 5 - Loss: 10.066631
[2025-05-11 14:17:31] Batch 10 - Loss: 9.630176
[2025-05-11 14:17:32] Batch 15 - Loss: 9.983130
[2025-05-11 14:17:34] Batch 20 - Loss: 9.833367
[2025-05-11 14:17:36] Batch 25 - Loss: 9.767643
[2025-05-11 14:17:38] Batch 30 - Loss: 9.986066
[2025-05-11 14:17:39] Batch 35 - Loss: 9.969049
[2025-05-11 14:17:41] Batch 40 - Loss: 10.078170
[2025-05-11 14:17:43] Batch 45 - Loss: 10.141732
[2025-05-11 14:17:44] Batch 50 - Loss: 10.222059
[2025-05-11 14:17:46] Batch 55 - Loss: 10.228544
[2025-05-11 14:17:48] Batch 60 - Loss: 10.299526
[2025-05-11 14:17:50] Batch 65 - Loss: 10.278944
[2025-05-11 14:17:51] Batch 70 - Loss: 10.177821
[2025-05-11 14:17:53] Batch 75 - Loss: 10.164614
[2025-05-11 14:17:55] Batch 80 - Loss: 10.120273
[2025-05-11 14:17:57] Batch 85 - Loss: 10.085936
[2025-05-11 14:17:59] Batch 90 - Loss: 10.011900
[2025-05-11 14:18:00] Batch 95 - Loss: 9.995964
[2025-05-11 14:18:08] === Epoch 45/100 Summary ===
Training Loss: 9.986453
Validation Loss: 11.760118
Learning Rate: 0.00003000
[2025-05-11 14:18:08] Generating detection visualizations for epoch 45...
[2025-05-11 14:18:08] Saved visualization to trained_models/custom_detector\visualizations\epoch_45\sample_1.png
[2025-05-11 14:18:08] Saved visualization to trained_models/custom_detector\visualizations\epoch_45\sample_2.png
[2025-05-11 14:18:09] Saved visualization to trained_models/custom_detector\visualizations\epoch_45\sample_3.png
[2025-05-11 14:18:09] Batch 0 - Loss: 9.452138
[2025-05-11 14:18:11] Batch 5 - Loss: 9.770717
[2025-05-11 14:18:12] Batch 10 - Loss: 9.909146
[2025-05-11 14:18:14] Batch 15 - Loss: 9.751738
[2025-05-11 14:18:16] Batch 20 - Loss: 9.722613
[2025-05-11 14:18:18] Batch 25 - Loss: 9.698749
[2025-05-11 14:18:20] Batch 30 - Loss: 9.929881
[2025-05-11 14:18:21] Batch 35 - Loss: 9.846935
[2025-05-11 14:18:23] Batch 40 - Loss: 9.938846
[2025-05-11 14:18:25] Batch 45 - Loss: 9.979288
