[2025-05-11 15:25:00] Starting training session
Model configuration: IMAGE_SIZE=(416, 416), BATCH_SIZE=16, EPOCHS=100, LEARNING_RATE=0.0001

[2025-05-11 15:25:00] === Starting Valorany Detector ===
[2025-05-11 15:25:00] Training data: ../tf_files/train.record
[2025-05-11 15:25:00] Validation data: ../tf_files/valid.record
[2025-05-11 15:25:00] Loading data from ../tf_files/train.record and ../tf_files/valid.record
[2025-05-11 15:25:01] Estimating dataset sizes...
[2025-05-11 15:25:01] Train dataset size: ~337 batches (approx. 5385 examples)
[2025-05-11 15:25:01] Validation dataset size: ~33 batches (approx. 514 examples)
[2025-05-11 15:25:01] Creating model...
[2025-05-11 15:25:01] Starting model training...
[2025-05-11 15:25:02] === Training Started ===
[2025-05-11 15:25:02] Model structure:
[2025-05-11 15:25:02] Total layers: 158
[2025-05-11 15:25:02] Total parameters: 3,181,393
[2025-05-11 15:25:02] Trainable parameters: 3,146,769
[2025-05-11 15:25:02] Non-trainable parameters: 34,624
[2025-05-11 15:25:16] Batch 0 - Loss: 476.818848
[2025-05-11 15:25:24] Batch 5 - Loss: 469.743805
[2025-05-11 15:25:31] Batch 10 - Loss: 442.957001
[2025-05-11 15:25:39] Batch 15 - Loss: 425.596527
[2025-05-11 15:25:47] Batch 20 - Loss: 408.308044
[2025-05-11 15:25:55] Batch 25 - Loss: 394.592468
[2025-05-11 15:26:03] Batch 30 - Loss: 381.722473
[2025-05-11 15:26:11] Batch 35 - Loss: 369.352783
[2025-05-11 15:26:19] Batch 40 - Loss: 359.020660
[2025-05-11 15:26:26] Batch 45 - Loss: 348.283600
[2025-05-11 15:26:34] Batch 50 - Loss: 338.798798
[2025-05-11 15:26:41] Batch 55 - Loss: 328.548492
[2025-05-11 15:26:49] Batch 60 - Loss: 319.588837
[2025-05-11 15:26:57] Batch 65 - Loss: 310.438141
[2025-05-11 15:27:05] Batch 70 - Loss: 301.499054
[2025-05-11 15:27:12] Batch 75 - Loss: 293.418579
[2025-05-11 15:27:20] Batch 80 - Loss: 285.014771
[2025-05-11 15:27:28] Batch 85 - Loss: 277.005249
[2025-05-11 15:27:35] Batch 90 - Loss: 269.021606
[2025-05-11 15:27:43] Batch 95 - Loss: 261.251160
[2025-05-11 15:27:55] === Epoch 1/100 Summary ===
Training Loss: 255.312851
Validation Loss: 121.810196
Learning Rate: 0.00003000
[2025-05-11 15:27:57] Batch 0 - Loss: 110.556053
[2025-05-11 15:28:04] Batch 5 - Loss: 101.260796
[2025-05-11 15:28:12] Batch 10 - Loss: 95.102936
[2025-05-11 15:28:19] Batch 15 - Loss: 90.373123
[2025-05-11 15:28:27] Batch 20 - Loss: 85.237839
[2025-05-11 15:28:35] Batch 25 - Loss: 80.628387
[2025-05-11 15:28:42] Batch 30 - Loss: 76.406654
[2025-05-11 15:28:50] Batch 35 - Loss: 72.233505
[2025-05-11 15:28:58] Batch 40 - Loss: 68.349106
[2025-05-11 15:29:05] Batch 45 - Loss: 64.702385
[2025-05-11 15:29:13] Batch 50 - Loss: 61.392353
[2025-05-11 15:29:21] Batch 55 - Loss: 58.656456
[2025-05-11 15:29:28] Batch 60 - Loss: 55.999050
[2025-05-11 15:29:36] Batch 65 - Loss: 53.552532
[2025-05-11 15:29:43] Batch 70 - Loss: 51.379585
[2025-05-11 15:29:51] Batch 75 - Loss: 49.130016
[2025-05-11 15:29:59] Batch 80 - Loss: 47.066441
[2025-05-11 15:30:06] Batch 85 - Loss: 45.209034
[2025-05-11 15:30:14] Batch 90 - Loss: 43.489990
[2025-05-11 15:30:21] Batch 95 - Loss: 41.850513
[2025-05-11 15:30:33] === Epoch 2/100 Summary ===
Training Loss: 40.653061
Validation Loss: 18.659904
Learning Rate: 0.00003000
[2025-05-11 15:30:34] Batch 0 - Loss: 11.285261
[2025-05-11 15:30:42] Batch 5 - Loss: 11.562076
[2025-05-11 15:30:50] Batch 10 - Loss: 11.381666
[2025-05-11 15:30:57] Batch 15 - Loss: 10.674665
[2025-05-11 15:31:05] Batch 20 - Loss: 10.320627
[2025-05-11 15:31:13] Batch 25 - Loss: 10.089498
[2025-05-11 15:31:20] Batch 30 - Loss: 9.804762
[2025-05-11 15:31:28] Batch 35 - Loss: 9.494556
[2025-05-11 15:31:35] Batch 40 - Loss: 9.356105
[2025-05-11 15:31:43] Batch 45 - Loss: 9.222054
[2025-05-11 15:31:51] Batch 50 - Loss: 9.042163
[2025-05-11 15:31:59] Batch 55 - Loss: 8.913416
[2025-05-11 15:32:06] Batch 60 - Loss: 8.754764
[2025-05-11 15:32:14] Batch 65 - Loss: 8.628581
[2025-05-11 15:32:22] Batch 70 - Loss: 8.478146
[2025-05-11 15:32:29] Batch 75 - Loss: 8.399493
[2025-05-11 15:32:37] Batch 80 - Loss: 8.306286
[2025-05-11 15:32:44] Batch 85 - Loss: 8.165612
[2025-05-11 15:32:52] Batch 90 - Loss: 8.030767
[2025-05-11 15:33:00] Batch 95 - Loss: 7.933239
[2025-05-11 15:33:11] === Epoch 3/100 Summary ===
Training Loss: 7.889432
Validation Loss: 15.252284
Learning Rate: 0.00003000
[2025-05-11 15:33:13] Batch 0 - Loss: 6.170002
[2025-05-11 15:33:20] Batch 5 - Loss: 5.864237
[2025-05-11 15:33:28] Batch 10 - Loss: 7.334158
[2025-05-11 15:33:36] Batch 15 - Loss: 6.835391
[2025-05-11 15:33:43] Batch 20 - Loss: 6.575479
[2025-05-11 15:33:51] Batch 25 - Loss: 6.475670
[2025-05-11 15:33:58] Batch 30 - Loss: 6.297945
[2025-05-11 15:34:06] Batch 35 - Loss: 6.130304
[2025-05-11 15:34:12] === Epoch 4/100 Summary ===
Training Loss: 6.095747
Validation Loss: 10.005710
Learning Rate: 0.00003000
[2025-05-11 15:34:14] Batch 0 - Loss: 6.695641
[2025-05-11 15:34:22] Batch 5 - Loss: 6.150549
[2025-05-11 15:34:29] Batch 10 - Loss: 6.431548
[2025-05-11 15:34:37] Batch 15 - Loss: 6.292964
[2025-05-11 15:34:45] Batch 20 - Loss: 6.240847
[2025-05-11 15:34:52] Batch 25 - Loss: 6.159365
[2025-05-11 15:35:00] Batch 30 - Loss: 5.984427
[2025-05-11 15:35:08] Batch 35 - Loss: 5.901491
[2025-05-11 15:35:15] Batch 40 - Loss: 5.913266
[2025-05-11 15:35:23] Batch 45 - Loss: 5.860984
[2025-05-11 15:35:30] Batch 50 - Loss: 5.811394
[2025-05-11 15:35:38] Batch 55 - Loss: 5.717301
[2025-05-11 15:35:46] Batch 60 - Loss: 5.636476
[2025-05-11 15:35:53] Batch 65 - Loss: 5.561913
[2025-05-11 15:36:01] Batch 70 - Loss: 5.545462
[2025-05-11 15:36:09] Batch 75 - Loss: 5.758778
[2025-05-11 15:36:16] Batch 80 - Loss: 5.697441
[2025-05-11 15:36:24] Batch 85 - Loss: 5.619148
[2025-05-11 15:36:32] Batch 90 - Loss: 5.787578
[2025-05-11 15:36:39] Batch 95 - Loss: 5.734049
[2025-05-11 15:36:51] === Epoch 5/100 Summary ===
Training Loss: 5.676611
Validation Loss: 6.339309
Learning Rate: 0.00003000
[2025-05-11 15:36:51] Generating detection visualizations for epoch 5...
[2025-05-11 15:36:52] Saved visualization to trained_models/custom_detector\visualizations\epoch_5\sample_1.png
[2025-05-11 15:36:52] Saved visualization to trained_models/custom_detector\visualizations\epoch_5\sample_2.png
[2025-05-11 15:36:53] Saved visualization to trained_models/custom_detector\visualizations\epoch_5\sample_3.png
[2025-05-11 15:36:54] Batch 0 - Loss: 4.516496
[2025-05-11 15:37:02] Batch 5 - Loss: 4.825569
[2025-05-11 15:37:10] Batch 10 - Loss: 4.638553
[2025-05-11 15:37:17] Batch 15 - Loss: 4.616937
[2025-05-11 15:37:25] Batch 20 - Loss: 4.762838
[2025-05-11 15:37:33] Batch 25 - Loss: 4.647962
[2025-05-11 15:37:40] Batch 30 - Loss: 4.635612
[2025-05-11 15:37:48] Batch 35 - Loss: 4.602228
[2025-05-11 15:37:56] Batch 40 - Loss: 4.804359
[2025-05-11 15:38:03] Batch 45 - Loss: 4.858185
[2025-05-11 15:38:11] Batch 50 - Loss: 4.836058
[2025-05-11 15:38:18] Batch 55 - Loss: 4.877826
[2025-05-11 15:38:26] Batch 60 - Loss: 4.839261
[2025-05-11 15:38:34] Batch 65 - Loss: 4.848995
[2025-05-11 15:38:41] Batch 70 - Loss: 4.830141
[2025-05-11 15:38:49] Batch 75 - Loss: 4.762225
[2025-05-11 15:38:57] Batch 80 - Loss: 4.709776
[2025-05-11 15:39:04] Batch 85 - Loss: 4.727858
[2025-05-11 15:39:12] Batch 90 - Loss: 4.702376
[2025-05-11 15:39:20] Batch 95 - Loss: 4.667804
[2025-05-11 15:39:31] === Epoch 6/100 Summary ===
Training Loss: 4.695724
Validation Loss: 5.492579
Learning Rate: 0.00003000
[2025-05-11 15:39:33] Batch 0 - Loss: 3.769061
[2025-05-11 15:39:40] Batch 5 - Loss: 4.624116
[2025-05-11 15:39:48] Batch 10 - Loss: 4.510242
[2025-05-11 15:39:56] Batch 15 - Loss: 4.500134
[2025-05-11 15:40:03] Batch 20 - Loss: 4.597484
[2025-05-11 15:40:11] Batch 25 - Loss: 4.524391
[2025-05-11 15:40:19] Batch 30 - Loss: 4.534951
[2025-05-11 15:40:26] Batch 35 - Loss: 4.510548
[2025-05-11 15:40:34] Batch 40 - Loss: 4.468871
[2025-05-11 15:40:42] Batch 45 - Loss: 4.469330
[2025-05-11 15:40:49] Batch 50 - Loss: 4.454781
[2025-05-11 15:40:57] Batch 55 - Loss: 4.416498
[2025-05-11 15:41:05] Batch 60 - Loss: 4.383400
[2025-05-11 15:41:12] Batch 65 - Loss: 4.401175
[2025-05-11 15:41:20] Batch 70 - Loss: 4.401812
[2025-05-11 15:41:28] Batch 75 - Loss: 4.376112
[2025-05-11 15:41:35] Batch 80 - Loss: 4.341621
[2025-05-11 15:41:43] Batch 85 - Loss: 4.325441
[2025-05-11 15:41:50] Batch 90 - Loss: 4.307847
[2025-05-11 15:41:58] Batch 95 - Loss: 4.308867
[2025-05-11 15:42:09] === Epoch 7/100 Summary ===
Training Loss: 4.299582
Validation Loss: 7.117614
Learning Rate: 0.00003000
[2025-05-11 15:42:11] Batch 0 - Loss: 6.162623
[2025-05-11 15:42:18] Batch 5 - Loss: 4.400691
[2025-05-11 15:42:26] Batch 10 - Loss: 4.092801
[2025-05-11 15:42:34] Batch 15 - Loss: 3.973135
[2025-05-11 15:42:41] Batch 20 - Loss: 4.249111
[2025-05-11 15:42:49] Batch 25 - Loss: 4.191978
[2025-05-11 15:42:57] Batch 30 - Loss: 4.193891
[2025-05-11 15:43:04] Batch 35 - Loss: 4.102980
[2025-05-11 15:43:10] === Epoch 8/100 Summary ===
Training Loss: 4.082463
Validation Loss: 6.105260
Learning Rate: 0.00003000
[2025-05-11 15:43:12] Batch 0 - Loss: 4.542915
[2025-05-11 15:43:20] Batch 5 - Loss: 4.288399
[2025-05-11 15:43:27] Batch 10 - Loss: 4.303123
[2025-05-11 15:43:35] Batch 15 - Loss: 4.658709
[2025-05-11 15:43:43] Batch 20 - Loss: 4.636670
[2025-05-11 15:43:50] Batch 25 - Loss: 4.472596
[2025-05-11 15:43:58] Batch 30 - Loss: 4.442405
[2025-05-11 15:44:06] Batch 35 - Loss: 4.383691
[2025-05-11 15:44:13] Batch 40 - Loss: 4.418049
[2025-05-11 15:44:21] Batch 45 - Loss: 4.434958
[2025-05-11 15:44:28] Batch 50 - Loss: 4.447259
[2025-05-11 15:44:36] Batch 55 - Loss: 4.437633
[2025-05-11 15:44:44] Batch 60 - Loss: 4.420378
[2025-05-11 15:44:51] Batch 65 - Loss: 4.384579
[2025-05-11 15:44:59] Batch 70 - Loss: 4.498147
[2025-05-11 15:45:07] Batch 75 - Loss: 4.595102
[2025-05-11 15:45:14] Batch 80 - Loss: 4.541808
[2025-05-11 15:45:22] Batch 85 - Loss: 4.497951
[2025-05-11 15:45:30] Batch 90 - Loss: 4.476373
[2025-05-11 15:45:37] Batch 95 - Loss: 4.462934
[2025-05-11 15:45:49] === Epoch 9/100 Summary ===
Training Loss: 4.423222
Validation Loss: 5.225450
Learning Rate: 0.00003000
[2025-05-11 15:45:50] Batch 0 - Loss: 3.465308
[2025-05-11 15:45:58] Batch 5 - Loss: 3.650906
[2025-05-11 15:46:06] Batch 10 - Loss: 3.518072
[2025-05-11 15:46:13] Batch 15 - Loss: 3.502498
[2025-05-11 15:46:21] Batch 20 - Loss: 3.449987
[2025-05-11 15:46:29] Batch 25 - Loss: 3.535691
[2025-05-11 15:46:36] Batch 30 - Loss: 3.605996
[2025-05-11 15:46:44] Batch 35 - Loss: 3.616169
[2025-05-11 15:46:52] Batch 40 - Loss: 3.585599
[2025-05-11 15:46:59] Batch 45 - Loss: 3.559345
[2025-05-11 15:47:07] Batch 50 - Loss: 3.649432
[2025-05-11 15:47:14] Batch 55 - Loss: 3.649521
[2025-05-11 15:47:22] Batch 60 - Loss: 3.642553
[2025-05-11 15:47:30] Batch 65 - Loss: 3.653055
[2025-05-11 15:47:37] Batch 70 - Loss: 3.638617
[2025-05-11 15:47:45] Batch 75 - Loss: 3.631998
[2025-05-11 15:47:53] Batch 80 - Loss: 3.630748
[2025-05-11 15:48:00] Batch 85 - Loss: 3.607996
[2025-05-11 15:48:08] Batch 90 - Loss: 3.609502
[2025-05-11 15:48:15] Batch 95 - Loss: 3.603611
[2025-05-11 15:48:27] === Epoch 10/100 Summary ===
Training Loss: 3.587977
Validation Loss: 5.420882
Learning Rate: 0.00003000
[2025-05-11 15:48:27] Generating detection visualizations for epoch 10...
[2025-05-11 15:48:27] Saved visualization to trained_models/custom_detector\visualizations\epoch_10\sample_1.png
[2025-05-11 15:48:28] Saved visualization to trained_models/custom_detector\visualizations\epoch_10\sample_2.png
[2025-05-11 15:48:28] Saved visualization to trained_models/custom_detector\visualizations\epoch_10\sample_3.png
[2025-05-11 15:48:29] Batch 0 - Loss: 2.505357
[2025-05-11 15:48:37] Batch 5 - Loss: 3.817702
[2025-05-11 15:48:45] Batch 10 - Loss: 3.474921
[2025-05-11 15:48:52] Batch 15 - Loss: 3.414320
[2025-05-11 15:49:00] Batch 20 - Loss: 3.347063
[2025-05-11 15:49:08] Batch 25 - Loss: 3.286940
[2025-05-11 15:49:15] Batch 30 - Loss: 3.335783
[2025-05-11 15:49:23] Batch 35 - Loss: 3.302469
[2025-05-11 15:49:31] Batch 40 - Loss: 3.329743
[2025-05-11 15:49:38] Batch 45 - Loss: 3.343337
[2025-05-11 15:49:46] Batch 50 - Loss: 3.376861
[2025-05-11 15:49:54] Batch 55 - Loss: 3.550298
[2025-05-11 15:50:01] Batch 60 - Loss: 3.534378
[2025-05-11 15:50:09] Batch 65 - Loss: 3.510304
[2025-05-11 15:50:17] Batch 70 - Loss: 3.602955
[2025-05-11 15:50:24] Batch 75 - Loss: 3.573361
[2025-05-11 15:50:32] Batch 80 - Loss: 3.556273
[2025-05-11 15:50:39] Batch 85 - Loss: 3.546233
[2025-05-11 15:50:47] Batch 90 - Loss: 3.500591
[2025-05-11 15:50:55] Batch 95 - Loss: 3.482223
[2025-05-11 15:51:06] === Epoch 11/100 Summary ===
Training Loss: 3.476230
Validation Loss: 4.922938
Learning Rate: 0.00003000
[2025-05-11 15:51:08] Batch 0 - Loss: 4.064960
[2025-05-11 15:51:16] Batch 5 - Loss: 2.829646
[2025-05-11 15:51:23] Batch 10 - Loss: 2.901736
[2025-05-11 15:51:31] Batch 15 - Loss: 3.050254
[2025-05-11 15:51:38] Batch 20 - Loss: 3.162248
[2025-05-11 15:51:46] Batch 25 - Loss: 3.171984
[2025-05-11 15:51:54] Batch 30 - Loss: 3.198685
[2025-05-11 15:52:01] Batch 35 - Loss: 3.152109
[2025-05-11 15:52:07] === Epoch 12/100 Summary ===
Training Loss: 3.152284
Validation Loss: 4.977615
Learning Rate: 0.00003000
[2025-05-11 15:52:09] Batch 0 - Loss: 3.212327
[2025-05-11 15:52:17] Batch 5 - Loss: 3.417231
[2025-05-11 15:52:24] Batch 10 - Loss: 3.555856
[2025-05-11 15:52:32] Batch 15 - Loss: 3.658788
[2025-05-11 15:52:40] Batch 20 - Loss: 3.717995
[2025-05-11 15:52:47] Batch 25 - Loss: 3.687855
[2025-05-11 15:52:55] Batch 30 - Loss: 3.659696
[2025-05-11 15:53:03] Batch 35 - Loss: 3.631301
[2025-05-11 15:53:10] Batch 40 - Loss: 3.600181
[2025-05-11 15:53:18] Batch 45 - Loss: 3.581778
[2025-05-11 15:53:26] Batch 50 - Loss: 3.555480
[2025-05-11 15:53:33] Batch 55 - Loss: 3.566813
[2025-05-11 15:53:41] Batch 60 - Loss: 3.577558
[2025-05-11 15:53:49] Batch 65 - Loss: 3.569955
[2025-05-11 15:53:57] Batch 70 - Loss: 3.525856
[2025-05-11 15:54:05] Batch 75 - Loss: 3.549834
[2025-05-11 15:54:13] Batch 80 - Loss: 3.516462
[2025-05-11 15:54:20] Batch 85 - Loss: 3.495309
[2025-05-11 15:54:28] Batch 90 - Loss: 3.471852
[2025-05-11 15:54:36] Batch 95 - Loss: 3.459175
[2025-05-11 15:54:47] === Epoch 13/100 Summary ===
Training Loss: 3.446539
Validation Loss: 5.256796
Learning Rate: 0.00003000
[2025-05-11 15:54:49] Batch 0 - Loss: 3.233257
[2025-05-11 15:54:56] Batch 5 - Loss: 3.037234
[2025-05-11 15:55:04] Batch 10 - Loss: 3.181787
[2025-05-11 15:55:12] Batch 15 - Loss: 3.154256
[2025-05-11 15:55:20] Batch 20 - Loss: 3.103225
[2025-05-11 15:55:27] Batch 25 - Loss: 3.136118
[2025-05-11 15:55:35] Batch 30 - Loss: 3.127588
[2025-05-11 15:55:43] Batch 35 - Loss: 3.112493
[2025-05-11 15:55:51] Batch 40 - Loss: 3.172912
[2025-05-11 15:55:59] Batch 45 - Loss: 3.163268
[2025-05-11 15:56:06] Batch 50 - Loss: 3.160414
[2025-05-11 15:56:14] Batch 55 - Loss: 3.149964
[2025-05-11 15:56:21] Batch 60 - Loss: 3.168756
[2025-05-11 15:56:29] Batch 65 - Loss: 3.157237
[2025-05-11 15:56:37] Batch 70 - Loss: 3.143355
[2025-05-11 15:56:44] Batch 75 - Loss: 3.117462
[2025-05-11 15:56:52] Batch 80 - Loss: 3.106440
[2025-05-11 15:57:00] Batch 85 - Loss: 3.107885
[2025-05-11 15:57:08] Batch 90 - Loss: 3.108334
[2025-05-11 15:57:16] Batch 95 - Loss: 3.119929
[2025-05-11 15:57:27] === Epoch 14/100 Summary ===
Training Loss: 3.109459
Validation Loss: 5.354849
Learning Rate: 0.00003000
[2025-05-11 15:57:28] Batch 0 - Loss: 2.868558
[2025-05-11 15:57:36] Batch 5 - Loss: 2.790403
[2025-05-11 15:57:44] Batch 10 - Loss: 2.620487
[2025-05-11 15:57:51] Batch 15 - Loss: 2.650676
[2025-05-11 15:57:59] Batch 20 - Loss: 2.623533
[2025-05-11 15:58:06] Batch 25 - Loss: 2.647300
[2025-05-11 15:58:14] Batch 30 - Loss: 2.716460
[2025-05-11 15:58:22] Batch 35 - Loss: 2.746544
[2025-05-11 15:58:29] Batch 40 - Loss: 2.756318
[2025-05-11 15:58:37] Batch 45 - Loss: 2.801407
[2025-05-11 15:58:45] Batch 50 - Loss: 2.833557
[2025-05-11 15:58:52] Batch 55 - Loss: 2.838397
[2025-05-11 15:59:00] Batch 60 - Loss: 2.826409
[2025-05-11 15:59:08] Batch 65 - Loss: 2.822598
[2025-05-11 15:59:15] Batch 70 - Loss: 2.822318
[2025-05-11 15:59:23] Batch 75 - Loss: 2.802088
[2025-05-11 15:59:31] Batch 80 - Loss: 2.811389
[2025-05-11 15:59:38] Batch 85 - Loss: 2.827601
[2025-05-11 15:59:46] Batch 90 - Loss: 2.813261
[2025-05-11 15:59:53] Batch 95 - Loss: 2.808944
[2025-05-11 16:00:05] === Epoch 15/100 Summary ===
Training Loss: 2.811406
Validation Loss: 4.952601
Learning Rate: 0.00003000
[2025-05-11 16:00:05] Generating detection visualizations for epoch 15...
[2025-05-11 16:00:05] Saved visualization to trained_models/custom_detector\visualizations\epoch_15\sample_1.png
[2025-05-11 16:00:05] Saved visualization to trained_models/custom_detector\visualizations\epoch_15\sample_2.png
[2025-05-11 16:00:06] Saved visualization to trained_models/custom_detector\visualizations\epoch_15\sample_3.png
[2025-05-11 16:00:07] Batch 0 - Loss: 2.042197
[2025-05-11 16:00:15] Batch 5 - Loss: 2.349879
[2025-05-11 16:00:23] Batch 10 - Loss: 2.720834
[2025-05-11 16:00:31] Batch 15 - Loss: 2.795895
[2025-05-11 16:00:38] Batch 20 - Loss: 2.762483
[2025-05-11 16:00:46] Batch 25 - Loss: 2.707520
[2025-05-11 16:00:54] Batch 30 - Loss: 2.740607
[2025-05-11 16:01:01] Batch 35 - Loss: 2.751182
[2025-05-11 16:01:07] === Epoch 16/100 Summary ===
Training Loss: 2.744616
Validation Loss: 4.892751
Learning Rate: 0.00003000
[2025-05-11 16:01:10] Batch 0 - Loss: 2.588517
[2025-05-11 16:01:17] Batch 5 - Loss: 3.232219
[2025-05-11 16:01:25] Batch 10 - Loss: 3.254269
[2025-05-11 16:01:32] Batch 15 - Loss: 3.272641
[2025-05-11 16:01:40] Batch 20 - Loss: 3.197985
[2025-05-11 16:01:48] Batch 25 - Loss: 3.179029
[2025-05-11 16:01:55] Batch 30 - Loss: 3.165777
[2025-05-11 16:02:03] Batch 35 - Loss: 3.161654
[2025-05-11 16:02:11] Batch 40 - Loss: 3.189371
[2025-05-11 16:02:18] Batch 45 - Loss: 3.171962
[2025-05-11 16:02:26] Batch 50 - Loss: 3.142072
[2025-05-11 16:02:34] Batch 55 - Loss: 3.086666
[2025-05-11 16:02:41] Batch 60 - Loss: 3.050866
[2025-05-11 16:02:49] Batch 65 - Loss: 3.034596
[2025-05-11 16:02:57] Batch 70 - Loss: 3.046476
[2025-05-11 16:03:04] Batch 75 - Loss: 3.032254
[2025-05-11 16:03:12] Batch 80 - Loss: 3.030751
[2025-05-11 16:03:20] Batch 85 - Loss: 3.025928
[2025-05-11 16:03:27] Batch 90 - Loss: 3.001006
[2025-05-11 16:03:35] Batch 95 - Loss: 2.972597
[2025-05-11 16:03:46] === Epoch 17/100 Summary ===
Training Loss: 2.987049
Validation Loss: 4.721376
Learning Rate: 0.00003000
[2025-05-11 16:03:48] Batch 0 - Loss: 2.733277
[2025-05-11 16:03:55] Batch 5 - Loss: 2.622634
[2025-05-11 16:04:03] Batch 10 - Loss: 2.654292
[2025-05-11 16:04:11] Batch 15 - Loss: 2.729331
[2025-05-11 16:04:19] Batch 20 - Loss: 2.776097
[2025-05-11 16:04:26] Batch 25 - Loss: 2.752510
[2025-05-11 16:04:34] Batch 30 - Loss: 2.768780
[2025-05-11 16:04:42] Batch 35 - Loss: 2.695695
[2025-05-11 16:04:49] Batch 40 - Loss: 2.680366
[2025-05-11 16:04:57] Batch 45 - Loss: 2.714077
[2025-05-11 16:05:05] Batch 50 - Loss: 2.725564
[2025-05-11 16:05:12] Batch 55 - Loss: 2.736268
[2025-05-11 16:05:20] Batch 60 - Loss: 2.761342
[2025-05-11 16:05:28] Batch 65 - Loss: 2.777531
[2025-05-11 16:05:35] Batch 70 - Loss: 2.744740
[2025-05-11 16:05:43] Batch 75 - Loss: 2.740399
[2025-05-11 16:05:51] Batch 80 - Loss: 2.744688
[2025-05-11 16:05:58] Batch 85 - Loss: 2.737100
[2025-05-11 16:06:06] Batch 90 - Loss: 2.737747
[2025-05-11 16:06:14] Batch 95 - Loss: 2.720332
[2025-05-11 16:06:25] === Epoch 18/100 Summary ===
Training Loss: 2.704803
Validation Loss: 4.205685
Learning Rate: 0.00003000
[2025-05-11 16:06:27] Batch 0 - Loss: 1.727777
[2025-05-11 16:06:34] Batch 5 - Loss: 2.504934
[2025-05-11 16:06:42] Batch 10 - Loss: 2.578755
[2025-05-11 16:06:50] Batch 15 - Loss: 2.610574
[2025-05-11 16:06:58] Batch 20 - Loss: 2.659316
[2025-05-11 16:07:05] Batch 25 - Loss: 2.618317
[2025-05-11 16:07:13] Batch 30 - Loss: 2.655765
[2025-05-11 16:07:21] Batch 35 - Loss: 2.662932
[2025-05-11 16:07:28] Batch 40 - Loss: 2.643419
[2025-05-11 16:07:36] Batch 45 - Loss: 2.655502
[2025-05-11 16:07:44] Batch 50 - Loss: 2.616853
[2025-05-11 16:07:51] Batch 55 - Loss: 2.639158
[2025-05-11 16:07:59] Batch 60 - Loss: 2.649153
[2025-05-11 16:08:07] Batch 65 - Loss: 2.632505
[2025-05-11 16:08:14] Batch 70 - Loss: 2.649750
[2025-05-11 16:08:22] Batch 75 - Loss: 2.657133
[2025-05-11 16:08:30] Batch 80 - Loss: 2.616857
[2025-05-11 16:08:37] Batch 85 - Loss: 2.608708
[2025-05-11 16:08:45] Batch 90 - Loss: 2.587162
[2025-05-11 16:08:53] Batch 95 - Loss: 2.607418
[2025-05-11 16:09:04] === Epoch 19/100 Summary ===
Training Loss: 2.612724
Validation Loss: 3.854951
Learning Rate: 0.00003000
[2025-05-11 16:09:06] Batch 0 - Loss: 2.034954
[2025-05-11 16:09:13] Batch 5 - Loss: 2.383949
[2025-05-11 16:09:21] Batch 10 - Loss: 2.134725
[2025-05-11 16:09:29] Batch 15 - Loss: 2.147232
[2025-05-11 16:09:36] Batch 20 - Loss: 2.225928
[2025-05-11 16:09:44] Batch 25 - Loss: 2.255076
[2025-05-11 16:09:52] Batch 30 - Loss: 2.351290
[2025-05-11 16:09:59] Batch 35 - Loss: 2.344785
[2025-05-11 16:10:05] === Epoch 20/100 Summary ===
Training Loss: 2.340094
Validation Loss: 3.666741
Learning Rate: 0.00003000
[2025-05-11 16:10:05] Generating detection visualizations for epoch 20...
[2025-05-11 16:10:06] Saved visualization to trained_models/custom_detector\visualizations\epoch_20\sample_1.png
[2025-05-11 16:10:06] Saved visualization to trained_models/custom_detector\visualizations\epoch_20\sample_2.png
[2025-05-11 16:10:07] Saved visualization to trained_models/custom_detector\visualizations\epoch_20\sample_3.png
[2025-05-11 16:10:09] Batch 0 - Loss: 3.102074
[2025-05-11 16:10:16] Batch 5 - Loss: 2.671235
[2025-05-11 16:10:24] Batch 10 - Loss: 2.981014
[2025-05-11 16:10:32] Batch 15 - Loss: 2.919740
[2025-05-11 16:10:39] Batch 20 - Loss: 2.854496
[2025-05-11 16:10:47] Batch 25 - Loss: 2.843583
[2025-05-11 16:10:55] Batch 30 - Loss: 2.793779
[2025-05-11 16:11:03] Batch 35 - Loss: 2.805302
[2025-05-11 16:11:11] Batch 40 - Loss: 2.767719
[2025-05-11 16:11:18] Batch 45 - Loss: 2.757740
[2025-05-11 16:11:26] Batch 50 - Loss: 2.713578
[2025-05-11 16:11:34] Batch 55 - Loss: 2.717581
[2025-05-11 16:11:41] Batch 60 - Loss: 2.716532
[2025-05-11 16:11:49] Batch 65 - Loss: 2.724463
[2025-05-11 16:11:57] Batch 70 - Loss: 2.703212
[2025-05-11 16:12:04] Batch 75 - Loss: 2.688783
[2025-05-11 16:12:12] Batch 80 - Loss: 2.659811
[2025-05-11 16:12:20] Batch 85 - Loss: 2.652272
[2025-05-11 16:12:27] Batch 90 - Loss: 2.634762
[2025-05-11 16:12:35] Batch 95 - Loss: 2.605953
[2025-05-11 16:12:47] === Epoch 21/100 Summary ===
Training Loss: 2.590139
Validation Loss: 3.540435
Learning Rate: 0.00003000
[2025-05-11 16:12:48] Batch 0 - Loss: 2.445414
[2025-05-11 16:12:56] Batch 5 - Loss: 2.983749
[2025-05-11 16:13:03] Batch 10 - Loss: 2.731032
[2025-05-11 16:13:11] Batch 15 - Loss: 2.591917
[2025-05-11 16:13:19] Batch 20 - Loss: 2.626648
[2025-05-11 16:13:26] Batch 25 - Loss: 2.522399
[2025-05-11 16:13:34] Batch 30 - Loss: 2.506151
[2025-05-11 16:13:42] Batch 35 - Loss: 2.463702
[2025-05-11 16:13:49] Batch 40 - Loss: 2.493773
[2025-05-11 16:13:57] Batch 45 - Loss: 2.507427
[2025-05-11 16:14:05] Batch 50 - Loss: 2.491762
[2025-05-11 16:14:13] Batch 55 - Loss: 2.448105
[2025-05-11 16:14:20] Batch 60 - Loss: 2.468272
[2025-05-11 16:14:28] Batch 65 - Loss: 2.483773
[2025-05-11 16:14:36] Batch 70 - Loss: 2.459691
[2025-05-11 16:14:43] Batch 75 - Loss: 2.438316
[2025-05-11 16:14:51] Batch 80 - Loss: 2.439314
[2025-05-11 16:14:59] Batch 85 - Loss: 2.440081
[2025-05-11 16:15:06] Batch 90 - Loss: 2.428041
[2025-05-11 16:15:14] Batch 95 - Loss: 2.417895
[2025-05-11 16:15:25] === Epoch 22/100 Summary ===
Training Loss: 2.408036
Validation Loss: 3.391247
Learning Rate: 0.00003000
[2025-05-11 16:15:27] Batch 0 - Loss: 2.573963
[2025-05-11 16:15:35] Batch 5 - Loss: 2.225254
[2025-05-11 16:15:43] Batch 10 - Loss: 2.302922
[2025-05-11 16:15:50] Batch 15 - Loss: 2.323822
[2025-05-11 16:15:58] Batch 20 - Loss: 2.309558
[2025-05-11 16:16:06] Batch 25 - Loss: 2.386249
[2025-05-11 16:16:13] Batch 30 - Loss: 2.354489
[2025-05-11 16:16:21] Batch 35 - Loss: 2.323169
[2025-05-11 16:16:29] Batch 40 - Loss: 2.315768
[2025-05-11 16:16:36] Batch 45 - Loss: 2.305482
[2025-05-11 16:16:44] Batch 50 - Loss: 2.292772
[2025-05-11 16:16:52] Batch 55 - Loss: 2.348930
[2025-05-11 16:17:00] Batch 60 - Loss: 2.323106
[2025-05-11 16:17:07] Batch 65 - Loss: 2.338881
[2025-05-11 16:17:15] Batch 70 - Loss: 2.361850
[2025-05-11 16:17:23] Batch 75 - Loss: 2.351948
[2025-05-11 16:17:30] Batch 80 - Loss: 2.333606
[2025-05-11 16:17:38] Batch 85 - Loss: 2.323587
[2025-05-11 16:17:46] Batch 90 - Loss: 2.304416
[2025-05-11 16:17:53] Batch 95 - Loss: 2.283696
[2025-05-11 16:18:05] === Epoch 23/100 Summary ===
Training Loss: 2.267547
Validation Loss: 3.040275
Learning Rate: 0.00003000
[2025-05-11 16:18:07] Batch 0 - Loss: 2.164265
[2025-05-11 16:18:14] Batch 5 - Loss: 2.183633
[2025-05-11 16:18:22] Batch 10 - Loss: 2.224830
[2025-05-11 16:18:30] Batch 15 - Loss: 2.123003
[2025-05-11 16:18:37] Batch 20 - Loss: 2.228138
[2025-05-11 16:18:45] Batch 25 - Loss: 2.202701
[2025-05-11 16:18:52] Batch 30 - Loss: 2.246135
[2025-05-11 16:19:00] Batch 35 - Loss: 2.207643
[2025-05-11 16:19:06] === Epoch 24/100 Summary ===
Training Loss: 2.200823
Validation Loss: 3.092073
Learning Rate: 0.00003000
[2025-05-11 16:19:08] Batch 0 - Loss: 2.066716
[2025-05-11 16:19:16] Batch 5 - Loss: 2.628986
[2025-05-11 16:19:23] Batch 10 - Loss: 2.538623
[2025-05-11 16:19:31] Batch 15 - Loss: 2.442629
[2025-05-11 16:19:39] Batch 20 - Loss: 2.410744
[2025-05-11 16:19:46] Batch 25 - Loss: 2.391724
[2025-05-11 16:19:54] Batch 30 - Loss: 2.344509
[2025-05-11 16:20:02] Batch 35 - Loss: 2.387140
[2025-05-11 16:20:09] Batch 40 - Loss: 2.384183
[2025-05-11 16:20:17] Batch 45 - Loss: 2.375677
[2025-05-11 16:20:25] Batch 50 - Loss: 2.374853
[2025-05-11 16:20:33] Batch 55 - Loss: 2.368851
[2025-05-11 16:20:40] Batch 60 - Loss: 2.377726
[2025-05-11 16:20:48] Batch 65 - Loss: 2.378331
[2025-05-11 16:20:56] Batch 70 - Loss: 2.342483
[2025-05-11 16:21:03] Batch 75 - Loss: 2.337758
[2025-05-11 16:21:11] Batch 80 - Loss: 2.334984
[2025-05-11 16:21:19] Batch 85 - Loss: 2.326160
[2025-05-11 16:21:26] Batch 90 - Loss: 2.318719
[2025-05-11 16:21:34] Batch 95 - Loss: 2.307446
[2025-05-11 16:21:45] === Epoch 25/100 Summary ===
Training Loss: 2.286157
Validation Loss: 3.545922
Learning Rate: 0.00003000
[2025-05-11 16:21:45] Generating detection visualizations for epoch 25...
[2025-05-11 16:21:46] Saved visualization to trained_models/custom_detector\visualizations\epoch_25\sample_1.png
[2025-05-11 16:21:46] Saved visualization to trained_models/custom_detector\visualizations\epoch_25\sample_2.png
[2025-05-11 16:21:46] Saved visualization to trained_models/custom_detector\visualizations\epoch_25\sample_3.png
[2025-05-11 16:21:48] Batch 0 - Loss: 2.656200
[2025-05-11 16:21:56] Batch 5 - Loss: 2.257779
[2025-05-11 16:22:03] Batch 10 - Loss: 2.255195
[2025-05-11 16:22:11] Batch 15 - Loss: 2.215302
[2025-05-11 16:22:19] Batch 20 - Loss: 2.144812
[2025-05-11 16:22:26] Batch 25 - Loss: 2.196561
[2025-05-11 16:22:34] Batch 30 - Loss: 2.191717
[2025-05-11 16:22:42] Batch 35 - Loss: 2.245973
[2025-05-11 16:22:49] Batch 40 - Loss: 2.282400
[2025-05-11 16:22:57] Batch 45 - Loss: 2.309669
[2025-05-11 16:23:05] Batch 50 - Loss: 2.307695
[2025-05-11 16:23:13] Batch 55 - Loss: 2.296831
[2025-05-11 16:23:20] Batch 60 - Loss: 2.293718
[2025-05-11 16:23:28] Batch 65 - Loss: 2.290309
[2025-05-11 16:23:36] Batch 70 - Loss: 2.265244
[2025-05-11 16:23:43] Batch 75 - Loss: 2.268340
[2025-05-11 16:23:51] Batch 80 - Loss: 2.261332
[2025-05-11 16:23:59] Batch 85 - Loss: 2.251200
[2025-05-11 16:24:06] Batch 90 - Loss: 2.240470
[2025-05-11 16:24:14] Batch 95 - Loss: 2.249667
[2025-05-11 16:24:26] === Epoch 26/100 Summary ===
Training Loss: 2.238542
Validation Loss: 3.422273
Learning Rate: 0.00003000
[2025-05-11 16:24:27] Batch 0 - Loss: 1.825781
[2025-05-11 16:24:35] Batch 5 - Loss: 2.016500
[2025-05-11 16:24:42] Batch 10 - Loss: 1.931676
[2025-05-11 16:24:50] Batch 15 - Loss: 2.005658
[2025-05-11 16:24:58] Batch 20 - Loss: 1.933324
[2025-05-11 16:25:05] Batch 25 - Loss: 2.071972
[2025-05-11 16:25:13] Batch 30 - Loss: 2.062610
[2025-05-11 16:25:21] Batch 35 - Loss: 2.127017
[2025-05-11 16:25:28] Batch 40 - Loss: 2.127108
[2025-05-11 16:25:36] Batch 45 - Loss: 2.103530
[2025-05-11 16:25:43] Batch 50 - Loss: 2.055149
[2025-05-11 16:25:51] Batch 55 - Loss: 2.057903
[2025-05-11 16:25:59] Batch 60 - Loss: 2.089271
[2025-05-11 16:26:06] Batch 65 - Loss: 2.081200
[2025-05-11 16:26:14] Batch 70 - Loss: 2.058535
[2025-05-11 16:26:22] Batch 75 - Loss: 2.056866
[2025-05-11 16:26:29] Batch 80 - Loss: 2.053450
[2025-05-11 16:26:37] Batch 85 - Loss: 2.042070
[2025-05-11 16:26:45] Batch 90 - Loss: 2.033937
[2025-05-11 16:26:52] Batch 95 - Loss: 2.033296
[2025-05-11 16:27:04] === Epoch 27/100 Summary ===
Training Loss: 2.027747
Validation Loss: 3.030922
Learning Rate: 0.00003000
[2025-05-11 16:27:05] Batch 0 - Loss: 2.531826
[2025-05-11 16:27:13] Batch 5 - Loss: 1.743701
[2025-05-11 16:27:20] Batch 10 - Loss: 1.747252
[2025-05-11 16:27:28] Batch 15 - Loss: 1.806562
[2025-05-11 16:27:36] Batch 20 - Loss: 1.807677
[2025-05-11 16:27:44] Batch 25 - Loss: 1.843202
[2025-05-11 16:27:51] Batch 30 - Loss: 1.861705
[2025-05-11 16:27:59] Batch 35 - Loss: 1.854886
[2025-05-11 16:28:05] === Epoch 28/100 Summary ===
Training Loss: 1.856550
Validation Loss: 3.112446
Learning Rate: 0.00003000
[2025-05-11 16:28:07] Batch 0 - Loss: 1.932414
[2025-05-11 16:28:15] Batch 5 - Loss: 2.150598
[2025-05-11 16:28:22] Batch 10 - Loss: 2.161260
[2025-05-11 16:28:30] Batch 15 - Loss: 2.121161
[2025-05-11 16:28:38] Batch 20 - Loss: 2.233293
[2025-05-11 16:28:45] Batch 25 - Loss: 2.301108
[2025-05-11 16:28:53] Batch 30 - Loss: 2.355479
[2025-05-11 16:29:01] Batch 35 - Loss: 2.334806
[2025-05-11 16:29:08] Batch 40 - Loss: 2.327070
[2025-05-11 16:29:16] Batch 45 - Loss: 2.319007
[2025-05-11 16:29:24] Batch 50 - Loss: 2.282996
[2025-05-11 16:29:31] Batch 55 - Loss: 2.252001
[2025-05-11 16:29:39] Batch 60 - Loss: 2.226512
[2025-05-11 16:29:47] Batch 65 - Loss: 2.231243
[2025-05-11 16:29:54] Batch 70 - Loss: 2.204100
[2025-05-11 16:30:02] Batch 75 - Loss: 2.180349
[2025-05-11 16:30:10] Batch 80 - Loss: 2.150334
[2025-05-11 16:30:17] Batch 85 - Loss: 2.147850
[2025-05-11 16:30:25] Batch 90 - Loss: 2.148989
[2025-05-11 16:30:33] Batch 95 - Loss: 2.134001
[2025-05-11 16:30:44] === Epoch 29/100 Summary ===
Training Loss: 2.133211
Validation Loss: 3.116596
Learning Rate: 0.00003000
[2025-05-11 16:30:46] Batch 0 - Loss: 1.619966
[2025-05-11 16:30:53] Batch 5 - Loss: 1.962162
[2025-05-11 16:31:01] Batch 10 - Loss: 1.867141
[2025-05-11 16:31:08] Batch 15 - Loss: 1.913965
[2025-05-11 16:31:16] Batch 20 - Loss: 1.917341
[2025-05-11 16:31:24] Batch 25 - Loss: 1.950780
[2025-05-11 16:31:31] Batch 30 - Loss: 1.940518
[2025-05-11 16:31:39] Batch 35 - Loss: 1.974923
[2025-05-11 16:31:47] Batch 40 - Loss: 2.026159
[2025-05-11 16:31:54] Batch 45 - Loss: 2.010743
[2025-05-11 16:32:02] Batch 50 - Loss: 2.017260
[2025-05-11 16:32:10] Batch 55 - Loss: 2.039380
[2025-05-11 16:32:17] Batch 60 - Loss: 2.047174
[2025-05-11 16:32:25] Batch 65 - Loss: 2.014855
[2025-05-11 16:32:33] Batch 70 - Loss: 2.021453
[2025-05-11 16:32:40] Batch 75 - Loss: 2.003664
[2025-05-11 16:32:48] Batch 80 - Loss: 2.012974
[2025-05-11 16:32:56] Batch 85 - Loss: 2.004345
[2025-05-11 16:33:03] Batch 90 - Loss: 1.992728
[2025-05-11 16:33:11] Batch 95 - Loss: 1.988668
[2025-05-11 16:33:22] === Epoch 30/100 Summary ===
Training Loss: 1.980699
Validation Loss: 3.041018
Learning Rate: 0.00003000
[2025-05-11 16:33:22] Generating detection visualizations for epoch 30...
[2025-05-11 16:33:23] Saved visualization to trained_models/custom_detector\visualizations\epoch_30\sample_1.png
[2025-05-11 16:33:23] Saved visualization to trained_models/custom_detector\visualizations\epoch_30\sample_2.png
[2025-05-11 16:33:23] Saved visualization to trained_models/custom_detector\visualizations\epoch_30\sample_3.png
[2025-05-11 16:33:25] Batch 0 - Loss: 1.979404
[2025-05-11 16:33:33] Batch 5 - Loss: 1.872374
[2025-05-11 16:33:40] Batch 10 - Loss: 1.910893
[2025-05-11 16:33:48] Batch 15 - Loss: 1.888445
[2025-05-11 16:33:56] Batch 20 - Loss: 1.895708
[2025-05-11 16:34:03] Batch 25 - Loss: 1.860270
[2025-05-11 16:34:11] Batch 30 - Loss: 1.829391
[2025-05-11 16:34:19] Batch 35 - Loss: 1.841436
[2025-05-11 16:34:26] Batch 40 - Loss: 1.823524
[2025-05-11 16:34:34] Batch 45 - Loss: 1.842699
[2025-05-11 16:34:42] Batch 50 - Loss: 1.836858
[2025-05-11 16:34:49] Batch 55 - Loss: 1.832031
[2025-05-11 16:34:57] Batch 60 - Loss: 1.820650
[2025-05-11 16:35:05] Batch 65 - Loss: 1.837665
[2025-05-11 16:35:12] Batch 70 - Loss: 1.854090
[2025-05-11 16:35:20] Batch 75 - Loss: 1.852111
[2025-05-11 16:35:28] Batch 80 - Loss: 1.834876
[2025-05-11 16:35:35] Batch 85 - Loss: 1.825360
[2025-05-11 16:35:43] Batch 90 - Loss: 1.796395
[2025-05-11 16:35:50] Batch 95 - Loss: 1.789397
[2025-05-11 16:36:02] === Epoch 31/100 Summary ===
Training Loss: 1.781513
Validation Loss: 2.861333
Learning Rate: 0.00003000
[2025-05-11 16:36:04] Batch 0 - Loss: 1.393041
[2025-05-11 16:36:11] Batch 5 - Loss: 1.631107
[2025-05-11 16:36:19] Batch 10 - Loss: 1.589783
[2025-05-11 16:36:26] Batch 15 - Loss: 1.596033
[2025-05-11 16:36:34] Batch 20 - Loss: 1.658478
[2025-05-11 16:36:42] Batch 25 - Loss: 1.634047
[2025-05-11 16:36:49] Batch 30 - Loss: 1.677279
[2025-05-11 16:36:57] Batch 35 - Loss: 1.693584
[2025-05-11 16:37:03] === Epoch 32/100 Summary ===
Training Loss: 1.687724
Validation Loss: 3.019076
Learning Rate: 0.00003000
[2025-05-11 16:37:05] Batch 0 - Loss: 1.867604
[2025-05-11 16:37:13] Batch 5 - Loss: 1.895593
[2025-05-11 16:37:21] Batch 10 - Loss: 1.997597
[2025-05-11 16:37:28] Batch 15 - Loss: 2.031101
[2025-05-11 16:37:36] Batch 20 - Loss: 1.991731
[2025-05-11 16:37:44] Batch 25 - Loss: 1.952656
[2025-05-11 16:37:51] Batch 30 - Loss: 2.014553
[2025-05-11 16:37:59] Batch 35 - Loss: 2.007578
[2025-05-11 16:38:07] Batch 40 - Loss: 2.025653
[2025-05-11 16:38:14] Batch 45 - Loss: 2.015430
[2025-05-11 16:38:22] Batch 50 - Loss: 2.041628
[2025-05-11 16:38:30] Batch 55 - Loss: 2.003951
[2025-05-11 16:38:38] Batch 60 - Loss: 2.011521
[2025-05-11 16:38:45] Batch 65 - Loss: 2.002795
[2025-05-11 16:38:53] Batch 70 - Loss: 1.992779
[2025-05-11 16:39:01] Batch 75 - Loss: 1.973447
[2025-05-11 16:39:09] Batch 80 - Loss: 1.951067
[2025-05-11 16:39:16] Batch 85 - Loss: 1.937047
[2025-05-11 16:39:24] Batch 90 - Loss: 1.925824
[2025-05-11 16:39:32] Batch 95 - Loss: 1.913404
[2025-05-11 16:39:43] === Epoch 33/100 Summary ===
Training Loss: 1.909870
Validation Loss: 3.187981
Learning Rate: 0.00003000
[2025-05-11 16:39:44] Batch 0 - Loss: 2.025736
[2025-05-11 16:39:52] Batch 5 - Loss: 1.794471
[2025-05-11 16:40:00] Batch 10 - Loss: 1.759209
[2025-05-11 16:40:08] Batch 15 - Loss: 1.807307
[2025-05-11 16:40:15] Batch 20 - Loss: 1.910468
[2025-05-11 16:40:23] Batch 25 - Loss: 1.911908
[2025-05-11 16:40:30] Batch 30 - Loss: 1.903634
[2025-05-11 16:40:38] Batch 35 - Loss: 1.893590
[2025-05-11 16:40:46] Batch 40 - Loss: 1.846409
[2025-05-11 16:40:53] Batch 45 - Loss: 1.881648
[2025-05-11 16:41:01] Batch 50 - Loss: 1.869514
[2025-05-11 16:41:09] Batch 55 - Loss: 1.856549
[2025-05-11 16:41:16] Batch 60 - Loss: 1.866125
[2025-05-11 16:41:24] Batch 65 - Loss: 1.876629
[2025-05-11 16:41:32] Batch 70 - Loss: 1.858791
[2025-05-11 16:41:39] Batch 75 - Loss: 1.855021
[2025-05-11 16:41:47] Batch 80 - Loss: 1.844566
[2025-05-11 16:41:54] Batch 85 - Loss: 1.854707
[2025-05-11 16:42:02] Batch 90 - Loss: 1.848005
[2025-05-11 16:42:10] Batch 95 - Loss: 1.853503
[2025-05-11 16:42:21] === Epoch 34/100 Summary ===
Training Loss: 1.835006
Validation Loss: 2.770485
Learning Rate: 0.00003000
[2025-05-11 16:42:23] Batch 0 - Loss: 1.630276
[2025-05-11 16:42:30] Batch 5 - Loss: 1.571492
[2025-05-11 16:42:38] Batch 10 - Loss: 1.516600
[2025-05-11 16:42:46] Batch 15 - Loss: 1.656519
[2025-05-11 16:42:53] Batch 20 - Loss: 1.742664
[2025-05-11 16:43:01] Batch 25 - Loss: 1.741179
[2025-05-11 16:43:08] Batch 30 - Loss: 1.725469
[2025-05-11 16:43:16] Batch 35 - Loss: 1.696102
[2025-05-11 16:43:24] Batch 40 - Loss: 1.663728
[2025-05-11 16:43:31] Batch 45 - Loss: 1.699982
[2025-05-11 16:43:39] Batch 50 - Loss: 1.684176
[2025-05-11 16:43:47] Batch 55 - Loss: 1.710184
[2025-05-11 16:43:54] Batch 60 - Loss: 1.701810
[2025-05-11 16:44:02] Batch 65 - Loss: 1.696741
[2025-05-11 16:44:10] Batch 70 - Loss: 1.711925
[2025-05-11 16:44:17] Batch 75 - Loss: 1.736517
[2025-05-11 16:44:25] Batch 80 - Loss: 1.723945
[2025-05-11 16:44:32] Batch 85 - Loss: 1.710362
[2025-05-11 16:44:40] Batch 90 - Loss: 1.692232
[2025-05-11 16:44:48] Batch 95 - Loss: 1.686396
[2025-05-11 16:44:59] === Epoch 35/100 Summary ===
Training Loss: 1.692985
Validation Loss: 2.886832
Learning Rate: 0.00003000
[2025-05-11 16:44:59] Generating detection visualizations for epoch 35...
[2025-05-11 16:44:59] Saved visualization to trained_models/custom_detector\visualizations\epoch_35\sample_1.png
[2025-05-11 16:45:00] Saved visualization to trained_models/custom_detector\visualizations\epoch_35\sample_2.png
[2025-05-11 16:45:00] Saved visualization to trained_models/custom_detector\visualizations\epoch_35\sample_3.png
[2025-05-11 16:45:02] Batch 0 - Loss: 1.318478
[2025-05-11 16:45:09] Batch 5 - Loss: 1.582540
[2025-05-11 16:45:17] Batch 10 - Loss: 1.515005
[2025-05-11 16:45:24] Batch 15 - Loss: 1.479021
[2025-05-11 16:45:32] Batch 20 - Loss: 1.492559
[2025-05-11 16:45:40] Batch 25 - Loss: 1.460551
[2025-05-11 16:45:47] Batch 30 - Loss: 1.498222
[2025-05-11 16:45:55] Batch 35 - Loss: 1.483012
[2025-05-11 16:46:01] === Epoch 36/100 Summary ===
Training Loss: 1.503435
Validation Loss: 2.880991
Learning Rate: 0.00003000
[2025-05-11 16:46:03] Batch 0 - Loss: 2.336546
[2025-05-11 16:46:10] Batch 5 - Loss: 1.896713
[2025-05-11 16:46:18] Batch 10 - Loss: 1.916925
[2025-05-11 16:46:26] Batch 15 - Loss: 2.047979
[2025-05-11 16:46:33] Batch 20 - Loss: 2.000648
[2025-05-11 16:46:41] Batch 25 - Loss: 1.921161
[2025-05-11 16:46:49] Batch 30 - Loss: 1.984835
[2025-05-11 16:46:56] Batch 35 - Loss: 1.946916
[2025-05-11 16:47:04] Batch 40 - Loss: 1.968921
[2025-05-11 16:47:12] Batch 45 - Loss: 1.944304
[2025-05-11 16:47:19] Batch 50 - Loss: 1.920239
[2025-05-11 16:47:27] Batch 55 - Loss: 1.948965
[2025-05-11 16:47:35] Batch 60 - Loss: 1.916858
[2025-05-11 16:47:42] Batch 65 - Loss: 1.897576
[2025-05-11 16:47:50] Batch 70 - Loss: 1.887609
[2025-05-11 16:47:58] Batch 75 - Loss: 1.853053
[2025-05-11 16:48:05] Batch 80 - Loss: 1.812961
[2025-05-11 16:48:13] Batch 85 - Loss: 1.808537
[2025-05-11 16:48:21] Batch 90 - Loss: 1.799303
[2025-05-11 16:48:28] Batch 95 - Loss: 1.792261
[2025-05-11 16:48:40] === Epoch 37/100 Summary ===
Training Loss: 1.782542
Validation Loss: 2.710299
Learning Rate: 0.00003000
[2025-05-11 16:48:41] Batch 0 - Loss: 1.306422
[2025-05-11 16:48:49] Batch 5 - Loss: 1.485333
[2025-05-11 16:48:57] Batch 10 - Loss: 1.550667
[2025-05-11 16:49:04] Batch 15 - Loss: 1.581561
[2025-05-11 16:49:12] Batch 20 - Loss: 1.569541
[2025-05-11 16:49:19] Batch 25 - Loss: 1.617141
[2025-05-11 16:49:27] Batch 30 - Loss: 1.638723
[2025-05-11 16:49:35] Batch 35 - Loss: 1.625422
[2025-05-11 16:49:42] Batch 40 - Loss: 1.663028
[2025-05-11 16:49:50] Batch 45 - Loss: 1.671328
[2025-05-11 16:49:58] Batch 50 - Loss: 1.670663
[2025-05-11 16:50:05] Batch 55 - Loss: 1.642651
[2025-05-11 16:50:13] Batch 60 - Loss: 1.631491
[2025-05-11 16:50:21] Batch 65 - Loss: 1.646206
[2025-05-11 16:50:28] Batch 70 - Loss: 1.656181
[2025-05-11 16:50:36] Batch 75 - Loss: 1.657349
[2025-05-11 16:50:44] Batch 80 - Loss: 1.643551
[2025-05-11 16:50:51] Batch 85 - Loss: 1.644158
[2025-05-11 16:50:59] Batch 90 - Loss: 1.649678
[2025-05-11 16:51:07] Batch 95 - Loss: 1.641202
[2025-05-11 16:51:18] === Epoch 38/100 Summary ===
Training Loss: 1.645854
Validation Loss: 2.638814
Learning Rate: 0.00003000
[2025-05-11 16:51:20] Batch 0 - Loss: 1.994492
[2025-05-11 16:51:27] Batch 5 - Loss: 1.555527
[2025-05-11 16:51:35] Batch 10 - Loss: 1.490853
[2025-05-11 16:51:43] Batch 15 - Loss: 1.467677
[2025-05-11 16:51:50] Batch 20 - Loss: 1.538486
[2025-05-11 16:51:58] Batch 25 - Loss: 1.567186
[2025-05-11 16:52:06] Batch 30 - Loss: 1.585454
[2025-05-11 16:52:13] Batch 35 - Loss: 1.585413
[2025-05-11 16:52:21] Batch 40 - Loss: 1.584753
[2025-05-11 16:52:28] Batch 45 - Loss: 1.582650
[2025-05-11 16:52:36] Batch 50 - Loss: 1.577274
[2025-05-11 16:52:44] Batch 55 - Loss: 1.549948
[2025-05-11 16:52:52] Batch 60 - Loss: 1.564454
[2025-05-11 16:52:59] Batch 65 - Loss: 1.573349
[2025-05-11 16:53:07] Batch 70 - Loss: 1.584188
[2025-05-11 16:53:14] Batch 75 - Loss: 1.580372
[2025-05-11 16:53:22] Batch 80 - Loss: 1.570261
[2025-05-11 16:53:30] Batch 85 - Loss: 1.557526
[2025-05-11 16:53:37] Batch 90 - Loss: 1.541941
[2025-05-11 16:53:45] Batch 95 - Loss: 1.535623
[2025-05-11 16:53:57] === Epoch 39/100 Summary ===
Training Loss: 1.525829
Validation Loss: 2.618181
Learning Rate: 0.00003000
[2025-05-11 16:53:58] Batch 0 - Loss: 2.143900
[2025-05-11 16:54:06] Batch 5 - Loss: 1.546644
[2025-05-11 16:54:13] Batch 10 - Loss: 1.591097
[2025-05-11 16:54:21] Batch 15 - Loss: 1.505211
[2025-05-11 16:54:29] Batch 20 - Loss: 1.474579
[2025-05-11 16:54:37] Batch 25 - Loss: 1.497537
[2025-05-11 16:54:44] Batch 30 - Loss: 1.472030
[2025-05-11 16:54:52] Batch 35 - Loss: 1.479805
[2025-05-11 16:54:58] === Epoch 40/100 Summary ===
Training Loss: 1.485104
Validation Loss: 2.640705
Learning Rate: 0.00003000
[2025-05-11 16:54:58] Generating detection visualizations for epoch 40...
[2025-05-11 16:54:58] Saved visualization to trained_models/custom_detector\visualizations\epoch_40\sample_1.png
[2025-05-11 16:54:59] Saved visualization to trained_models/custom_detector\visualizations\epoch_40\sample_2.png
[2025-05-11 16:54:59] Saved visualization to trained_models/custom_detector\visualizations\epoch_40\sample_3.png
[2025-05-11 16:55:01] Batch 0 - Loss: 1.827530
[2025-05-11 16:55:09] Batch 5 - Loss: 1.716200
[2025-05-11 16:55:16] Batch 10 - Loss: 1.701913
[2025-05-11 16:55:24] Batch 15 - Loss: 1.641356
[2025-05-11 16:55:32] Batch 20 - Loss: 1.628901
[2025-05-11 16:55:39] Batch 25 - Loss: 1.645935
[2025-05-11 16:55:47] Batch 30 - Loss: 1.685315
[2025-05-11 16:55:55] Batch 35 - Loss: 1.659690
[2025-05-11 16:56:02] Batch 40 - Loss: 1.644384
[2025-05-11 16:56:10] Batch 45 - Loss: 1.634797
[2025-05-11 16:56:17] Batch 50 - Loss: 1.622373
[2025-05-11 16:56:25] Batch 55 - Loss: 1.623396
[2025-05-11 16:56:33] Batch 60 - Loss: 1.616537
[2025-05-11 16:56:40] Batch 65 - Loss: 1.615290
[2025-05-11 16:56:48] Batch 70 - Loss: 1.607281
[2025-05-11 16:56:56] Batch 75 - Loss: 1.583061
[2025-05-11 16:57:03] Batch 80 - Loss: 1.572213
[2025-05-11 16:57:11] Batch 85 - Loss: 1.554664
[2025-05-11 16:57:19] Batch 90 - Loss: 1.564360
[2025-05-11 16:57:26] Batch 95 - Loss: 1.551749
[2025-05-11 16:57:38] === Epoch 41/100 Summary ===
Training Loss: 1.547931
Validation Loss: 2.690265
Learning Rate: 0.00003000
[2025-05-11 16:57:39] Batch 0 - Loss: 1.268946
[2025-05-11 16:57:47] Batch 5 - Loss: 1.462697
[2025-05-11 16:57:54] Batch 10 - Loss: 1.473089
[2025-05-11 16:58:02] Batch 15 - Loss: 1.539328
[2025-05-11 16:58:10] Batch 20 - Loss: 1.585069
[2025-05-11 16:58:17] Batch 25 - Loss: 1.561382
[2025-05-11 16:58:25] Batch 30 - Loss: 1.571055
[2025-05-11 16:58:33] Batch 35 - Loss: 1.572225
[2025-05-11 16:58:40] Batch 40 - Loss: 1.555771
[2025-05-11 16:58:48] Batch 45 - Loss: 1.571437
[2025-05-11 16:58:56] Batch 50 - Loss: 1.552176
[2025-05-11 16:59:03] Batch 55 - Loss: 1.557585
[2025-05-11 16:59:11] Batch 60 - Loss: 1.586044
[2025-05-11 16:59:19] Batch 65 - Loss: 1.588982
[2025-05-11 16:59:26] Batch 70 - Loss: 1.573284
[2025-05-11 16:59:34] Batch 75 - Loss: 1.569881
[2025-05-11 16:59:42] Batch 80 - Loss: 1.569918
[2025-05-11 16:59:49] Batch 85 - Loss: 1.564176
[2025-05-11 16:59:57] Batch 90 - Loss: 1.562137
[2025-05-11 17:00:05] Batch 95 - Loss: 1.565597
[2025-05-11 17:00:16] === Epoch 42/100 Summary ===
Training Loss: 1.557496
Validation Loss: 2.773443
Learning Rate: 0.00003000
[2025-05-11 17:00:17] Batch 0 - Loss: 1.053419
[2025-05-11 17:00:25] Batch 5 - Loss: 1.450015
[2025-05-11 17:00:33] Batch 10 - Loss: 1.417809
[2025-05-11 17:00:40] Batch 15 - Loss: 1.411730
[2025-05-11 17:00:48] Batch 20 - Loss: 1.384167
[2025-05-11 17:00:56] Batch 25 - Loss: 1.379559
[2025-05-11 17:01:03] Batch 30 - Loss: 1.391869
[2025-05-11 17:01:11] Batch 35 - Loss: 1.370766
[2025-05-11 17:01:19] Batch 40 - Loss: 1.381880
[2025-05-11 17:01:26] Batch 45 - Loss: 1.368694
[2025-05-11 17:01:34] Batch 50 - Loss: 1.390735
[2025-05-11 17:01:41] Batch 55 - Loss: 1.414024
[2025-05-11 17:01:49] Batch 60 - Loss: 1.415956
[2025-05-11 17:01:57] Batch 65 - Loss: 1.456950
[2025-05-11 17:02:04] Batch 70 - Loss: 1.451147
[2025-05-11 17:02:12] Batch 75 - Loss: 1.435972
[2025-05-11 17:02:20] Batch 80 - Loss: 1.432638
[2025-05-11 17:02:27] Batch 85 - Loss: 1.428408
[2025-05-11 17:02:35] Batch 90 - Loss: 1.409572
[2025-05-11 17:02:43] Batch 95 - Loss: 1.411856
[2025-05-11 17:02:54] === Epoch 43/100 Summary ===
Training Loss: 1.410959
Validation Loss: 2.711448
Learning Rate: 0.00003000
[2025-05-11 17:02:56] Batch 0 - Loss: 1.480842
[2025-05-11 17:03:03] Batch 5 - Loss: 1.325598
[2025-05-11 17:03:11] Batch 10 - Loss: 1.376942
[2025-05-11 17:03:18] Batch 15 - Loss: 1.407732
[2025-05-11 17:03:26] Batch 20 - Loss: 1.386466
[2025-05-11 17:03:34] Batch 25 - Loss: 1.344470
[2025-05-11 17:03:41] Batch 30 - Loss: 1.385142
[2025-05-11 17:03:49] Batch 35 - Loss: 1.359320
[2025-05-11 17:03:55] === Epoch 44/100 Summary ===
Training Loss: 1.353611
Validation Loss: 2.584885
Learning Rate: 0.00003000
[2025-05-11 17:03:57] Batch 0 - Loss: 2.032112
[2025-05-11 17:04:05] Batch 5 - Loss: 1.651827
[2025-05-11 17:04:13] Batch 10 - Loss: 1.709358
[2025-05-11 17:04:20] Batch 15 - Loss: 1.673768
[2025-05-11 17:04:28] Batch 20 - Loss: 1.668851
[2025-05-11 17:04:35] Batch 25 - Loss: 1.603083
[2025-05-11 17:04:43] Batch 30 - Loss: 1.604101
[2025-05-11 17:04:51] Batch 35 - Loss: 1.609353
[2025-05-11 17:04:58] Batch 40 - Loss: 1.596216
[2025-05-11 17:05:06] Batch 45 - Loss: 1.582557
[2025-05-11 17:05:14] Batch 50 - Loss: 1.588964
[2025-05-11 17:05:21] Batch 55 - Loss: 1.588123
[2025-05-11 17:05:29] Batch 60 - Loss: 1.570916
[2025-05-11 17:05:37] Batch 65 - Loss: 1.554194
[2025-05-11 17:05:44] Batch 70 - Loss: 1.555363
[2025-05-11 17:05:52] Batch 75 - Loss: 1.544843
[2025-05-11 17:05:59] Batch 80 - Loss: 1.519142
[2025-05-11 17:06:07] Batch 85 - Loss: 1.505798
[2025-05-11 17:06:15] Batch 90 - Loss: 1.487284
[2025-05-11 17:06:22] Batch 95 - Loss: 1.482170
[2025-05-11 17:06:34] === Epoch 45/100 Summary ===
Training Loss: 1.477502
Validation Loss: 2.934555
Learning Rate: 0.00003000
[2025-05-11 17:06:34] Generating detection visualizations for epoch 45...
[2025-05-11 17:06:34] Saved visualization to trained_models/custom_detector\visualizations\epoch_45\sample_1.png
[2025-05-11 17:06:34] Saved visualization to trained_models/custom_detector\visualizations\epoch_45\sample_2.png
[2025-05-11 17:06:35] Saved visualization to trained_models/custom_detector\visualizations\epoch_45\sample_3.png
[2025-05-11 17:06:36] Batch 0 - Loss: 0.961002
[2025-05-11 17:06:44] Batch 5 - Loss: 1.098537
[2025-05-11 17:06:52] Batch 10 - Loss: 1.141449
[2025-05-11 17:06:59] Batch 15 - Loss: 1.196426
[2025-05-11 17:07:07] Batch 20 - Loss: 1.212791
[2025-05-11 17:07:15] Batch 25 - Loss: 1.201389
[2025-05-11 17:07:22] Batch 30 - Loss: 1.254980
[2025-05-11 17:07:30] Batch 35 - Loss: 1.294687
[2025-05-11 17:07:38] Batch 40 - Loss: 1.304588
[2025-05-11 17:07:45] Batch 45 - Loss: 1.316770
[2025-05-11 17:07:53] Batch 50 - Loss: 1.377655
[2025-05-11 17:08:01] Batch 55 - Loss: 1.378701
[2025-05-11 17:08:08] Batch 60 - Loss: 1.377990
[2025-05-11 17:08:16] Batch 65 - Loss: 1.392704
[2025-05-11 17:08:23] Batch 70 - Loss: 1.404640
[2025-05-11 17:08:31] Batch 75 - Loss: 1.396579
[2025-05-11 17:08:39] Batch 80 - Loss: 1.386184
[2025-05-11 17:08:46] Batch 85 - Loss: 1.395037
[2025-05-11 17:08:54] Batch 90 - Loss: 1.386084
[2025-05-11 17:09:02] Batch 95 - Loss: 1.380955
[2025-05-11 17:09:13] === Epoch 46/100 Summary ===
Training Loss: 1.385335
Validation Loss: 2.677644
Learning Rate: 0.00003000
[2025-05-11 17:09:14] Batch 0 - Loss: 1.236675
[2025-05-11 17:09:22] Batch 5 - Loss: 1.465754
[2025-05-11 17:09:30] Batch 10 - Loss: 1.430995
[2025-05-11 17:09:37] Batch 15 - Loss: 1.431314
[2025-05-11 17:09:45] Batch 20 - Loss: 1.423066
[2025-05-11 17:09:53] Batch 25 - Loss: 1.430563
[2025-05-11 17:10:00] Batch 30 - Loss: 1.423993
[2025-05-11 17:10:08] Batch 35 - Loss: 1.439306
[2025-05-11 17:10:16] Batch 40 - Loss: 1.427407
[2025-05-11 17:10:23] Batch 45 - Loss: 1.406753
[2025-05-11 17:10:31] Batch 50 - Loss: 1.378445
[2025-05-11 17:10:39] Batch 55 - Loss: 1.363568
[2025-05-11 17:10:46] Batch 60 - Loss: 1.367518
[2025-05-11 17:10:54] Batch 65 - Loss: 1.338038
[2025-05-11 17:11:02] Batch 70 - Loss: 1.343024
[2025-05-11 17:11:09] Batch 75 - Loss: 1.356130
[2025-05-11 17:11:17] Batch 80 - Loss: 1.349202
[2025-05-11 17:11:25] Batch 85 - Loss: 1.350044
[2025-05-11 17:11:32] Batch 90 - Loss: 1.344185
[2025-05-11 17:11:40] Batch 95 - Loss: 1.327202
[2025-05-11 17:11:51] === Epoch 47/100 Summary ===
Training Loss: 1.323660
Validation Loss: 2.617084
Learning Rate: 0.00003000
[2025-05-11 17:11:53] Batch 0 - Loss: 1.275118
[2025-05-11 17:12:00] Batch 5 - Loss: 1.191599
[2025-05-11 17:12:08] Batch 10 - Loss: 1.262435
[2025-05-11 17:12:15] Batch 15 - Loss: 1.173631
[2025-05-11 17:12:23] Batch 20 - Loss: 1.154147
[2025-05-11 17:12:31] Batch 25 - Loss: 1.185066
[2025-05-11 17:12:38] Batch 30 - Loss: 1.181879
[2025-05-11 17:12:46] Batch 35 - Loss: 1.171811
[2025-05-11 17:12:52] === Epoch 48/100 Summary ===
Training Loss: 1.174068
Validation Loss: 2.496915
Learning Rate: 0.00003000
[2025-05-11 17:12:54] Batch 0 - Loss: 1.190442
[2025-05-11 17:13:02] Batch 5 - Loss: 1.325670
[2025-05-11 17:13:09] Batch 10 - Loss: 1.362856
[2025-05-11 17:13:17] Batch 15 - Loss: 1.349849
[2025-05-11 17:13:24] Batch 20 - Loss: 1.439151
[2025-05-11 17:13:32] Batch 25 - Loss: 1.400566
[2025-05-11 17:13:40] Batch 30 - Loss: 1.414558
[2025-05-11 17:13:47] Batch 35 - Loss: 1.408424
[2025-05-11 17:13:55] Batch 40 - Loss: 1.408473
[2025-05-11 17:14:03] Batch 45 - Loss: 1.412602
[2025-05-11 17:14:10] Batch 50 - Loss: 1.416430
[2025-05-11 17:14:18] Batch 55 - Loss: 1.404870
[2025-05-11 17:14:26] Batch 60 - Loss: 1.402696
[2025-05-11 17:14:33] Batch 65 - Loss: 1.387734
[2025-05-11 17:14:41] Batch 70 - Loss: 1.385723
[2025-05-11 17:14:49] Batch 75 - Loss: 1.376016
[2025-05-11 17:14:56] Batch 80 - Loss: 1.360344
[2025-05-11 17:15:04] Batch 85 - Loss: 1.357944
[2025-05-11 17:15:12] Batch 90 - Loss: 1.346974
[2025-05-11 17:15:19] Batch 95 - Loss: 1.342712
[2025-05-11 17:15:30] === Epoch 49/100 Summary ===
Training Loss: 1.340622
Validation Loss: 2.635125
Learning Rate: 0.00003000
[2025-05-11 17:15:32] Batch 0 - Loss: 1.421596
[2025-05-11 17:15:40] Batch 5 - Loss: 1.484003
[2025-05-11 17:15:47] Batch 10 - Loss: 1.459935
[2025-05-11 17:15:55] Batch 15 - Loss: 1.412157
[2025-05-11 17:16:03] Batch 20 - Loss: 1.334342
[2025-05-11 17:16:10] Batch 25 - Loss: 1.364996
[2025-05-11 17:16:18] Batch 30 - Loss: 1.331619
[2025-05-11 17:16:25] Batch 35 - Loss: 1.304526
[2025-05-11 17:16:33] Batch 40 - Loss: 1.308826
[2025-05-11 17:16:41] Batch 45 - Loss: 1.306332
[2025-05-11 17:16:48] Batch 50 - Loss: 1.317117
[2025-05-11 17:16:56] Batch 55 - Loss: 1.341596
[2025-05-11 17:17:03] Batch 60 - Loss: 1.333844
[2025-05-11 17:17:11] Batch 65 - Loss: 1.344011
[2025-05-11 17:17:19] Batch 70 - Loss: 1.325652
[2025-05-11 17:17:26] Batch 75 - Loss: 1.317214
[2025-05-11 17:17:34] Batch 80 - Loss: 1.319303
[2025-05-11 17:17:42] Batch 85 - Loss: 1.306824
[2025-05-11 17:17:49] Batch 90 - Loss: 1.321767
[2025-05-11 17:17:57] Batch 95 - Loss: 1.312760
[2025-05-11 17:18:08] === Epoch 50/100 Summary ===
Training Loss: 1.304729
Validation Loss: 2.633917
Learning Rate: 0.00003000
[2025-05-11 17:18:08] Generating detection visualizations for epoch 50...
[2025-05-11 17:18:09] Saved visualization to trained_models/custom_detector\visualizations\epoch_50\sample_1.png
[2025-05-11 17:18:09] Saved visualization to trained_models/custom_detector\visualizations\epoch_50\sample_2.png
[2025-05-11 17:18:09] Saved visualization to trained_models/custom_detector\visualizations\epoch_50\sample_3.png
[2025-05-11 17:18:11] Batch 0 - Loss: 1.142972
[2025-05-11 17:18:18] Batch 5 - Loss: 1.177309
[2025-05-11 17:18:26] Batch 10 - Loss: 1.156793
[2025-05-11 17:18:34] Batch 15 - Loss: 1.227025
[2025-05-11 17:18:41] Batch 20 - Loss: 1.176236
[2025-05-11 17:18:49] Batch 25 - Loss: 1.202323
[2025-05-11 17:18:56] Batch 30 - Loss: 1.205225
[2025-05-11 17:19:04] Batch 35 - Loss: 1.254297
[2025-05-11 17:19:12] Batch 40 - Loss: 1.264037
[2025-05-11 17:19:19] Batch 45 - Loss: 1.288193
[2025-05-11 17:19:27] Batch 50 - Loss: 1.282449
[2025-05-11 17:19:35] Batch 55 - Loss: 1.267725
[2025-05-11 17:19:42] Batch 60 - Loss: 1.281165
[2025-05-11 17:19:50] Batch 65 - Loss: 1.287133
[2025-05-11 17:19:57] Batch 70 - Loss: 1.264703
[2025-05-11 17:20:05] Batch 75 - Loss: 1.246604
[2025-05-11 17:20:13] Batch 80 - Loss: 1.251119
[2025-05-11 17:20:20] Batch 85 - Loss: 1.250627
[2025-05-11 17:20:28] Batch 90 - Loss: 1.241571
[2025-05-11 17:20:36] Batch 95 - Loss: 1.243053
[2025-05-11 17:20:47] === Epoch 51/100 Summary ===
Training Loss: 1.248524
Validation Loss: 2.801453
Learning Rate: 0.00003000
[2025-05-11 17:20:48] Batch 0 - Loss: 1.467928
[2025-05-11 17:20:56] Batch 5 - Loss: 0.992903
[2025-05-11 17:21:04] Batch 10 - Loss: 1.130065
[2025-05-11 17:21:11] Batch 15 - Loss: 1.127744
[2025-05-11 17:21:19] Batch 20 - Loss: 1.077680
[2025-05-11 17:21:26] Batch 25 - Loss: 1.088319
[2025-05-11 17:21:34] Batch 30 - Loss: 1.106547
[2025-05-11 17:21:42] Batch 35 - Loss: 1.095560
[2025-05-11 17:21:48] === Epoch 52/100 Summary ===
Training Loss: 1.090572
Validation Loss: 2.703657
Learning Rate: 0.00003000
[2025-05-11 17:21:50] Batch 0 - Loss: 1.326218
[2025-05-11 17:21:57] Batch 5 - Loss: 1.204670
[2025-05-11 17:22:05] Batch 10 - Loss: 1.302302
[2025-05-11 17:22:13] Batch 15 - Loss: 1.318587
[2025-05-11 17:22:20] Batch 20 - Loss: 1.297875
[2025-05-11 17:22:28] Batch 25 - Loss: 1.293751
[2025-05-11 17:22:36] Batch 30 - Loss: 1.344819
[2025-05-11 17:22:43] Batch 35 - Loss: 1.341893
[2025-05-11 17:22:51] Batch 40 - Loss: 1.372429
[2025-05-11 17:22:59] Batch 45 - Loss: 1.395911
[2025-05-11 17:23:06] Batch 50 - Loss: 1.370451
[2025-05-11 17:23:14] Batch 55 - Loss: 1.390266
[2025-05-11 17:23:21] Batch 60 - Loss: 1.392120
[2025-05-11 17:23:29] Batch 65 - Loss: 1.377755
[2025-05-11 17:23:37] Batch 70 - Loss: 1.366716
[2025-05-11 17:23:44] Batch 75 - Loss: 1.353986
[2025-05-11 17:23:52] Batch 80 - Loss: 1.334692
[2025-05-11 17:24:00] Batch 85 - Loss: 1.326485
[2025-05-11 17:24:07] Batch 90 - Loss: 1.317086
[2025-05-11 17:24:15] Batch 95 - Loss: 1.310308
[2025-05-11 17:24:26] === Epoch 53/100 Summary ===
Training Loss: 1.305845
Validation Loss: 2.483295
Learning Rate: 0.00003000
[2025-05-11 17:24:28] Batch 0 - Loss: 0.852049
[2025-05-11 17:24:36] Batch 5 - Loss: 1.088466
[2025-05-11 17:24:43] Batch 10 - Loss: 1.131976
[2025-05-11 17:24:51] Batch 15 - Loss: 1.102209
[2025-05-11 17:24:59] Batch 20 - Loss: 1.071881
[2025-05-11 17:25:06] Batch 25 - Loss: 1.139660
[2025-05-11 17:25:14] Batch 30 - Loss: 1.179553
[2025-05-11 17:25:21] Batch 35 - Loss: 1.182893
[2025-05-11 17:25:29] Batch 40 - Loss: 1.235967
[2025-05-11 17:25:37] Batch 45 - Loss: 1.243563
[2025-05-11 17:25:44] Batch 50 - Loss: 1.230743
[2025-05-11 17:25:52] Batch 55 - Loss: 1.212313
[2025-05-11 17:26:00] Batch 60 - Loss: 1.225629
[2025-05-11 17:26:07] Batch 65 - Loss: 1.251968
[2025-05-11 17:26:15] Batch 70 - Loss: 1.248819
[2025-05-11 17:26:23] Batch 75 - Loss: 1.248777
[2025-05-11 17:26:30] Batch 80 - Loss: 1.237011
[2025-05-11 17:26:38] Batch 85 - Loss: 1.224589
[2025-05-11 17:26:46] Batch 90 - Loss: 1.208904
[2025-05-11 17:26:53] Batch 95 - Loss: 1.204319
[2025-05-11 17:27:05] === Epoch 54/100 Summary ===
Training Loss: 1.197859
Validation Loss: 2.463993
Learning Rate: 0.00003000
[2025-05-11 17:27:06] Batch 0 - Loss: 0.846920
[2025-05-11 17:27:14] Batch 5 - Loss: 1.058657
[2025-05-11 17:27:22] Batch 10 - Loss: 1.131132
[2025-05-11 17:27:29] Batch 15 - Loss: 1.166885
[2025-05-11 17:27:37] Batch 20 - Loss: 1.158462
[2025-05-11 17:27:45] Batch 25 - Loss: 1.139895
[2025-05-11 17:27:52] Batch 30 - Loss: 1.117929
[2025-05-11 17:28:00] Batch 35 - Loss: 1.115088
[2025-05-11 17:28:08] Batch 40 - Loss: 1.129102
[2025-05-11 17:28:15] Batch 45 - Loss: 1.139062
[2025-05-11 17:28:23] Batch 50 - Loss: 1.144592
[2025-05-11 17:28:30] Batch 55 - Loss: 1.129543
[2025-05-11 17:28:38] Batch 60 - Loss: 1.141781
[2025-05-11 17:28:46] Batch 65 - Loss: 1.141606
[2025-05-11 17:28:53] Batch 70 - Loss: 1.132069
[2025-05-11 17:29:01] Batch 75 - Loss: 1.115848
[2025-05-11 17:29:09] Batch 80 - Loss: 1.117048
[2025-05-11 17:29:16] Batch 85 - Loss: 1.112688
[2025-05-11 17:29:24] Batch 90 - Loss: 1.118247
[2025-05-11 17:29:32] Batch 95 - Loss: 1.122715
[2025-05-11 17:29:43] === Epoch 55/100 Summary ===
Training Loss: 1.128833
Validation Loss: 2.401152
Learning Rate: 0.00003000
[2025-05-11 17:29:43] Generating detection visualizations for epoch 55...
[2025-05-11 17:29:44] Saved visualization to trained_models/custom_detector\visualizations\epoch_55\sample_1.png
[2025-05-11 17:29:44] Saved visualization to trained_models/custom_detector\visualizations\epoch_55\sample_2.png
[2025-05-11 17:29:45] Saved visualization to trained_models/custom_detector\visualizations\epoch_55\sample_3.png
[2025-05-11 17:29:46] Batch 0 - Loss: 1.796776
[2025-05-11 17:29:54] Batch 5 - Loss: 1.250763
[2025-05-11 17:30:02] Batch 10 - Loss: 1.201353
[2025-05-11 17:30:10] Batch 15 - Loss: 1.105073
[2025-05-11 17:30:17] Batch 20 - Loss: 1.169679
[2025-05-11 17:30:25] Batch 25 - Loss: 1.132044
[2025-05-11 17:30:33] Batch 30 - Loss: 1.144049
[2025-05-11 17:30:40] Batch 35 - Loss: 1.124308
[2025-05-11 17:30:47] === Epoch 56/100 Summary ===
Training Loss: 1.130472
Validation Loss: 2.355415
Learning Rate: 0.00003000
[2025-05-11 17:30:49] Batch 0 - Loss: 1.757347
[2025-05-11 17:30:56] Batch 5 - Loss: 1.236201
[2025-05-11 17:31:04] Batch 10 - Loss: 1.276700
[2025-05-11 17:31:12] Batch 15 - Loss: 1.226723
[2025-05-11 17:31:20] Batch 20 - Loss: 1.259763
[2025-05-11 17:31:27] Batch 25 - Loss: 1.240132
[2025-05-11 17:31:35] Batch 30 - Loss: 1.239710
[2025-05-11 17:31:43] Batch 35 - Loss: 1.267861
[2025-05-11 17:31:50] Batch 40 - Loss: 1.258946
[2025-05-11 17:31:58] Batch 45 - Loss: 1.246733
[2025-05-11 17:32:06] Batch 50 - Loss: 1.231464
[2025-05-11 17:32:14] Batch 55 - Loss: 1.236148
[2025-05-11 17:32:21] Batch 60 - Loss: 1.225133
[2025-05-11 17:32:29] Batch 65 - Loss: 1.214863
[2025-05-11 17:32:37] Batch 70 - Loss: 1.223176
[2025-05-11 17:32:45] Batch 75 - Loss: 1.224180
[2025-05-11 17:32:52] Batch 80 - Loss: 1.215086
[2025-05-11 17:33:00] Batch 85 - Loss: 1.209520
[2025-05-11 17:33:08] Batch 90 - Loss: 1.199593
[2025-05-11 17:33:15] Batch 95 - Loss: 1.207018
[2025-05-11 17:33:27] === Epoch 57/100 Summary ===
Training Loss: 1.199037
Validation Loss: 2.351751
Learning Rate: 0.00003000
[2025-05-11 17:33:29] Batch 0 - Loss: 0.778705
[2025-05-11 17:33:37] Batch 5 - Loss: 1.069896
[2025-05-11 17:33:44] Batch 10 - Loss: 1.105006
[2025-05-11 17:33:52] Batch 15 - Loss: 1.147791
[2025-05-11 17:34:00] Batch 20 - Loss: 1.184419
[2025-05-11 17:34:07] Batch 25 - Loss: 1.160089
[2025-05-11 17:34:15] Batch 30 - Loss: 1.181370
[2025-05-11 17:34:23] Batch 35 - Loss: 1.194397
[2025-05-11 17:34:31] Batch 40 - Loss: 1.208541
[2025-05-11 17:34:38] Batch 45 - Loss: 1.222963
[2025-05-11 17:34:46] Batch 50 - Loss: 1.226209
[2025-05-11 17:34:54] Batch 55 - Loss: 1.207199
[2025-05-11 17:35:02] Batch 60 - Loss: 1.200924
[2025-05-11 17:35:09] Batch 65 - Loss: 1.208975
[2025-05-11 17:35:17] Batch 70 - Loss: 1.203780
[2025-05-11 17:35:25] Batch 75 - Loss: 1.194822
[2025-05-11 17:35:32] Batch 80 - Loss: 1.185828
[2025-05-11 17:35:40] Batch 85 - Loss: 1.183586
[2025-05-11 17:35:48] Batch 90 - Loss: 1.183311
[2025-05-11 17:35:56] Batch 95 - Loss: 1.176996
[2025-05-11 17:36:07] === Epoch 58/100 Summary ===
Training Loss: 1.168068
Validation Loss: 2.380791
Learning Rate: 0.00003000
[2025-05-11 17:36:09] Batch 0 - Loss: 1.517691
[2025-05-11 17:36:16] Batch 5 - Loss: 1.062531
[2025-05-11 17:36:24] Batch 10 - Loss: 1.042806
[2025-05-11 17:36:32] Batch 15 - Loss: 1.090408
[2025-05-11 17:36:39] Batch 20 - Loss: 1.041566
[2025-05-11 17:36:47] Batch 25 - Loss: 1.058900
[2025-05-11 17:36:55] Batch 30 - Loss: 1.098474
[2025-05-11 17:37:03] Batch 35 - Loss: 1.111378
[2025-05-11 17:37:11] Batch 40 - Loss: 1.110071
[2025-05-11 17:37:18] Batch 45 - Loss: 1.135527
[2025-05-11 17:37:26] Batch 50 - Loss: 1.130867
[2025-05-11 17:37:34] Batch 55 - Loss: 1.114809
[2025-05-11 17:37:41] Batch 60 - Loss: 1.103523
[2025-05-11 17:37:49] Batch 65 - Loss: 1.107851
[2025-05-11 17:37:57] Batch 70 - Loss: 1.113688
[2025-05-11 17:38:05] Batch 75 - Loss: 1.114724
[2025-05-11 17:38:12] Batch 80 - Loss: 1.120030
[2025-05-11 17:38:20] Batch 85 - Loss: 1.116225
[2025-05-11 17:38:28] Batch 90 - Loss: 1.101120
[2025-05-11 17:38:36] Batch 95 - Loss: 1.084453
[2025-05-11 17:38:47] === Epoch 59/100 Summary ===
Training Loss: 1.075266
Validation Loss: 2.304491
Learning Rate: 0.00003000
[2025-05-11 17:38:49] Batch 0 - Loss: 1.174618
[2025-05-11 17:38:56] Batch 5 - Loss: 0.862261
[2025-05-11 17:39:04] Batch 10 - Loss: 0.790859
[2025-05-11 17:39:12] Batch 15 - Loss: 0.881439
[2025-05-11 17:39:20] Batch 20 - Loss: 0.969500
[2025-05-11 17:39:27] Batch 25 - Loss: 0.952038
[2025-05-11 17:39:35] Batch 30 - Loss: 0.958021
[2025-05-11 17:39:43] Batch 35 - Loss: 0.943187
[2025-05-11 17:39:49] === Epoch 60/100 Summary ===
Training Loss: 0.944415
Validation Loss: 2.428705
Learning Rate: 0.00003000
[2025-05-11 17:39:49] Generating detection visualizations for epoch 60...
[2025-05-11 17:39:50] Saved visualization to trained_models/custom_detector\visualizations\epoch_60\sample_1.png
[2025-05-11 17:39:50] Saved visualization to trained_models/custom_detector\visualizations\epoch_60\sample_2.png
[2025-05-11 17:39:50] Saved visualization to trained_models/custom_detector\visualizations\epoch_60\sample_3.png
[2025-05-11 17:39:52] Batch 0 - Loss: 0.842954
[2025-05-11 17:40:00] Batch 5 - Loss: 1.257192
[2025-05-11 17:40:08] Batch 10 - Loss: 1.142228
[2025-05-11 17:40:16] Batch 15 - Loss: 1.164538
[2025-05-11 17:40:23] Batch 20 - Loss: 1.175191
[2025-05-11 17:40:31] Batch 25 - Loss: 1.165115
[2025-05-11 17:40:39] Batch 30 - Loss: 1.188145
[2025-05-11 17:40:46] Batch 35 - Loss: 1.167323
[2025-05-11 17:40:54] Batch 40 - Loss: 1.159779
[2025-05-11 17:41:02] Batch 45 - Loss: 1.150977
[2025-05-11 17:41:09] Batch 50 - Loss: 1.144740
[2025-05-11 17:41:17] Batch 55 - Loss: 1.158430
[2025-05-11 17:41:25] Batch 60 - Loss: 1.148972
[2025-05-11 17:41:32] Batch 65 - Loss: 1.140336
[2025-05-11 17:41:40] Batch 70 - Loss: 1.128864
[2025-05-11 17:41:48] Batch 75 - Loss: 1.115094
[2025-05-11 17:41:56] Batch 80 - Loss: 1.112294
[2025-05-11 17:42:03] Batch 85 - Loss: 1.100816
[2025-05-11 17:42:11] Batch 90 - Loss: 1.101156
[2025-05-11 17:42:19] Batch 95 - Loss: 1.098601
[2025-05-11 17:42:30] === Epoch 61/100 Summary ===
Training Loss: 1.098088
Validation Loss: 2.506010
Learning Rate: 0.00003000
[2025-05-11 17:42:32] Batch 0 - Loss: 1.164387
[2025-05-11 17:42:39] Batch 5 - Loss: 1.098153
[2025-05-11 17:42:47] Batch 10 - Loss: 1.021853
[2025-05-11 17:42:55] Batch 15 - Loss: 1.041662
[2025-05-11 17:43:03] Batch 20 - Loss: 1.034461
[2025-05-11 17:43:10] Batch 25 - Loss: 1.042140
[2025-05-11 17:43:18] Batch 30 - Loss: 1.039328
[2025-05-11 17:43:26] Batch 35 - Loss: 1.001130
[2025-05-11 17:43:33] Batch 40 - Loss: 1.004003
[2025-05-11 17:43:41] Batch 45 - Loss: 1.010901
[2025-05-11 17:43:49] Batch 50 - Loss: 1.019491
[2025-05-11 17:43:56] Batch 55 - Loss: 1.023123
[2025-05-11 17:44:04] Batch 60 - Loss: 1.024044
[2025-05-11 17:44:12] Batch 65 - Loss: 1.030917
[2025-05-11 17:44:20] Batch 70 - Loss: 1.025712
[2025-05-11 17:44:27] Batch 75 - Loss: 1.025246
[2025-05-11 17:44:35] Batch 80 - Loss: 1.016166
[2025-05-11 17:44:43] Batch 85 - Loss: 1.022619
[2025-05-11 17:44:50] Batch 90 - Loss: 1.018420
[2025-05-11 17:44:58] Batch 95 - Loss: 1.009722
[2025-05-11 17:45:09] === Epoch 62/100 Summary ===
Training Loss: 1.015812
Validation Loss: 2.375110
Learning Rate: 0.00003000
[2025-05-11 17:45:11] Batch 0 - Loss: 1.078343
[2025-05-11 17:45:19] Batch 5 - Loss: 0.977315
[2025-05-11 17:45:26] Batch 10 - Loss: 1.018816
[2025-05-11 17:45:34] Batch 15 - Loss: 1.061184
[2025-05-11 17:45:42] Batch 20 - Loss: 1.076990
[2025-05-11 17:45:49] Batch 25 - Loss: 1.093604
[2025-05-11 17:45:57] Batch 30 - Loss: 1.102872
[2025-05-11 17:46:05] Batch 35 - Loss: 1.141042
[2025-05-11 17:46:12] Batch 40 - Loss: 1.127027
[2025-05-11 17:46:20] Batch 45 - Loss: 1.148012
[2025-05-11 17:46:28] Batch 50 - Loss: 1.143386
[2025-05-11 17:46:36] Batch 55 - Loss: 1.125208
[2025-05-11 17:46:43] Batch 60 - Loss: 1.121942
[2025-05-11 17:46:51] Batch 65 - Loss: 1.099300
[2025-05-11 17:46:59] Batch 70 - Loss: 1.096566
[2025-05-11 17:47:06] Batch 75 - Loss: 1.092577
[2025-05-11 17:47:14] Batch 80 - Loss: 1.078098
[2025-05-11 17:47:22] Batch 85 - Loss: 1.077097
[2025-05-11 17:47:29] Batch 90 - Loss: 1.062418
[2025-05-11 17:47:37] Batch 95 - Loss: 1.054216
[2025-05-11 17:47:49] === Epoch 63/100 Summary ===
Training Loss: 1.042742
Validation Loss: 2.342399
Learning Rate: 0.00003000
[2025-05-11 17:47:50] Batch 0 - Loss: 1.138534
[2025-05-11 17:47:58] Batch 5 - Loss: 0.775038
[2025-05-11 17:48:05] Batch 10 - Loss: 0.922606
[2025-05-11 17:48:13] Batch 15 - Loss: 0.894693
[2025-05-11 17:48:21] Batch 20 - Loss: 0.952900
[2025-05-11 17:48:29] Batch 25 - Loss: 0.929350
[2025-05-11 17:48:36] Batch 30 - Loss: 0.945350
[2025-05-11 17:48:44] Batch 35 - Loss: 0.936476
[2025-05-11 17:48:50] === Epoch 64/100 Summary ===
Training Loss: 0.943001
Validation Loss: 2.324269
Learning Rate: 0.00000600
[2025-05-11 17:48:52] Batch 0 - Loss: 0.970694
[2025-05-11 17:49:00] Batch 5 - Loss: 0.990147
[2025-05-11 17:49:08] Batch 10 - Loss: 1.016361
[2025-05-11 17:49:15] Batch 15 - Loss: 1.088465
[2025-05-11 17:49:23] Batch 20 - Loss: 1.113149
[2025-05-11 17:49:31] Batch 25 - Loss: 1.102848
[2025-05-11 17:49:38] Batch 30 - Loss: 1.081508
[2025-05-11 17:49:46] Batch 35 - Loss: 1.085560
[2025-05-11 17:49:54] Batch 40 - Loss: 1.110278
[2025-05-11 17:50:02] Batch 45 - Loss: 1.095407
[2025-05-11 17:50:09] Batch 50 - Loss: 1.092107
[2025-05-11 17:50:17] Batch 55 - Loss: 1.085717
[2025-05-11 17:50:25] Batch 60 - Loss: 1.099240
[2025-05-11 17:50:32] Batch 65 - Loss: 1.090139
[2025-05-11 17:50:40] Batch 70 - Loss: 1.080692
[2025-05-11 17:50:48] Batch 75 - Loss: 1.063905
[2025-05-11 17:50:55] Batch 80 - Loss: 1.045478
[2025-05-11 17:51:03] Batch 85 - Loss: 1.040289
[2025-05-11 17:51:11] Batch 90 - Loss: 1.038522
[2025-05-11 17:51:19] Batch 95 - Loss: 1.019497
[2025-05-11 17:51:30] === Epoch 65/100 Summary ===
Training Loss: 1.022702
Validation Loss: 2.234817
Learning Rate: 0.00000600
[2025-05-11 17:51:30] Generating detection visualizations for epoch 65...
[2025-05-11 17:51:31] Saved visualization to trained_models/custom_detector\visualizations\epoch_65\sample_1.png
[2025-05-11 17:51:31] Saved visualization to trained_models/custom_detector\visualizations\epoch_65\sample_2.png
[2025-05-11 17:51:31] Saved visualization to trained_models/custom_detector\visualizations\epoch_65\sample_3.png
[2025-05-11 17:51:33] Batch 0 - Loss: 0.818394
[2025-05-11 17:51:41] Batch 5 - Loss: 0.788436
[2025-05-11 17:51:48] Batch 10 - Loss: 0.896883
[2025-05-11 17:51:56] Batch 15 - Loss: 0.884010
[2025-05-11 17:52:04] Batch 20 - Loss: 0.900607
[2025-05-11 17:52:11] Batch 25 - Loss: 0.936655
[2025-05-11 17:52:19] Batch 30 - Loss: 0.965483
[2025-05-11 17:52:27] Batch 35 - Loss: 0.972872
[2025-05-11 17:52:35] Batch 40 - Loss: 0.972430
[2025-05-11 17:52:42] Batch 45 - Loss: 0.998985
[2025-05-11 17:52:50] Batch 50 - Loss: 1.013645
[2025-05-11 17:52:58] Batch 55 - Loss: 1.010644
[2025-05-11 17:53:06] Batch 60 - Loss: 1.008818
[2025-05-11 17:53:13] Batch 65 - Loss: 1.023800
[2025-05-11 17:53:21] Batch 70 - Loss: 1.016725
[2025-05-11 17:53:29] Batch 75 - Loss: 1.002582
[2025-05-11 17:53:36] Batch 80 - Loss: 0.995191
[2025-05-11 17:53:44] Batch 85 - Loss: 0.978505
[2025-05-11 17:53:52] Batch 90 - Loss: 0.969144
[2025-05-11 17:54:00] Batch 95 - Loss: 0.969616
[2025-05-11 17:54:11] === Epoch 66/100 Summary ===
Training Loss: 0.966886
Validation Loss: 2.235528
Learning Rate: 0.00000600
[2025-05-11 17:54:13] Batch 0 - Loss: 0.938354
[2025-05-11 17:54:20] Batch 5 - Loss: 0.979410
[2025-05-11 17:54:28] Batch 10 - Loss: 0.867417
[2025-05-11 17:54:36] Batch 15 - Loss: 0.867331
[2025-05-11 17:54:43] Batch 20 - Loss: 0.877452
[2025-05-11 17:54:51] Batch 25 - Loss: 0.903531
[2025-05-11 17:54:59] Batch 30 - Loss: 0.900811
[2025-05-11 17:55:07] Batch 35 - Loss: 0.872793
[2025-05-11 17:55:14] Batch 40 - Loss: 0.890143
[2025-05-11 17:55:22] Batch 45 - Loss: 0.884850
[2025-05-11 17:55:30] Batch 50 - Loss: 0.894376
[2025-05-11 17:55:37] Batch 55 - Loss: 0.891345
[2025-05-11 17:55:45] Batch 60 - Loss: 0.884370
[2025-05-11 17:55:53] Batch 65 - Loss: 0.873568
[2025-05-11 17:56:00] Batch 70 - Loss: 0.869798
[2025-05-11 17:56:08] Batch 75 - Loss: 0.881610
[2025-05-11 17:56:16] Batch 80 - Loss: 0.876904
[2025-05-11 17:56:24] Batch 85 - Loss: 0.871271
[2025-05-11 17:56:31] Batch 90 - Loss: 0.862961
[2025-05-11 17:56:39] Batch 95 - Loss: 0.858306
[2025-05-11 17:56:50] === Epoch 67/100 Summary ===
Training Loss: 0.865049
Validation Loss: 2.252414
Learning Rate: 0.00000600
[2025-05-11 17:56:52] Batch 0 - Loss: 1.408344
[2025-05-11 17:57:00] Batch 5 - Loss: 0.820499
[2025-05-11 17:57:07] Batch 10 - Loss: 0.863899
[2025-05-11 17:57:15] Batch 15 - Loss: 0.798958
[2025-05-11 17:57:23] Batch 20 - Loss: 0.883293
[2025-05-11 17:57:30] Batch 25 - Loss: 0.879776
[2025-05-11 17:57:38] Batch 30 - Loss: 0.846350
[2025-05-11 17:57:46] Batch 35 - Loss: 0.850058
[2025-05-11 17:57:52] === Epoch 68/100 Summary ===
Training Loss: 0.847803
Validation Loss: 2.231874
Learning Rate: 0.00000600
[2025-05-11 17:57:54] Batch 0 - Loss: 1.060518
[2025-05-11 17:58:02] Batch 5 - Loss: 1.179953
[2025-05-11 17:58:10] Batch 10 - Loss: 1.157920
[2025-05-11 17:58:17] Batch 15 - Loss: 1.082745
[2025-05-11 17:58:25] Batch 20 - Loss: 1.041639
[2025-05-11 17:58:33] Batch 25 - Loss: 1.069509
[2025-05-11 17:58:40] Batch 30 - Loss: 1.087331
[2025-05-11 17:58:48] Batch 35 - Loss: 1.082618
[2025-05-11 17:58:56] Batch 40 - Loss: 1.057616
[2025-05-11 17:59:04] Batch 45 - Loss: 1.039470
[2025-05-11 17:59:11] Batch 50 - Loss: 1.029473
[2025-05-11 17:59:19] Batch 55 - Loss: 1.025530
[2025-05-11 17:59:27] Batch 60 - Loss: 1.022078
[2025-05-11 17:59:34] Batch 65 - Loss: 1.026600
[2025-05-11 17:59:42] Batch 70 - Loss: 1.018164
[2025-05-11 17:59:50] Batch 75 - Loss: 1.018288
[2025-05-11 17:59:58] Batch 80 - Loss: 1.009560
[2025-05-11 18:00:05] Batch 85 - Loss: 1.001090
[2025-05-11 18:00:13] Batch 90 - Loss: 0.986686
[2025-05-11 18:00:21] Batch 95 - Loss: 0.972214
[2025-05-11 18:00:32] === Epoch 69/100 Summary ===
Training Loss: 0.970194
Validation Loss: 2.199891
Learning Rate: 0.00000600
[2025-05-11 18:00:34] Batch 0 - Loss: 1.111399
[2025-05-11 18:00:42] Batch 5 - Loss: 0.726468
[2025-05-11 18:00:49] Batch 10 - Loss: 0.813856
[2025-05-11 18:00:57] Batch 15 - Loss: 0.943047
[2025-05-11 18:01:05] Batch 20 - Loss: 0.950315
[2025-05-11 18:01:13] Batch 25 - Loss: 0.930879
[2025-05-11 18:01:20] Batch 30 - Loss: 0.952565
[2025-05-11 18:01:28] Batch 35 - Loss: 0.927835
[2025-05-11 18:01:36] Batch 40 - Loss: 0.925483
[2025-05-11 18:01:43] Batch 45 - Loss: 0.936948
[2025-05-11 18:01:51] Batch 50 - Loss: 0.928763
[2025-05-11 18:01:59] Batch 55 - Loss: 0.931245
[2025-05-11 18:02:07] Batch 60 - Loss: 0.933462
[2025-05-11 18:02:14] Batch 65 - Loss: 0.951894
[2025-05-11 18:02:22] Batch 70 - Loss: 0.933944
[2025-05-11 18:02:30] Batch 75 - Loss: 0.922744
[2025-05-11 18:02:37] Batch 80 - Loss: 0.930766
[2025-05-11 18:02:45] Batch 85 - Loss: 0.928655
[2025-05-11 18:02:53] Batch 90 - Loss: 0.926835
[2025-05-11 18:03:01] Batch 95 - Loss: 0.922210
[2025-05-11 18:03:12] === Epoch 70/100 Summary ===
Training Loss: 0.918379
Validation Loss: 2.208386
Learning Rate: 0.00000600
[2025-05-11 18:03:12] Generating detection visualizations for epoch 70...
[2025-05-11 18:03:12] Saved visualization to trained_models/custom_detector\visualizations\epoch_70\sample_1.png
[2025-05-11 18:03:13] Saved visualization to trained_models/custom_detector\visualizations\epoch_70\sample_2.png
[2025-05-11 18:03:13] Saved visualization to trained_models/custom_detector\visualizations\epoch_70\sample_3.png
[2025-05-11 18:03:15] Batch 0 - Loss: 1.167300
[2025-05-11 18:03:22] Batch 5 - Loss: 0.882535
[2025-05-11 18:03:30] Batch 10 - Loss: 0.935493
[2025-05-11 18:03:38] Batch 15 - Loss: 1.023512
[2025-05-11 18:03:46] Batch 20 - Loss: 0.968834
[2025-05-11 18:03:53] Batch 25 - Loss: 0.976759
[2025-05-11 18:04:01] Batch 30 - Loss: 0.980585
[2025-05-11 18:04:09] Batch 35 - Loss: 1.010936
[2025-05-11 18:04:16] Batch 40 - Loss: 0.977118
[2025-05-11 18:04:24] Batch 45 - Loss: 0.961259
[2025-05-11 18:04:32] Batch 50 - Loss: 0.958432
[2025-05-11 18:04:40] Batch 55 - Loss: 0.957753
[2025-05-11 18:04:47] Batch 60 - Loss: 0.951004
[2025-05-11 18:04:55] Batch 65 - Loss: 0.935404
[2025-05-11 18:05:03] Batch 70 - Loss: 0.933297
[2025-05-11 18:05:10] Batch 75 - Loss: 0.927111
[2025-05-11 18:05:18] Batch 80 - Loss: 0.919912
[2025-05-11 18:05:26] Batch 85 - Loss: 0.926754
[2025-05-11 18:05:33] Batch 90 - Loss: 0.929483
[2025-05-11 18:05:41] Batch 95 - Loss: 0.929651
[2025-05-11 18:05:53] === Epoch 71/100 Summary ===
Training Loss: 0.929867
Validation Loss: 2.222424
Learning Rate: 0.00000600
[2025-05-11 18:05:54] Batch 0 - Loss: 0.811329
[2025-05-11 18:06:02] Batch 5 - Loss: 0.933636
[2025-05-11 18:06:09] Batch 10 - Loss: 0.907984
[2025-05-11 18:06:17] Batch 15 - Loss: 0.887590
[2025-05-11 18:06:25] Batch 20 - Loss: 0.869050
[2025-05-11 18:06:33] Batch 25 - Loss: 0.850300
[2025-05-11 18:06:40] Batch 30 - Loss: 0.840014
[2025-05-11 18:06:48] Batch 35 - Loss: 0.820173
[2025-05-11 18:06:54] === Epoch 72/100 Summary ===
Training Loss: 0.828231
Validation Loss: 2.205701
Learning Rate: 0.00000600
[2025-05-11 18:06:56] Batch 0 - Loss: 0.646388
[2025-05-11 18:07:04] Batch 5 - Loss: 0.873964
[2025-05-11 18:07:12] Batch 10 - Loss: 1.037119
[2025-05-11 18:07:19] Batch 15 - Loss: 1.026035
[2025-05-11 18:07:27] Batch 20 - Loss: 1.018982
[2025-05-11 18:07:35] Batch 25 - Loss: 1.052507
[2025-05-11 18:07:43] Batch 30 - Loss: 1.076405
[2025-05-11 18:07:50] Batch 35 - Loss: 1.090333
[2025-05-11 18:07:58] Batch 40 - Loss: 1.079224
[2025-05-11 18:08:06] Batch 45 - Loss: 1.074433
[2025-05-11 18:08:13] Batch 50 - Loss: 1.071284
[2025-05-11 18:08:21] Batch 55 - Loss: 1.058709
[2025-05-11 18:08:29] Batch 60 - Loss: 1.061545
[2025-05-11 18:08:36] Batch 65 - Loss: 1.051267
[2025-05-11 18:08:44] Batch 70 - Loss: 1.039286
[2025-05-11 18:08:52] Batch 75 - Loss: 1.019780
[2025-05-11 18:09:00] Batch 80 - Loss: 1.007131
[2025-05-11 18:09:07] Batch 85 - Loss: 0.994231
[2025-05-11 18:09:15] Batch 90 - Loss: 0.993381
[2025-05-11 18:09:23] Batch 95 - Loss: 0.990419
[2025-05-11 18:09:34] === Epoch 73/100 Summary ===
Training Loss: 0.987717
Validation Loss: 2.160065
Learning Rate: 0.00000600
[2025-05-11 18:09:36] Batch 0 - Loss: 1.059336
[2025-05-11 18:09:44] Batch 5 - Loss: 0.968075
[2025-05-11 18:09:51] Batch 10 - Loss: 0.949609
[2025-05-11 18:09:59] Batch 15 - Loss: 0.896646
[2025-05-11 18:10:07] Batch 20 - Loss: 0.931657
[2025-05-11 18:10:14] Batch 25 - Loss: 0.956044
[2025-05-11 18:10:22] Batch 30 - Loss: 0.926192
[2025-05-11 18:10:30] Batch 35 - Loss: 0.922203
[2025-05-11 18:10:38] Batch 40 - Loss: 0.932874
[2025-05-11 18:10:45] Batch 45 - Loss: 0.959920
[2025-05-11 18:10:53] Batch 50 - Loss: 0.982624
[2025-05-11 18:11:01] Batch 55 - Loss: 0.982477
[2025-05-11 18:11:08] Batch 60 - Loss: 0.967181
[2025-05-11 18:11:16] Batch 65 - Loss: 0.937762
[2025-05-11 18:11:24] Batch 70 - Loss: 0.924497
[2025-05-11 18:11:32] Batch 75 - Loss: 0.908086
[2025-05-11 18:11:39] Batch 80 - Loss: 0.897537
[2025-05-11 18:11:47] Batch 85 - Loss: 0.887941
[2025-05-11 18:11:55] Batch 90 - Loss: 0.882139
[2025-05-11 18:12:02] Batch 95 - Loss: 0.887078
[2025-05-11 18:12:14] === Epoch 74/100 Summary ===
Training Loss: 0.893206
Validation Loss: 2.164456
Learning Rate: 0.00000600
[2025-05-11 18:12:15] Batch 0 - Loss: 0.869327
[2025-05-11 18:12:23] Batch 5 - Loss: 0.764180
[2025-05-11 18:12:31] Batch 10 - Loss: 0.839376
[2025-05-11 18:12:38] Batch 15 - Loss: 0.845364
[2025-05-11 18:12:46] Batch 20 - Loss: 0.845561
[2025-05-11 18:12:54] Batch 25 - Loss: 0.843450
[2025-05-11 18:13:02] Batch 30 - Loss: 0.865201
[2025-05-11 18:13:09] Batch 35 - Loss: 0.850160
[2025-05-11 18:13:17] Batch 40 - Loss: 0.862666
[2025-05-11 18:13:25] Batch 45 - Loss: 0.873433
[2025-05-11 18:13:33] Batch 50 - Loss: 0.864354
[2025-05-11 18:13:40] Batch 55 - Loss: 0.872168
[2025-05-11 18:13:48] Batch 60 - Loss: 0.856597
[2025-05-11 18:13:56] Batch 65 - Loss: 0.863042
[2025-05-11 18:14:03] Batch 70 - Loss: 0.859797
[2025-05-11 18:14:11] Batch 75 - Loss: 0.860493
[2025-05-11 18:14:19] Batch 80 - Loss: 0.857478
[2025-05-11 18:14:27] Batch 85 - Loss: 0.858504
[2025-05-11 18:14:34] Batch 90 - Loss: 0.849293
[2025-05-11 18:14:42] Batch 95 - Loss: 0.842287
[2025-05-11 18:14:53] === Epoch 75/100 Summary ===
Training Loss: 0.840527
Validation Loss: 2.201787
Learning Rate: 0.00000600
[2025-05-11 18:14:53] Generating detection visualizations for epoch 75...
[2025-05-11 18:14:54] Saved visualization to trained_models/custom_detector\visualizations\epoch_75\sample_1.png
[2025-05-11 18:14:54] Saved visualization to trained_models/custom_detector\visualizations\epoch_75\sample_2.png
[2025-05-11 18:14:54] Saved visualization to trained_models/custom_detector\visualizations\epoch_75\sample_3.png
[2025-05-11 18:14:56] Batch 0 - Loss: 0.702284
[2025-05-11 18:15:04] Batch 5 - Loss: 0.628835
[2025-05-11 18:15:11] Batch 10 - Loss: 0.694567
[2025-05-11 18:15:19] Batch 15 - Loss: 0.809838
[2025-05-11 18:15:27] Batch 20 - Loss: 0.825350
[2025-05-11 18:15:34] Batch 25 - Loss: 0.852349
[2025-05-11 18:15:42] Batch 30 - Loss: 0.820599
[2025-05-11 18:15:50] Batch 35 - Loss: 0.826132
[2025-05-11 18:15:56] === Epoch 76/100 Summary ===
Training Loss: 0.822788
Validation Loss: 2.171084
Learning Rate: 0.00000600
[2025-05-11 18:15:58] Batch 0 - Loss: 1.153336
[2025-05-11 18:16:06] Batch 5 - Loss: 0.974874
[2025-05-11 18:16:13] Batch 10 - Loss: 0.940048
[2025-05-11 18:16:21] Batch 15 - Loss: 0.982344
[2025-05-11 18:16:29] Batch 20 - Loss: 1.023908
[2025-05-11 18:16:36] Batch 25 - Loss: 0.993470
[2025-05-11 18:16:44] Batch 30 - Loss: 1.002096
[2025-05-11 18:16:52] Batch 35 - Loss: 0.988833
[2025-05-11 18:17:00] Batch 40 - Loss: 0.990582
[2025-05-11 18:17:07] Batch 45 - Loss: 0.986574
[2025-05-11 18:17:15] Batch 50 - Loss: 0.970478
[2025-05-11 18:17:23] Batch 55 - Loss: 0.954442
[2025-05-11 18:17:30] Batch 60 - Loss: 0.949515
[2025-05-11 18:17:38] Batch 65 - Loss: 0.959645
[2025-05-11 18:17:46] Batch 70 - Loss: 0.948551
[2025-05-11 18:17:54] Batch 75 - Loss: 0.946358
[2025-05-11 18:18:01] Batch 80 - Loss: 0.938405
[2025-05-11 18:18:09] Batch 85 - Loss: 0.918898
[2025-05-11 18:18:17] Batch 90 - Loss: 0.926146
[2025-05-11 18:18:24] Batch 95 - Loss: 0.916525
[2025-05-11 18:18:36] === Epoch 77/100 Summary ===
Training Loss: 0.910305
Validation Loss: 2.137771
Learning Rate: 0.00000600
[2025-05-11 18:18:38] Batch 0 - Loss: 0.707106
[2025-05-11 18:18:45] Batch 5 - Loss: 0.865380
[2025-05-11 18:18:53] Batch 10 - Loss: 0.910377
[2025-05-11 18:19:01] Batch 15 - Loss: 0.895942
[2025-05-11 18:19:08] Batch 20 - Loss: 0.894068
[2025-05-11 18:19:16] Batch 25 - Loss: 0.869969
[2025-05-11 18:19:24] Batch 30 - Loss: 0.870848
[2025-05-11 18:19:32] Batch 35 - Loss: 0.876761
[2025-05-11 18:19:39] Batch 40 - Loss: 0.887507
[2025-05-11 18:19:47] Batch 45 - Loss: 0.887727
[2025-05-11 18:19:55] Batch 50 - Loss: 0.908076
[2025-05-11 18:20:03] Batch 55 - Loss: 0.900483
[2025-05-11 18:20:10] Batch 60 - Loss: 0.905631
[2025-05-11 18:20:18] Batch 65 - Loss: 0.908519
[2025-05-11 18:20:26] Batch 70 - Loss: 0.904948
[2025-05-11 18:20:33] Batch 75 - Loss: 0.901760
[2025-05-11 18:20:41] Batch 80 - Loss: 0.914091
[2025-05-11 18:20:49] Batch 85 - Loss: 0.903902
[2025-05-11 18:20:57] Batch 90 - Loss: 0.895514
[2025-05-11 18:21:04] Batch 95 - Loss: 0.885733
[2025-05-11 18:21:16] === Epoch 78/100 Summary ===
Training Loss: 0.875726
Validation Loss: 2.155660
Learning Rate: 0.00000600
[2025-05-11 18:21:17] Batch 0 - Loss: 1.171971
[2025-05-11 18:21:25] Batch 5 - Loss: 0.971425
[2025-05-11 18:21:33] Batch 10 - Loss: 0.917571
[2025-05-11 18:21:41] Batch 15 - Loss: 0.863256
[2025-05-11 18:21:48] Batch 20 - Loss: 0.837500
[2025-05-11 18:21:56] Batch 25 - Loss: 0.857743
[2025-05-11 18:22:04] Batch 30 - Loss: 0.858730
[2025-05-11 18:22:11] Batch 35 - Loss: 0.860533
[2025-05-11 18:22:19] Batch 40 - Loss: 0.863636
[2025-05-11 18:22:27] Batch 45 - Loss: 0.885905
[2025-05-11 18:22:35] Batch 50 - Loss: 0.879505
[2025-05-11 18:22:42] Batch 55 - Loss: 0.873144
[2025-05-11 18:22:50] Batch 60 - Loss: 0.860573
[2025-05-11 18:22:58] Batch 65 - Loss: 0.851512
[2025-05-11 18:23:05] Batch 70 - Loss: 0.843812
[2025-05-11 18:23:13] Batch 75 - Loss: 0.843198
[2025-05-11 18:23:21] Batch 80 - Loss: 0.852812
[2025-05-11 18:23:29] Batch 85 - Loss: 0.856037
[2025-05-11 18:23:36] Batch 90 - Loss: 0.854607
[2025-05-11 18:23:44] Batch 95 - Loss: 0.852984
[2025-05-11 18:23:56] === Epoch 79/100 Summary ===
Training Loss: 0.846053
Validation Loss: 2.171419
Learning Rate: 0.00000600
[2025-05-11 18:23:57] Batch 0 - Loss: 1.149659
[2025-05-11 18:24:05] Batch 5 - Loss: 0.879390
[2025-05-11 18:24:12] Batch 10 - Loss: 0.819758
[2025-05-11 18:24:20] Batch 15 - Loss: 0.826360
[2025-05-11 18:24:28] Batch 20 - Loss: 0.831835
[2025-05-11 18:24:36] Batch 25 - Loss: 0.861821
[2025-05-11 18:24:43] Batch 30 - Loss: 0.837680
[2025-05-11 18:24:51] Batch 35 - Loss: 0.831396
[2025-05-11 18:24:57] === Epoch 80/100 Summary ===
Training Loss: 0.825287
Validation Loss: 2.183223
Learning Rate: 0.00000600
[2025-05-11 18:24:57] Generating detection visualizations for epoch 80...
[2025-05-11 18:24:58] Saved visualization to trained_models/custom_detector\visualizations\epoch_80\sample_1.png
[2025-05-11 18:24:58] Saved visualization to trained_models/custom_detector\visualizations\epoch_80\sample_2.png
[2025-05-11 18:24:58] Saved visualization to trained_models/custom_detector\visualizations\epoch_80\sample_3.png
[2025-05-11 18:25:00] Batch 0 - Loss: 0.620396
[2025-05-11 18:25:08] Batch 5 - Loss: 1.042116
[2025-05-11 18:25:16] Batch 10 - Loss: 0.917548
[2025-05-11 18:25:23] Batch 15 - Loss: 0.898717
[2025-05-11 18:25:31] Batch 20 - Loss: 0.920498
[2025-05-11 18:25:39] Batch 25 - Loss: 0.945776
[2025-05-11 18:25:47] Batch 30 - Loss: 0.940732
[2025-05-11 18:25:54] Batch 35 - Loss: 0.951671
[2025-05-11 18:26:02] Batch 40 - Loss: 0.931175
[2025-05-11 18:26:10] Batch 45 - Loss: 0.935159
[2025-05-11 18:26:18] Batch 50 - Loss: 0.929736
[2025-05-11 18:26:25] Batch 55 - Loss: 0.957119
[2025-05-11 18:26:33] Batch 60 - Loss: 0.940540
[2025-05-11 18:26:41] Batch 65 - Loss: 0.925674
[2025-05-11 18:26:48] Batch 70 - Loss: 0.916264
[2025-05-11 18:26:56] Batch 75 - Loss: 0.912109
[2025-05-11 18:27:04] Batch 80 - Loss: 0.914862
[2025-05-11 18:27:12] Batch 85 - Loss: 0.920274
[2025-05-11 18:27:19] Batch 90 - Loss: 0.904694
[2025-05-11 18:27:27] Batch 95 - Loss: 0.900403
[2025-05-11 18:27:38] === Epoch 81/100 Summary ===
Training Loss: 0.895956
Validation Loss: 2.168501
Learning Rate: 0.00000600
[2025-05-11 18:27:40] Batch 0 - Loss: 1.140342
[2025-05-11 18:27:48] Batch 5 - Loss: 0.760854
[2025-05-11 18:27:55] Batch 10 - Loss: 0.822638
[2025-05-11 18:28:03] Batch 15 - Loss: 0.809051
[2025-05-11 18:28:11] Batch 20 - Loss: 0.849674
[2025-05-11 18:28:19] Batch 25 - Loss: 0.849903
[2025-05-11 18:28:26] Batch 30 - Loss: 0.864790
[2025-05-11 18:28:34] Batch 35 - Loss: 0.886059
[2025-05-11 18:28:42] Batch 40 - Loss: 0.891979
[2025-05-11 18:28:50] Batch 45 - Loss: 0.890099
[2025-05-11 18:28:57] Batch 50 - Loss: 0.907680
[2025-05-11 18:29:05] Batch 55 - Loss: 0.903001
[2025-05-11 18:29:13] Batch 60 - Loss: 0.887217
[2025-05-11 18:29:21] Batch 65 - Loss: 0.882120
[2025-05-11 18:29:28] Batch 70 - Loss: 0.882439
[2025-05-11 18:29:36] Batch 75 - Loss: 0.886684
[2025-05-11 18:29:44] Batch 80 - Loss: 0.881650
[2025-05-11 18:29:51] Batch 85 - Loss: 0.878235
[2025-05-11 18:29:59] Batch 90 - Loss: 0.876955
[2025-05-11 18:30:07] Batch 95 - Loss: 0.871341
[2025-05-11 18:30:19] === Epoch 82/100 Summary ===
Training Loss: 0.866157
Validation Loss: 2.132651
Learning Rate: 0.00000600
[2025-05-11 18:30:20] Batch 0 - Loss: 0.880903
[2025-05-11 18:30:28] Batch 5 - Loss: 0.926945
[2025-05-11 18:30:35] Batch 10 - Loss: 0.836820
[2025-05-11 18:30:43] Batch 15 - Loss: 0.881062
[2025-05-11 18:30:51] Batch 20 - Loss: 0.880170
[2025-05-11 18:30:59] Batch 25 - Loss: 0.881450
[2025-05-11 18:31:06] Batch 30 - Loss: 0.879045
[2025-05-11 18:31:14] Batch 35 - Loss: 0.890113
[2025-05-11 18:31:22] Batch 40 - Loss: 0.896424
[2025-05-11 18:31:30] Batch 45 - Loss: 0.910468
[2025-05-11 18:31:37] Batch 50 - Loss: 0.891011
[2025-05-11 18:31:45] Batch 55 - Loss: 0.883164
[2025-05-11 18:31:53] Batch 60 - Loss: 0.886544
[2025-05-11 18:32:00] Batch 65 - Loss: 0.886657
[2025-05-11 18:32:08] Batch 70 - Loss: 0.874970
[2025-05-11 18:32:16] Batch 75 - Loss: 0.881052
[2025-05-11 18:32:23] Batch 80 - Loss: 0.867445
[2025-05-11 18:32:31] Batch 85 - Loss: 0.859689
[2025-05-11 18:32:39] Batch 90 - Loss: 0.857955
[2025-05-11 18:32:47] Batch 95 - Loss: 0.851111
[2025-05-11 18:32:58] === Epoch 83/100 Summary ===
Training Loss: 0.847391
Validation Loss: 2.154363
Learning Rate: 0.00000600
[2025-05-11 18:33:00] Batch 0 - Loss: 0.778781
[2025-05-11 18:33:07] Batch 5 - Loss: 0.722387
[2025-05-11 18:33:15] Batch 10 - Loss: 0.744737
[2025-05-11 18:33:23] Batch 15 - Loss: 0.786654
[2025-05-11 18:33:30] Batch 20 - Loss: 0.819226
[2025-05-11 18:33:38] Batch 25 - Loss: 0.830604
[2025-05-11 18:33:46] Batch 30 - Loss: 0.814271
[2025-05-11 18:33:54] Batch 35 - Loss: 0.799854
[2025-05-11 18:34:00] === Epoch 84/100 Summary ===
Training Loss: 0.796905
Validation Loss: 2.168111
Learning Rate: 0.00000600
[2025-05-11 18:34:02] Batch 0 - Loss: 0.771519
[2025-05-11 18:34:09] Batch 5 - Loss: 0.800208
[2025-05-11 18:34:17] Batch 10 - Loss: 0.969339
[2025-05-11 18:34:25] Batch 15 - Loss: 0.982164
[2025-05-11 18:34:33] Batch 20 - Loss: 0.989438
[2025-05-11 18:34:40] Batch 25 - Loss: 0.956622
[2025-05-11 18:34:48] Batch 30 - Loss: 0.934319
[2025-05-11 18:34:56] Batch 35 - Loss: 0.923935
[2025-05-11 18:35:03] Batch 40 - Loss: 0.938408
[2025-05-11 18:35:11] Batch 45 - Loss: 0.937393
[2025-05-11 18:35:19] Batch 50 - Loss: 0.920934
[2025-05-11 18:35:26] Batch 55 - Loss: 0.922940
[2025-05-11 18:35:34] Batch 60 - Loss: 0.924553
[2025-05-11 18:35:42] Batch 65 - Loss: 0.908661
[2025-05-11 18:35:49] Batch 70 - Loss: 0.912185
[2025-05-11 18:35:57] Batch 75 - Loss: 0.914858
[2025-05-11 18:36:05] Batch 80 - Loss: 0.913489
[2025-05-11 18:36:12] Batch 85 - Loss: 0.901785
[2025-05-11 18:36:20] Batch 90 - Loss: 0.900787
[2025-05-11 18:36:28] Batch 95 - Loss: 0.886149
[2025-05-11 18:36:39] === Epoch 85/100 Summary ===
Training Loss: 0.881646
Validation Loss: 2.146847
Learning Rate: 0.00000600
[2025-05-11 18:36:39] Generating detection visualizations for epoch 85...
[2025-05-11 18:36:39] Saved visualization to trained_models/custom_detector\visualizations\epoch_85\sample_1.png
[2025-05-11 18:36:40] Saved visualization to trained_models/custom_detector\visualizations\epoch_85\sample_2.png
[2025-05-11 18:36:40] Saved visualization to trained_models/custom_detector\visualizations\epoch_85\sample_3.png
[2025-05-11 18:36:42] Batch 0 - Loss: 1.074053
[2025-05-11 18:36:49] Batch 5 - Loss: 0.852704
[2025-05-11 18:36:57] Batch 10 - Loss: 0.898061
[2025-05-11 18:37:05] Batch 15 - Loss: 0.856544
[2025-05-11 18:37:13] Batch 20 - Loss: 0.901367
[2025-05-11 18:37:20] Batch 25 - Loss: 0.876990
[2025-05-11 18:37:28] Batch 30 - Loss: 0.860701
[2025-05-11 18:37:36] Batch 35 - Loss: 0.866339
[2025-05-11 18:37:43] Batch 40 - Loss: 0.870756
[2025-05-11 18:37:51] Batch 45 - Loss: 0.875944
[2025-05-11 18:37:59] Batch 50 - Loss: 0.901273
[2025-05-11 18:38:07] Batch 55 - Loss: 0.903738
[2025-05-11 18:38:14] Batch 60 - Loss: 0.890244
[2025-05-11 18:38:22] Batch 65 - Loss: 0.890236
[2025-05-11 18:38:30] Batch 70 - Loss: 0.869770
[2025-05-11 18:38:37] Batch 75 - Loss: 0.852408
[2025-05-11 18:38:45] Batch 80 - Loss: 0.860968
[2025-05-11 18:38:53] Batch 85 - Loss: 0.852955
[2025-05-11 18:39:01] Batch 90 - Loss: 0.852338
[2025-05-11 18:39:08] Batch 95 - Loss: 0.850955
[2025-05-11 18:39:20] === Epoch 86/100 Summary ===
Training Loss: 0.857540
Validation Loss: 2.150395
Learning Rate: 0.00000600
[2025-05-11 18:39:21] Batch 0 - Loss: 0.663255
[2025-05-11 18:39:29] Batch 5 - Loss: 0.815815
[2025-05-11 18:39:37] Batch 10 - Loss: 0.803322
[2025-05-11 18:39:44] Batch 15 - Loss: 0.839316
[2025-05-11 18:39:52] Batch 20 - Loss: 0.805909
[2025-05-11 18:40:00] Batch 25 - Loss: 0.807856
[2025-05-11 18:40:08] Batch 30 - Loss: 0.788153
[2025-05-11 18:40:15] Batch 35 - Loss: 0.807886
[2025-05-11 18:40:23] Batch 40 - Loss: 0.857160
[2025-05-11 18:40:31] Batch 45 - Loss: 0.853188
[2025-05-11 18:40:38] Batch 50 - Loss: 0.855006
[2025-05-11 18:40:46] Batch 55 - Loss: 0.851793
[2025-05-11 18:40:54] Batch 60 - Loss: 0.838836
[2025-05-11 18:41:02] Batch 65 - Loss: 0.842552
[2025-05-11 18:41:09] Batch 70 - Loss: 0.833646
[2025-05-11 18:41:17] Batch 75 - Loss: 0.830521
[2025-05-11 18:41:25] Batch 80 - Loss: 0.834200
[2025-05-11 18:41:32] Batch 85 - Loss: 0.825994
[2025-05-11 18:41:40] Batch 90 - Loss: 0.825078
[2025-05-11 18:41:48] Batch 95 - Loss: 0.817394
[2025-05-11 18:41:59] === Epoch 87/100 Summary ===
Training Loss: 0.811737
Validation Loss: 2.190849
Learning Rate: 0.00000120
[2025-05-11 18:42:01] Batch 0 - Loss: 0.805502
[2025-05-11 18:42:08] Batch 5 - Loss: 0.775763
[2025-05-11 18:42:16] Batch 10 - Loss: 0.718649
[2025-05-11 18:42:24] Batch 15 - Loss: 0.725391
[2025-05-11 18:42:31] Batch 20 - Loss: 0.700973
[2025-05-11 18:42:39] Batch 25 - Loss: 0.672704
[2025-05-11 18:42:47] Batch 30 - Loss: 0.683142
[2025-05-11 18:42:54] Batch 35 - Loss: 0.715434
[2025-05-11 18:43:00] === Epoch 88/100 Summary ===
Training Loss: 0.715637
Validation Loss: 2.170046
Learning Rate: 0.00000120
[2025-05-11 18:43:02] Batch 0 - Loss: 1.257050
[2025-05-11 18:43:10] Batch 5 - Loss: 0.905105
[2025-05-11 18:43:18] Batch 10 - Loss: 0.950512
[2025-05-11 18:43:25] Batch 15 - Loss: 0.997212
[2025-05-11 18:43:33] Batch 20 - Loss: 0.975426
[2025-05-11 18:43:41] Batch 25 - Loss: 0.931025
[2025-05-11 18:43:49] Batch 30 - Loss: 0.945077
[2025-05-11 18:43:56] Batch 35 - Loss: 0.955960
[2025-05-11 18:44:04] Batch 40 - Loss: 0.957498
[2025-05-11 18:44:12] Batch 45 - Loss: 0.949432
[2025-05-11 18:44:19] Batch 50 - Loss: 0.934860
[2025-05-11 18:44:27] Batch 55 - Loss: 0.930530
[2025-05-11 18:44:35] Batch 60 - Loss: 0.941520
[2025-05-11 18:44:42] Batch 65 - Loss: 0.933872
[2025-05-11 18:44:50] Batch 70 - Loss: 0.933239
[2025-05-11 18:44:58] Batch 75 - Loss: 0.910021
[2025-05-11 18:45:06] Batch 80 - Loss: 0.902734
[2025-05-11 18:45:13] Batch 85 - Loss: 0.895555
[2025-05-11 18:45:21] Batch 90 - Loss: 0.886358
[2025-05-11 18:45:29] Batch 95 - Loss: 0.886774
[2025-05-11 18:45:40] === Epoch 89/100 Summary ===
Training Loss: 0.880659
Validation Loss: 2.144387
Learning Rate: 0.00000120
[2025-05-11 18:45:42] Batch 0 - Loss: 0.897982
[2025-05-11 18:45:49] Batch 5 - Loss: 0.960448
[2025-05-11 18:45:57] Batch 10 - Loss: 0.891211
[2025-05-11 18:46:05] Batch 15 - Loss: 0.835182
[2025-05-11 18:46:12] Batch 20 - Loss: 0.871779
[2025-05-11 18:46:20] Batch 25 - Loss: 0.846369
[2025-05-11 18:46:28] Batch 30 - Loss: 0.843202
[2025-05-11 18:46:35] Batch 35 - Loss: 0.822651
[2025-05-11 18:46:43] Batch 40 - Loss: 0.815060
[2025-05-11 18:46:51] Batch 45 - Loss: 0.862722
[2025-05-11 18:46:59] Batch 50 - Loss: 0.848592
[2025-05-11 18:47:06] Batch 55 - Loss: 0.870143
[2025-05-11 18:47:14] Batch 60 - Loss: 0.863936
[2025-05-11 18:47:22] Batch 65 - Loss: 0.863313
[2025-05-11 18:47:29] Batch 70 - Loss: 0.852357
[2025-05-11 18:47:37] Batch 75 - Loss: 0.855985
[2025-05-11 18:47:45] Batch 80 - Loss: 0.851767
[2025-05-11 18:47:52] Batch 85 - Loss: 0.855244
[2025-05-11 18:48:00] Batch 90 - Loss: 0.850098
[2025-05-11 18:48:08] Batch 95 - Loss: 0.844899
[2025-05-11 18:48:19] === Epoch 90/100 Summary ===
Training Loss: 0.843792
Validation Loss: 2.123087
Learning Rate: 0.00000120
[2025-05-11 18:48:19] Generating detection visualizations for epoch 90...
[2025-05-11 18:48:20] Saved visualization to trained_models/custom_detector\visualizations\epoch_90\sample_1.png
[2025-05-11 18:48:20] Saved visualization to trained_models/custom_detector\visualizations\epoch_90\sample_2.png
[2025-05-11 18:48:20] Saved visualization to trained_models/custom_detector\visualizations\epoch_90\sample_3.png
[2025-05-11 18:48:22] Batch 0 - Loss: 0.890785
[2025-05-11 18:48:30] Batch 5 - Loss: 0.816950
[2025-05-11 18:48:37] Batch 10 - Loss: 0.766401
[2025-05-11 18:48:45] Batch 15 - Loss: 0.829864
[2025-05-11 18:48:53] Batch 20 - Loss: 0.781210
[2025-05-11 18:49:00] Batch 25 - Loss: 0.815142
[2025-05-11 18:49:08] Batch 30 - Loss: 0.827462
[2025-05-11 18:49:16] Batch 35 - Loss: 0.845356
[2025-05-11 18:49:24] Batch 40 - Loss: 0.846020
[2025-05-11 18:49:31] Batch 45 - Loss: 0.861665
[2025-05-11 18:49:39] Batch 50 - Loss: 0.847648
[2025-05-11 18:49:47] Batch 55 - Loss: 0.831164
[2025-05-11 18:49:54] Batch 60 - Loss: 0.838404
[2025-05-11 18:50:02] Batch 65 - Loss: 0.818687
[2025-05-11 18:50:10] Batch 70 - Loss: 0.815724
[2025-05-11 18:50:17] Batch 75 - Loss: 0.812657
[2025-05-11 18:50:25] Batch 80 - Loss: 0.813234
[2025-05-11 18:50:33] Batch 85 - Loss: 0.812772
[2025-05-11 18:50:40] Batch 90 - Loss: 0.806610
[2025-05-11 18:50:48] Batch 95 - Loss: 0.802002
[2025-05-11 18:51:00] === Epoch 91/100 Summary ===
Training Loss: 0.796940
Validation Loss: 2.142129
Learning Rate: 0.00000120
[2025-05-11 18:51:01] Batch 0 - Loss: 1.005979
[2025-05-11 18:51:09] Batch 5 - Loss: 0.749391
[2025-05-11 18:51:17] Batch 10 - Loss: 0.710907
[2025-05-11 18:51:24] Batch 15 - Loss: 0.685598
[2025-05-11 18:51:32] Batch 20 - Loss: 0.668043
[2025-05-11 18:51:40] Batch 25 - Loss: 0.701631
[2025-05-11 18:51:47] Batch 30 - Loss: 0.702294
[2025-05-11 18:51:55] Batch 35 - Loss: 0.708319
[2025-05-11 18:52:01] === Epoch 92/100 Summary ===
Training Loss: 0.712143
Validation Loss: 2.143005
Learning Rate: 0.00000120
[2025-05-11 18:52:03] Batch 0 - Loss: 1.232616
[2025-05-11 18:52:11] Batch 5 - Loss: 0.937625
[2025-05-11 18:52:18] Batch 10 - Loss: 1.019808
[2025-05-11 18:52:26] Batch 15 - Loss: 1.024918
[2025-05-11 18:52:34] Batch 20 - Loss: 0.993294
[2025-05-11 18:52:42] Batch 25 - Loss: 0.985676
[2025-05-11 18:52:49] Batch 30 - Loss: 0.996630
[2025-05-11 18:52:57] Batch 35 - Loss: 0.989199
[2025-05-11 18:53:05] Batch 40 - Loss: 0.974391
[2025-05-11 18:53:12] Batch 45 - Loss: 0.952532
[2025-05-11 18:53:20] Batch 50 - Loss: 0.936055
[2025-05-11 18:53:28] Batch 55 - Loss: 0.926232
[2025-05-11 18:53:35] Batch 60 - Loss: 0.916575
[2025-05-11 18:53:43] Batch 65 - Loss: 0.908507
[2025-05-11 18:53:51] Batch 70 - Loss: 0.891508
[2025-05-11 18:53:58] Batch 75 - Loss: 0.888433
[2025-05-11 18:54:06] Batch 80 - Loss: 0.883668
[2025-05-11 18:54:14] Batch 85 - Loss: 0.885573
[2025-05-11 18:54:22] Batch 90 - Loss: 0.880519
[2025-05-11 18:54:30] Batch 95 - Loss: 0.867253
[2025-05-11 18:54:41] === Epoch 93/100 Summary ===
Training Loss: 0.860422
Validation Loss: 2.138630
Learning Rate: 0.00000120
[2025-05-11 18:54:43] Batch 0 - Loss: 1.065359
[2025-05-11 18:54:51] Batch 5 - Loss: 0.811148
[2025-05-11 18:54:58] Batch 10 - Loss: 0.776525
[2025-05-11 18:55:06] Batch 15 - Loss: 0.780947
[2025-05-11 18:55:14] Batch 20 - Loss: 0.777609
[2025-05-11 18:55:22] Batch 25 - Loss: 0.800146
[2025-05-11 18:55:29] Batch 30 - Loss: 0.805445
[2025-05-11 18:55:37] Batch 35 - Loss: 0.806985
[2025-05-11 18:55:45] Batch 40 - Loss: 0.846889
[2025-05-11 18:55:53] Batch 45 - Loss: 0.861950
[2025-05-11 18:56:01] Batch 50 - Loss: 0.874386
[2025-05-11 18:56:08] Batch 55 - Loss: 0.872390
[2025-05-11 18:56:16] Batch 60 - Loss: 0.873123
[2025-05-11 18:56:24] Batch 65 - Loss: 0.869885
[2025-05-11 18:56:31] Batch 70 - Loss: 0.863622
[2025-05-11 18:56:39] Batch 75 - Loss: 0.861388
[2025-05-11 18:56:47] Batch 80 - Loss: 0.854343
[2025-05-11 18:56:55] Batch 85 - Loss: 0.845287
[2025-05-11 18:57:02] Batch 90 - Loss: 0.840864
[2025-05-11 18:57:10] Batch 95 - Loss: 0.846667
[2025-05-11 18:57:22] === Epoch 94/100 Summary ===
Training Loss: 0.842822
Validation Loss: 2.120262
Learning Rate: 0.00000120
[2025-05-11 18:57:23] Batch 0 - Loss: 0.875130
[2025-05-11 18:57:31] Batch 5 - Loss: 0.771560
[2025-05-11 18:57:39] Batch 10 - Loss: 0.958979
[2025-05-11 18:57:47] Batch 15 - Loss: 0.990951
[2025-05-11 18:57:55] Batch 20 - Loss: 0.921563
[2025-05-11 18:58:02] Batch 25 - Loss: 0.938715
[2025-05-11 18:58:10] Batch 30 - Loss: 0.887378
[2025-05-11 18:58:18] Batch 35 - Loss: 0.884490
[2025-05-11 18:58:25] Batch 40 - Loss: 0.887549
[2025-05-11 18:58:33] Batch 45 - Loss: 0.873488
[2025-05-11 18:58:41] Batch 50 - Loss: 0.851926
[2025-05-11 18:58:48] Batch 55 - Loss: 0.850825
[2025-05-11 18:58:56] Batch 60 - Loss: 0.846983
[2025-05-11 18:59:04] Batch 65 - Loss: 0.833380
[2025-05-11 18:59:12] Batch 70 - Loss: 0.821398
[2025-05-11 18:59:19] Batch 75 - Loss: 0.809426
[2025-05-11 18:59:27] Batch 80 - Loss: 0.801364
[2025-05-11 18:59:35] Batch 85 - Loss: 0.797757
[2025-05-11 18:59:43] Batch 90 - Loss: 0.788650
[2025-05-11 18:59:51] Batch 95 - Loss: 0.784090
[2025-05-11 19:00:02] === Epoch 95/100 Summary ===
Training Loss: 0.774777
Validation Loss: 2.136410
Learning Rate: 0.00000120
[2025-05-11 19:00:02] Generating detection visualizations for epoch 95...
[2025-05-11 19:00:03] Saved visualization to trained_models/custom_detector\visualizations\epoch_95\sample_1.png
[2025-05-11 19:00:03] Saved visualization to trained_models/custom_detector\visualizations\epoch_95\sample_2.png
[2025-05-11 19:00:03] Saved visualization to trained_models/custom_detector\visualizations\epoch_95\sample_3.png
[2025-05-11 19:00:05] Batch 0 - Loss: 0.925712
[2025-05-11 19:00:13] Batch 5 - Loss: 0.694052
[2025-05-11 19:00:20] Batch 10 - Loss: 0.641128
[2025-05-11 19:00:28] Batch 15 - Loss: 0.660671
[2025-05-11 19:00:36] Batch 20 - Loss: 0.661357
[2025-05-11 19:00:43] Batch 25 - Loss: 0.666013
[2025-05-11 19:00:51] Batch 30 - Loss: 0.678343
[2025-05-11 19:00:59] Batch 35 - Loss: 0.690665
[2025-05-11 19:01:05] === Epoch 96/100 Summary ===
Training Loss: 0.688780
Validation Loss: 2.143498
Learning Rate: 0.00000120
[2025-05-11 19:01:07] Batch 0 - Loss: 1.015982
[2025-05-11 19:01:15] Batch 5 - Loss: 1.024045
[2025-05-11 19:01:22] Batch 10 - Loss: 0.933143
[2025-05-11 19:01:30] Batch 15 - Loss: 0.970409
[2025-05-11 19:01:38] Batch 20 - Loss: 0.942263
[2025-05-11 19:01:45] Batch 25 - Loss: 0.958421
[2025-05-11 19:01:53] Batch 30 - Loss: 0.937049
[2025-05-11 19:02:01] Batch 35 - Loss: 0.919686
[2025-05-11 19:02:08] Batch 40 - Loss: 0.916166
[2025-05-11 19:02:16] Batch 45 - Loss: 0.913132
[2025-05-11 19:02:24] Batch 50 - Loss: 0.914754
[2025-05-11 19:02:31] Batch 55 - Loss: 0.911414
[2025-05-11 19:02:39] Batch 60 - Loss: 0.900068
[2025-05-11 19:02:47] Batch 65 - Loss: 0.891860
[2025-05-11 19:02:54] Batch 70 - Loss: 0.878440
[2025-05-11 19:03:02] Batch 75 - Loss: 0.867914
[2025-05-11 19:03:10] Batch 80 - Loss: 0.864085
[2025-05-11 19:03:17] Batch 85 - Loss: 0.856624
[2025-05-11 19:03:25] Batch 90 - Loss: 0.851898
[2025-05-11 19:03:33] Batch 95 - Loss: 0.847081
[2025-05-11 19:03:44] === Epoch 97/100 Summary ===
Training Loss: 0.838424
Validation Loss: 2.138434
Learning Rate: 0.00000120
[2025-05-11 19:03:46] Batch 0 - Loss: 0.976204
[2025-05-11 19:03:54] Batch 5 - Loss: 0.776040
[2025-05-11 19:04:01] Batch 10 - Loss: 0.832000
[2025-05-11 19:04:09] Batch 15 - Loss: 0.814226
[2025-05-11 19:04:17] Batch 20 - Loss: 0.835729
[2025-05-11 19:04:24] Batch 25 - Loss: 0.874026
[2025-05-11 19:04:32] Batch 30 - Loss: 0.861293
[2025-05-11 19:04:40] Batch 35 - Loss: 0.851014
[2025-05-11 19:04:48] Batch 40 - Loss: 0.846240
[2025-05-11 19:04:55] Batch 45 - Loss: 0.831412
[2025-05-11 19:05:03] Batch 50 - Loss: 0.845829
[2025-05-11 19:05:11] Batch 55 - Loss: 0.841276
[2025-05-11 19:05:18] Batch 60 - Loss: 0.843022
[2025-05-11 19:05:26] Batch 65 - Loss: 0.837730
[2025-05-11 19:05:34] Batch 70 - Loss: 0.839520
[2025-05-11 19:05:41] Batch 75 - Loss: 0.835058
[2025-05-11 19:05:49] Batch 80 - Loss: 0.833220
[2025-05-11 19:05:57] Batch 85 - Loss: 0.820800
[2025-05-11 19:06:05] Batch 90 - Loss: 0.824191
[2025-05-11 19:06:12] Batch 95 - Loss: 0.813443
[2025-05-11 19:06:24] === Epoch 98/100 Summary ===
Training Loss: 0.810202
Validation Loss: 2.113635
Learning Rate: 0.00000120
[2025-05-11 19:06:25] Batch 0 - Loss: 0.988791
[2025-05-11 19:06:33] Batch 5 - Loss: 0.823750
[2025-05-11 19:06:41] Batch 10 - Loss: 0.900771
[2025-05-11 19:06:48] Batch 15 - Loss: 0.920293
[2025-05-11 19:06:56] Batch 20 - Loss: 0.860659
[2025-05-11 19:07:04] Batch 25 - Loss: 0.865676
[2025-05-11 19:07:12] Batch 30 - Loss: 0.850909
[2025-05-11 19:07:19] Batch 35 - Loss: 0.820277
[2025-05-11 19:07:27] Batch 40 - Loss: 0.815928
[2025-05-11 19:07:35] Batch 45 - Loss: 0.822505
[2025-05-11 19:07:42] Batch 50 - Loss: 0.813991
[2025-05-11 19:07:50] Batch 55 - Loss: 0.810185
[2025-05-11 19:07:58] Batch 60 - Loss: 0.804818
[2025-05-11 19:08:06] Batch 65 - Loss: 0.792601
[2025-05-11 19:08:13] Batch 70 - Loss: 0.810083
[2025-05-11 19:08:21] Batch 75 - Loss: 0.817106
[2025-05-11 19:08:29] Batch 80 - Loss: 0.825562
[2025-05-11 19:08:36] Batch 85 - Loss: 0.811497
[2025-05-11 19:08:44] Batch 90 - Loss: 0.809289
[2025-05-11 19:08:52] Batch 95 - Loss: 0.804910
[2025-05-11 19:09:03] === Epoch 99/100 Summary ===
Training Loss: 0.809493
Validation Loss: 2.130069
Learning Rate: 0.00000120
[2025-05-11 19:09:05] Batch 0 - Loss: 1.022062
[2025-05-11 19:09:13] Batch 5 - Loss: 0.839921
[2025-05-11 19:09:20] Batch 10 - Loss: 0.768718
[2025-05-11 19:09:28] Batch 15 - Loss: 0.763121
[2025-05-11 19:09:36] Batch 20 - Loss: 0.737020
[2025-05-11 19:09:44] Batch 25 - Loss: 0.705431
[2025-05-11 19:09:51] Batch 30 - Loss: 0.691971
[2025-05-11 19:09:59] Batch 35 - Loss: 0.710635
[2025-05-11 19:10:05] === Epoch 100/100 Summary ===
Training Loss: 0.715537
Validation Loss: 2.136144
Learning Rate: 0.00000120
[2025-05-11 19:10:05] Generating detection visualizations for epoch 100...
[2025-05-11 19:10:05] Saved visualization to trained_models/custom_detector\visualizations\epoch_100\sample_1.png
[2025-05-11 19:10:06] Saved visualization to trained_models/custom_detector\visualizations\epoch_100\sample_2.png
[2025-05-11 19:10:06] Saved visualization to trained_models/custom_detector\visualizations\epoch_100\sample_3.png
[2025-05-11 19:10:06] === Training Completed ===
[2025-05-11 19:10:06] Best epoch: 98
[2025-05-11 19:10:06] Best validation loss: 2.113635
[2025-05-11 19:10:06] Model training completed in 225.08 minutes
[2025-05-11 19:10:06] Model saved to trained_models/custom_detector\final_model.keras
[2025-05-11 19:10:07] Training history plot saved to trained_models/custom_detector\training_history.png
[2025-05-11 19:10:07] === Valorant Detector Completed ===
